1
00:00:17,039 --> 00:00:23,259
https://youtu.be/kHiuZiu8lsg
Thus far we've examined how ignorance can
be strategically created and also how it can

2
00:00:23,259 --> 00:00:29,949
inadvertently emerge out of pervasive psychological
and social processes. What happens when interested

3
00:00:29,949 --> 00:00:36,899
parties disagree about whether ignorance should
be maintained or knowledge sought out? Excellent

4
00:00:36,899 --> 00:00:41,989
examples may be found in disputes regarding
the regulation of scientific research and

5
00:00:41,989 --> 00:00:48,360
technological development. There are long-running
debates about the extent to which scientists,

6
00:00:48,360 --> 00:00:54,089
engineers, and other creative enterprises
can and should be regulated, and if so by

7
00:00:54,089 --> 00:00:59,350
whom and on what grounds.
The question of whether experiment X should

8
00:00:59,350 --> 00:01:05,869
be performed or technology Y developed is
perennial. The most difficult versions of

9
00:01:05,869 --> 00:01:10,720
this question arise when pursuing the object
of one's curiosity violates others' deeply

10
00:01:10,720 --> 00:01:17,720
held values or ethical principles, or induces
great fear. These issues have a lengthy history.

11
00:01:18,880 --> 00:01:23,180
Popular images of scholars and scientists
pursuing knowledge with horrific consequences

12
00:01:23,180 --> 00:01:30,180
for themselves and others range from the 16th-century
legend of Faustus to Frankenstein to Bruce

13
00:01:31,700 --> 00:01:38,700
Banner (the Hulk). Present-day examples of
scientific research and technological developments

14
00:01:38,700 --> 00:01:45,250
evoking this kind of conflict include stem
cell research, human cloning, the Large Hadron

15
00:01:45,250 --> 00:01:50,320
Collider, and genetic modification of food
crops.

16
00:01:50,320 --> 00:01:54,600
Before it began operations, fears that the
Large Hadron Collider could generate a black

17
00:01:54,600 --> 00:02:00,640
hole that would swallow the Earth made international
headlines, and debates over its safety have

18
00:02:00,640 --> 00:02:06,670
continued, including lawsuits intended to
halt its operation. The nub of the problem

19
00:02:06,670 --> 00:02:13,170
of course is risk, and a peculiarly modern
version of risk at that. The sociologist Ulrich

20
00:02:13,170 --> 00:02:18,909
Beck's (1992) classic work crystallized a
distinction between older and newer risks

21
00:02:18,909 --> 00:02:24,790
associated with experimentation and exploration.
The older risks were localized and often restricted

22
00:02:24,790 --> 00:02:31,040
to the risk-takers themselves. The new risks,
according to writers like Beck, are global

23
00:02:31,040 --> 00:02:36,129
and catastrophic. The concerns about the Large
Hadron Collider fit Beck's definition of the

24
00:02:36,129 --> 00:02:40,709
new risks.
When fears about proposed experiments or technological

25
00:02:40,709 --> 00:02:46,819
developments concern the potential misuse
of potentially beneficial research or technology,

26
00:02:46,819 --> 00:02:53,279
disputes of this kind are known as "dual use
dilemmas". There's a broad active network

27
00:02:53,279 --> 00:02:59,669
of researchers on this topic. I participated
in a research workshop at The Australian National University

28
00:02:59,669 --> 00:03:05,109
on this topic, from which a book emerged with
contributions from scientists, ethicists,

29
00:03:05,109 --> 00:03:09,639
and social scientists.
Probably the most famous example is the controversy

30
00:03:09,639 --> 00:03:14,930
arising from the development of nuclear fission
technology, which gave us the means to nuclear

31
00:03:14,930 --> 00:03:21,010
warfare on the one hand but numerous peacetime
applications on the other. The fiercest debates

32
00:03:21,010 --> 00:03:28,010
these days on dual use dilemmas focus on biological
experiments and nanotechnology. The American

33
00:03:29,230 --> 00:03:35,480
National Research Council report listed"experiments
of concern" as those including any of the

34
00:03:35,480 --> 00:03:40,859
following capabilities:
1. demonstrating how to render a vaccine ineffective;

35
00:03:40,859 --> 00:03:46,989
2. enhancing resistance to therapeutically
useful antibiotics or antiviral agents;

36
00:03:46,989 --> 00:03:52,730
3. enhancing the virulence of a pathogen or
render a non-pathogen virulent;

37
00:03:52,730 --> 00:03:59,730
4. increasing the transmissibility of a pathogen;
5. altering the host range of a pathogen;

38
00:03:59,930 --> 00:04:05,790
6. enabling the evasion of diagnosis and/or
detection by established methods; and

39
00:04:05,790 --> 00:04:09,989
7. enabling the weaponization of a biological
agent or toxin.

40
00:04:09,989 --> 00:04:16,799
There are three kinds of concern underpinning
dual-use dilemmas. The first arises from foreseeable

41
00:04:16,799 --> 00:04:22,389
misuses that could ensue from an experiment
or new technology. Most obvious are experiments

42
00:04:22,389 --> 00:04:27,210
or developments intended to create weapons
in the first place. Examples include the German

43
00:04:27,210 --> 00:04:32,440
scientists responsible for gas warfare in
World War I and American scientists responsible

44
00:04:32,440 --> 00:04:37,910
for atomic warfare at the end of World War
II. But not as obvious are the opportunities

45
00:04:37,910 --> 00:04:44,910
to exploit nonmilitary research or technology.
An example of potential misuse of a rather

46
00:04:44,940 --> 00:04:51,440
mundane technology would be terrorists or
organized crime networks exploiting illegal

47
00:04:51,440 --> 00:04:57,620
botox manufacturing facilities to distill
botulinum toxin.

48
00:04:57,620 --> 00:05:02,650
Research results published in 2005 announced
the complete genetic sequencing of the 1918

49
00:05:02,650 --> 00:05:09,430
influenza A (H1N1) virus (also known as. the
"Spanish flu") and also its resurrection using

50
00:05:09,430 --> 00:05:14,680
reverse genetic techniques. This is the virus
that killed between 20 and 100 million people

51
00:05:14,680 --> 00:05:19,980
in 1918 to 1919. Prior to publication of the
reverse-engineering paper, the US National

52
00:05:19,980 --> 00:05:25,250
Science Advisory Board for Biosecurity was
asked to consider the consequences. The Board

53
00:05:25,250 --> 00:05:29,060
decided that the scientific benefits flowing
from publication of this information about

54
00:05:29,060 --> 00:05:35,790
the Spanish flu outweighed the risk of misuse.
Understandably, publication of this information

55
00:05:35,790 --> 00:05:42,280
aroused concerns that malign agents could
use it to reconstruct H1N1. The same issues

56
00:05:42,280 --> 00:05:48,970
have been raised concerning the publication
of the H5N1 influenza ("bird flu") genome.

57
00:05:48,970 --> 00:05:54,020
The second type of concern is foreseeable
catastrophic accidents that could arise from

58
00:05:54,020 --> 00:05:59,450
unintended consequences of research or technological
developments. The possibility that current

59
00:05:59,450 --> 00:06:05,200
stockpiles of smallpox could be accidentally
let loose is the kind of event to be concerned

60
00:06:05,200 --> 00:06:09,470
about here. Such an event also is, for some
people, an argument against research enterprises

61
00:06:09,470 --> 00:06:16,360
such as the reengineering of H1N1.
The third type of concern is in some ways

62
00:06:16,360 --> 00:06:23,360
more worrying: Unforeseeable potential misuses
or accidents. After all, a lot of research

63
00:06:23,850 --> 00:06:30,480
yields unanticipated findings and/or opportunities
for new technological developments. A 2001

64
00:06:30,480 --> 00:06:35,430
paper on mousepox virus research at The Australian
National University is an example of this

65
00:06:35,430 --> 00:06:41,890
kind of serendipity. The researchers were
on the track of a genetically engineered sterility

66
00:06:41,890 --> 00:06:48,000
treatment that would combat mouse plagues
in Australia. But this research project also

67
00:06:48,000 --> 00:06:55,000
led to the creation of a highly virulent strain
of mousepox. The strain the researchers created

68
00:06:55,000 --> 00:07:02,000
killed both mice with resistance to smallpox
and mice vaccinated against smallpox.

69
00:07:02,290 --> 00:07:08,850
Moreover, the principles by which this new
strain was engineered were readily generalizable,

70
00:07:08,850 --> 00:07:15,050
and raised the possibility of creating a highly
virulent strain of smallpox resistant to available

71
00:07:15,050 --> 00:07:22,050
vaccines. Indeed, in 2003 a team of scientists
at St Louis University published research

72
00:07:22,420 --> 00:07:29,420
in which they had extended the mousepox results
to cowpox, a virus that can infect humans.

73
00:07:30,450 --> 00:07:36,300
The fear, of course, is that these technological
possibilities could be exploited by terrorists.

74
00:07:36,300 --> 00:07:41,040
Recent advances in synthetic genomics have
magnified this problem. It is now possible

75
00:07:41,040 --> 00:07:46,920
not only to enhance the virulence of extant
viruses, but also create new viruses from

76
00:07:46,920 --> 00:07:50,710
scratch.
The moral and political questions raised by

77
00:07:50,710 --> 00:07:55,450
cases like these are not easy to resolve,
for at least three reasons. First, the pros

78
00:07:55,450 --> 00:08:01,690
and cons often are unknown to at least some
extent. Second, even the known pros and cons

79
00:08:01,690 --> 00:08:06,060
usually weigh heavily on both sides of the
argument. There are very good reasons for

80
00:08:06,060 --> 00:08:10,870
going ahead with the research and very good
reasons for prohibiting it.

81
00:08:10,870 --> 00:08:17,590
The third reason applies only to some cases,
and it makes those cases the toughest of all.

82
00:08:17,590 --> 00:08:22,280
"Dual use dilemmas" is a slightly misleading
phrase, in a technical sense that relates

83
00:08:22,280 --> 00:08:28,180
to this third reason. Many cases are really
just tradeoffs, where at least in principle

84
00:08:28,180 --> 00:08:32,380
rational negotiators could weigh the costs
and benefits according to their lights and

85
00:08:32,380 --> 00:08:38,399
arrive at an optimal decision. But some cases
genuinely are social dilemmas, in the following

86
00:08:38,399 --> 00:08:44,990
sense of the term: Choices dictated by rational,
calculating self-interest nevertheless will

87
00:08:44,990 --> 00:08:51,230
lead to the destruction of the common good
and, ultimately, everyone's own interests.

88
00:08:51,230 --> 00:08:57,499
Social dilemmas like these aren't new. Garrett
Hardin's "tragedy of the commons" is a famous

89
00:08:57,499 --> 00:09:03,430
and much debated example. Hardin pointed out
that if all members in a group used finite

90
00:09:03,430 --> 00:09:08,490
common resources for their own gain and with
no regard for others, these resources would

91
00:09:08,490 --> 00:09:15,059
be depleted. The key to understanding this
as a dilemma is that even rational individuals

92
00:09:15,059 --> 00:09:21,259
can argue that if they don't exploit the resource
others will, thereby justifying their own

93
00:09:21,259 --> 00:09:27,649
exploitation of the resource.
A typical arms-race is another obvious example.

94
00:09:27,649 --> 00:09:32,670
Researchers in countries A and B know that
each country has the means to revive an extinct,

95
00:09:32,670 --> 00:09:38,339
virulent pathogen that could be exploited
as a bioweapon. If the researchers in country

96
00:09:38,339 --> 00:09:44,410
A revive the pathogen and researchers in country
B do not, country A temporarily enjoys a tactical

97
00:09:44,410 --> 00:09:50,800
advantage over country B. However, it also
imposes a risk on both A and B of theft or

98
00:09:50,800 --> 00:09:56,389
accidental release of the pathogen. If country
B responds by duplicating this feat then B

99
00:09:56,389 --> 00:10:03,389
regains equal footing with A but now has multiplied
the risk of accidental release or theft. Conversely,

100
00:10:03,649 --> 00:10:09,930
if A restrains from reviving the pathogen
then B could play A for a sucker by reviving

101
00:10:09,930 --> 00:10:15,269
it. It is in each country's self-interest
to revive the pathogen in order to avoid being

102
00:10:15,269 --> 00:10:20,550
trumped by the other, but the result is the
creation of dread risks that neither country

103
00:10:20,550 --> 00:10:26,399
wants to bear. You may have heard about "Prisoner's
Dilemma" or "Chicken Game". These are types

104
00:10:26,399 --> 00:10:33,029
of social dilemmas, and some dual use dilemmas
are structurally similar to them.

105
00:10:33,029 --> 00:10:36,970
Social dilemmas present a very difficult problem
in the regulation of curiosity (and other

106
00:10:36,970 --> 00:10:41,779
kinds of choice behavior) because solutions
cannot be worked out solely on the basis of

107
00:10:41,779 --> 00:10:47,860
appeals to rational self-interest. Once you
know what to look for, you can find social

108
00:10:47,860 --> 00:10:52,329
dilemmas in all sorts of places. The research
literature on how to deal with and resolve

109
00:10:52,329 --> 00:10:59,329
them includes contributions from economics,
psychology, political science, anthropology,

110
00:11:00,009 --> 00:11:05,480
sociology and applied mathematics. This research
has had applications ranging from environmental

111
00:11:05,480 --> 00:11:11,869
policy formation to marriage guidance counseling.
But it shouldn't surprise you too much to

112
00:11:11,869 --> 00:11:17,529
learn that most of the early work on social
dilemmas stemmed from and was supported by

113
00:11:17,529 --> 00:11:22,879
the American military.
So to conclude, let's try putting the proverbial

114
00:11:22,879 --> 00:11:28,879
shoe on the other foot. The dual-use dilemmas
literature focuses almost exclusively on scientific

115
00:11:28,879 --> 00:11:34,309
research and technological developments that
could be weaponized. But what about the reverse

116
00:11:34,309 --> 00:11:41,309
process: Military research and development
with nonmilitary benefits? Or, for that matter,

117
00:11:41,809 --> 00:11:48,809
R & D from illicit or immoral sources that
yield legitimate spinoffs and applications?

118
00:11:49,509 --> 00:11:52,160
These prospects appear to have been relatively
neglected.

119
00:11:52,160 --> 00:11:59,149
Nevertheless, it isn't hard to find examples:
The internet, for one. The net originated

120
00:11:59,149 --> 00:12:05,209
with the American Defense Advanced Research
Projects Agency (DARPA), was rapidly taken

121
00:12:05,209 --> 00:12:11,749
up by university-based researchers via their
defense-funded research grants, and morphed

122
00:12:11,749 --> 00:12:18,740
by the late 1980's into the NSFnet. Once the
net escaped its military confines, certain

123
00:12:18,740 --> 00:12:24,509
less than licit industries spearheaded its
development. As Peter Nowak shows in his entertaining

124
00:12:24,509 --> 00:12:30,309
and informative (if sometimes overstated)
book "Sex, Bombs and Burgers", the pornography

125
00:12:30,309 --> 00:12:35,819
industry was responsible for internet-defining
innovations such as live video streaming,

126
00:12:35,819 --> 00:12:42,069
video-conferencing and key aspects of internet
security provision. Before long, mainstream

127
00:12:42,069 --> 00:12:48,660
businesses were quietly adopting ways of using
the net pioneered by the porn industry.

128
00:12:48,660 --> 00:12:52,059
Of course, I'm not proposing that research
funding agencies should start throwing financial

129
00:12:52,059 --> 00:12:57,129
support behind research and development in
the porn industry. My point is just that this

130
00:12:57,129 --> 00:13:04,129
"other" dual use dilemma merits greater attention
and study than it has received so far.

131
00:13:04,389 --> 00:13:07,819
Dual use dilemmas are a special case of a
broader idea about ignorance that has deep

132
00:13:07,819 --> 00:13:14,129
roots in many cultures and histories, namely
the idea that there are things humanity should

133
00:13:14,129 --> 00:13:20,970
never know. Some topics are deemed taboo via
religious doctrines, and others by arguments

134
00:13:20,970 --> 00:13:27,110
about negative consequences or misuses arising
from knowing about them. The notion that there

135
00:13:27,110 --> 00:13:33,350
are things we shouldn't know suggests that
we have a concept of "virtuous ignorance",

136
00:13:33,350 --> 00:13:36,149
and in the next lecture we'll explore this
concept.

