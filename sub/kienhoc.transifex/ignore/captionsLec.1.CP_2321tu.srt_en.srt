1
00:00:20,529 --> 00:00:24,710
https://youtu.be/cEPF1P8kz-E
So what I want to do at the end of each week
is to reflect back on the lectures of each

2
00:00:24,710 --> 00:00:30,699
week and to think about what they tell us
about complex problems; both understanding

3
00:00:30,699 --> 00:00:33,120
them and dealing with them.

4
00:00:33,120 --> 00:00:37,360
It's probably worth starting by talking about
what I mean by complex problems, so things

5
00:00:37,360 --> 00:00:43,690
like global warming, the obesity epidemic
and organised crime for example.

6
00:00:43,690 --> 00:00:49,309
Complex problems have three dimensions which
are illustrated in this three pentagons. First,

7
00:00:49,309 --> 00:00:54,269
is bringing together different kinds of knowledge.
Second, is understanding and managing ignorance,

8
00:00:54,269 --> 00:00:59,489
or as have referred to it here, diverse
unknowns. And third, is taking action. And

9
00:00:59,489 --> 00:01:04,500
it's taking action based on what we know and
what we don't know, not just what we know

10
00:01:04,500 --> 00:01:05,330
alone.

11
00:01:05,330 --> 00:01:11,520
Let me talk about each of those in a little
more detail. So if we were studying organised

12
00:01:11,520 --> 00:01:16,840
crime for example, we would want to bring together
different disciplines, for example, criminology,

13
00:01:16,840 --> 00:01:22,310
psychology, economics, sociology. We would also
want to bring together the perspectives of

14
00:01:22,310 --> 00:01:28,130
people who understood this from practice; stakeholders. So
people like the police who deal with organised

15
00:01:28,130 --> 00:01:33,159
crime; people like victims who are affected
by organised crime; and if you could, you would

16
00:01:33,159 --> 00:01:37,320
also want to get the perspectives of perpetrators,
the criminals themselves, although that's

17
00:01:37,320 --> 00:01:40,390
a lot more complicated.

18
00:01:40,390 --> 00:01:45,579
When we're talking about the bottom pentagon,
action, we're talking about things like

19
00:01:45,579 --> 00:01:52,570
technological innovation, government policy,
professional practice and community behaviours

20
00:01:52,570 --> 00:01:55,159
and attitudes.

21
00:01:55,159 --> 00:02:00,799
So if we reflect on ignorance or diverse unknowns,
we're interested in it, in its own right,

22
00:02:00,799 --> 00:02:05,899
as we're talking about in these lectures,
but we're also interested in it in how it

23
00:02:05,899 --> 00:02:11,250
affects what we know and how it affects the
actions that we take. And this will become

24
00:02:11,250 --> 00:02:14,840
particularly important when we are dealing
with complex problems.

25
00:02:14,840 --> 00:02:20,010
What I am going to do, is I'm going to
reflect on what the five lectures meant for

26
00:02:20,010 --> 00:02:24,670
me in my ability to think about complex problems
and we'd love to hear from you through the

27
00:02:24,670 --> 00:02:28,720
discussion forum about what it meant for you.
And we'll be putting some questions on the

28
00:02:28,720 --> 00:02:33,569
discussion forum to prompt you to contribute.

29
00:02:33,569 --> 00:02:38,530
So let's start with lecture one: Five Reasons
Why We Think We Know More Than We Do. And

30
00:02:38,530 --> 00:02:43,950
three of those reasons, the ones here highlighted
in black, are the ones that were most salient

31
00:02:43,950 --> 00:02:44,900
for me.

32
00:02:44,900 --> 00:02:49,360
Let's start with the first: treating other
sources as equivalent to what we've found

33
00:02:49,360 --> 00:02:54,150
out for ourselves. When we're dealing with
complex problems, we're often dealing with

34
00:02:54,150 --> 00:02:58,950
problems on a much bigger scale and so we
have to rely even more on the knowledge of

35
00:02:58,950 --> 00:03:04,950
others than on what we can bring that we've
got from first-hand experience. When we do

36
00:03:04,950 --> 00:03:10,390
that, the importance of being able to judge
both relevance and quality of the contributions

37
00:03:10,390 --> 00:03:15,099
of others becomes really important. We need
to be able to figure out what contributions

38
00:03:15,099 --> 00:03:19,439
are relevant and how relevant they are, and
what the quality is; how good is the evidence;

39
00:03:19,439 --> 00:03:25,730
how good are the methods that have been used
to develop that evidence.

40
00:03:25,730 --> 00:03:32,180
Quality is also really important in the reason
for thinking we know more than we do, that

41
00:03:32,180 --> 00:03:37,659
is based on over-confidence in estimations
and predictions. Again, with complex problems

42
00:03:37,659 --> 00:03:42,000
we have this issue of scale. We're going
to have more estimations and predictions when

43
00:03:42,000 --> 00:03:47,019
we're dealing with complex problems, and
so we need first to be aware of that bias,

44
00:03:47,019 --> 00:03:52,250
but we also need to be able to judge what
underpins the estimations and predictions,

45
00:03:52,250 --> 00:03:57,120
what the quality of that evidence is. Are
they just based on people's bias, or is there

46
00:03:57,120 --> 00:04:00,040
actually some solid evidence underpinning
them?

47
00:04:00,040 --> 00:04:05,939
So the third reason that is important is
our propensity to fill in the blanks. There

48
00:04:05,939 --> 00:04:11,140
are three guides to how we fill in the blanks:
context; stereotypes; and assumptions

49
00:04:11,140 --> 00:04:16,909
of similarity. Context was what Mike was talking
about in his example in the restaurant. So

50
00:04:16,909 --> 00:04:21,599
if we're in an Indian restaurant, the context
will make us think that hot means "spicy",

51
00:04:21,599 --> 00:04:25,810
whereas if we're in a Russian restaurant,
the context will make us think that "hot"

52
00:04:25,810 --> 00:04:27,820
is "high temperature".

53
00:04:27,820 --> 00:04:32,789
The other way of filling in the blanks is
to build on stereotypes. So we all have images

54
00:04:32,789 --> 00:04:36,820
in our mind of what a doctor is or what a
fire-fighter is, or how we should deal with

55
00:04:36,820 --> 00:04:43,820
financial matters. Stereotypes apply to people
and behaviours. And what we'll think about

56
00:04:44,090 --> 00:04:48,440
is if we"re dealing with a doctor, for example,
and they say something that's ambiguous, weï¿½ll

57
00:04:48,440 --> 00:04:52,419
draw on that stereotype for what we think
they mean.

58
00:04:52,419 --> 00:04:57,930
The third guide to how we fill in the blanks
is that we often assume that people are similar

59
00:04:57,930 --> 00:05:03,970
to us, so that what they mean is what we would
mean in that situation. All of these three

60
00:05:03,970 --> 00:05:07,370
things happen when we"re dealing with complex
problems.

61
00:05:07,370 --> 00:05:11,680
A lot of researchers dealing with complex
problems suggest that what we need to do is

62
00:05:11,680 --> 00:05:18,680
to develop a common language upfront. So it's
been well realised now through a lot of experience,

63
00:05:18,720 --> 00:05:23,330
that when an economist talks about "value",
what she means is not the same as when a philosopher

64
00:05:23,330 --> 00:05:30,330
talks about "value" and so there is some "value" - a
different form of value again - in developing

65
00:05:32,080 --> 00:05:36,860
a common language. That's not something that
I particularly favour, because a) there's

66
00:05:36,860 --> 00:05:42,410
a lot of transaction costs upfront involved
in doing that and, b) it only deals with one

67
00:05:42,410 --> 00:05:47,569
of these things that happens; it only deals
with the assumption of similarity. And there

68
00:05:47,569 --> 00:05:52,720
are two other ways that filling in the blanks
can occur. I much prefer to keep all three

69
00:05:52,720 --> 00:05:57,900
ways of filling the blanks at the front of
mind and to look for "ah-ha" moments in

70
00:05:57,900 --> 00:06:03,039
a collaboration when you realise that people
are talking past each other and to then use

71
00:06:03,039 --> 00:06:09,259
that as a way of helping people to understand
each other's different perspectives and assumptions,

72
00:06:09,259 --> 00:06:15,449
which can be a very rich and rewarding part
of working together on a complex problem.

73
00:06:15,449 --> 00:06:22,449
Moving on to lecture two, Metaphors for Not
Knowing. The metaphor that I find most useful

74
00:06:22,449 --> 00:06:28,599
is the one of the island in the sea of ignorance.
So we have the island of knowledge in the

75
00:06:28,599 --> 00:06:34,819
ocean or sea of ignorance. And the reason
why this metaphor is useful is thinking about

76
00:06:34,819 --> 00:06:40,500
the shoreline. So the shoreline is what we
know we don't know. And what this metaphor

77
00:06:40,500 --> 00:06:47,080
tells us is that as knowledge grows, as the
island grows, the shoreline also grows and

78
00:06:47,080 --> 00:06:52,060
we become more aware of what we know we don't
know.

79
00:06:52,060 --> 00:06:56,449
So there's always a temptation when you're
doing research on complex problems to find

80
00:06:56,449 --> 00:07:01,650
out more and more and more, hoping that you're
going to get to the answer. Instead, what

81
00:07:01,650 --> 00:07:07,180
you often find is that you do learn more,
but you also find out more and more about

82
00:07:07,180 --> 00:07:11,840
what you don't know. And of course in dealing
with a complex problem there comes a time

83
00:07:11,840 --> 00:07:18,319
when you have to act, and usually you will
have to act on incomplete knowledge. This

84
00:07:18,319 --> 00:07:23,759
metaphor helps us understand that imperfection
is inevitable and that's going to be one of

85
00:07:23,759 --> 00:07:29,569
the themes of my talks in this course; is
the inevitability of imperfection and understanding

86
00:07:29,569 --> 00:07:35,240
how important that is in what we do. So one
reason for imperfection is that the more we

87
00:07:35,240 --> 00:07:40,099
know, the more we are aware of what we don't
know.

88
00:07:40,099 --> 00:07:44,770
So in the third lecture, Thinking effectively
about Ignorance, I'm going to concentrate

89
00:07:44,770 --> 00:07:50,250
on Ann Kerwin's matrix. I'm going to leave
Mike Smithson's typology to one side and to

90
00:07:50,250 --> 00:07:55,720
deal with that in Ignorance II, when we look
at how different disciplines deal with ignorance

91
00:07:55,720 --> 00:08:00,750
because, in fact, different disciplines deal
with different parts of the typology. Here

92
00:08:00,750 --> 00:08:07,750
I'm going to deal with Ann Kerwin's matrix.
First, here's what we know we know. And here's

93
00:08:08,550 --> 00:08:13,280
what we know that we don't know; the ignorance
that we're conscious of. If you like this

94
00:08:13,280 --> 00:08:17,680
takes us back to the metaphor of the island
of knowledge in the ocean of ignorance. What

95
00:08:17,680 --> 00:08:22,860
we know we know is the shoreline - sorry,
what we know we don't know is the shoreline

96
00:08:22,860 --> 00:08:25,240
around the island.

97
00:08:25,240 --> 00:08:31,500
Let's move on then to unknown knowns or tacit
knowledge. This is the knowledge that we aren't

98
00:08:31,500 --> 00:08:36,280
aware that we have and one of the best ways
of becoming aware of tacit knowledge is to

99
00:08:36,280 --> 00:08:42,940
go and spend time in another culture. Suddenly,
the ways that we know how to behave seem inappropriate

100
00:08:42,940 --> 00:08:49,940
and we become aware of how much knowing how
to behave is ingrained in what we do.

101
00:08:50,750 --> 00:08:54,920
Another form of tacit knowledge is what we
learn when we are trained in a discipline,

102
00:08:54,920 --> 00:08:59,510
particularly when we become academics. The
way that we approach a problem becomes second

103
00:08:59,510 --> 00:09:04,980
nature to us because our disciplinary knowledge
becomes ingrained and becomes tacit. So when

104
00:09:04,980 --> 00:09:09,179
we are dealing with complex problems, bringing
people together from different disciplines

105
00:09:09,179 --> 00:09:13,980
often leads to a very interesting discussion
about how the problem should be tackled, because

106
00:09:13,980 --> 00:09:19,260
each discipline has got its own approach.
This can be another rich source of developing

107
00:09:19,260 --> 00:09:25,280
understanding among members of teams in complex
problems.

108
00:09:25,280 --> 00:09:28,860
Tacit knowledge is also an important part
of professional knowledge. So if we think

109
00:09:28,860 --> 00:09:33,699
about fire-fighters and nurses, for example,
they know exactly what to do in an emergency

110
00:09:33,699 --> 00:09:39,709
because that's ingrained in their knowledge;
it's become part of their unconscious knowledge,

111
00:09:39,709 --> 00:09:46,520
their unknown knowns. And these are also important
when we "fill in the blanks" as we talked

112
00:09:46,520 --> 00:09:50,819
about earlier. In "filling in the blanks"
we often draw on our tacit knowledge, not

113
00:09:50,819 --> 00:09:53,470
our explicit knowledge.

114
00:09:53,470 --> 00:09:59,449
Let's move on then to the last quadrant of
the matrix. This is the unknown unknown. And

115
00:09:59,449 --> 00:10:04,880
when we're dealing with complex problems,
unknown unknowns are almost inevitable. One

116
00:10:04,880 --> 00:10:11,439
of our tasks as researchers is to try to minimise
them and to help make them manageable. But

117
00:10:11,439 --> 00:10:16,280
even so there are going to be times when we're
caught by surprise. What we want to do is

118
00:10:16,280 --> 00:10:21,280
build systems of dealing with the complex
problem that are resilient of those kinds

119
00:10:21,280 --> 00:10:25,929
of surprises and that are adaptable and nimble
in dealing with them.

120
00:10:25,929 --> 00:10:30,839
Mike talked about one way that we can find
out about unknown unknowns. While we can't

121
00:10:30,839 --> 00:10:36,260
see our own, we can see them in other people.
And, again, one of the advantages of bringing

122
00:10:36,260 --> 00:10:41,079
together disciplines and stakeholders to work
on a complex problem is that if you set the

123
00:10:41,079 --> 00:10:47,069
environment up correctly, they can help each
other see each other's unknown unknowns. Of

124
00:10:47,069 --> 00:10:50,160
course there are always going to be things
that we can't predict. There are always going

125
00:10:50,160 --> 00:10:57,160
to be elements of surprise, and that's another
reason why imperfection is inevitable, where

126
00:10:57,220 --> 00:11:04,220
there are always going to be some things that
we can't know, that we don't know.

127
00:11:04,309 --> 00:11:09,370
Let's move on then to the fourth lecture.
And here I want to concentrate on one aspect,

128
00:11:09,370 --> 00:11:14,230
namely, taboo. This is important if you're
the leader of a project and you,re encountering

129
00:11:14,230 --> 00:11:19,630
resistance to something that you want the
team to do. In your checklist of things that

130
00:11:19,630 --> 00:11:24,569
you tick off why that resistance might be
occurring, taboo needs to be on there, because

131
00:11:24,569 --> 00:11:30,740
you would handle somebody who was confronting
a taboo, you'd handle that quite differently

132
00:11:30,740 --> 00:11:35,709
from what was happening if somebody was just
being lazy or bloody-minded. And so you need

133
00:11:35,709 --> 00:11:39,959
to be aware that that might be a reason why
people don't want to do something that you

134
00:11:39,959 --> 00:11:43,860
might be encouraging them to do.

135
00:11:43,860 --> 00:11:48,579
So let's move on to the last lecture, the
one about Negative Knowledge. And here I want

136
00:11:48,579 --> 00:11:53,620
to concentrate on so-called wicked problems.
Many of the complex problems that we deal

137
00:11:53,620 --> 00:11:57,860
with are wicked problems and by that we mean
that they're problems that don't have a

138
00:11:57,860 --> 00:12:04,819
clear-cut perfect solution. This is another
reason why imperfection is inevitable because

139
00:12:04,819 --> 00:12:10,150
there are many problems where there is no
single perfect answer, no magic bullet that

140
00:12:10,150 --> 00:12:14,900
will get rid of it. Instead there are just
things that we can do that are better or worse.

141
00:12:14,900 --> 00:12:19,600
So if we think about organised crime, we're
never going to get rid of organised crime

142
00:12:19,600 --> 00:12:24,280
completely, but there are things that we can
do to make it much less common and to make

143
00:12:24,280 --> 00:12:27,669
it much less impactful.

144
00:12:27,669 --> 00:12:34,669
I want to finish by drawing on the aphorism
that Mike drew our attention to by Peter Medawar

145
00:12:35,510 --> 00:12:40,980
as science is the art of a soluble. And here
I want to draw on a sculpture from The Australian

146
00:12:40,980 --> 00:12:45,569
National University campus where this lecture
is being filmed. This is a sculpture by Tim

147
00:12:45,569 --> 00:12:50,679
Spellman called Kulla's Ripple. And what I'm
going to do is unceremoniously chop this sculpture

148
00:12:50,679 --> 00:12:57,679
in half. Half of the sculpture, this half,
can be seen as a metaphor for our methods

149
00:12:58,480 --> 00:13:04,020
for dealing with soluble problems. We have
lots of robust methods for dealing with soluble

150
00:13:04,020 --> 00:13:10,270
problems. On the other hand when we're talking
about insoluble or wicked problems, we have

151
00:13:10,270 --> 00:13:16,270
a much smaller set of methods to deal with,
that help us deal with them, much less sophisticated

152
00:13:16,270 --> 00:13:22,270
ways of thinking about wicked or complex problems.
And I'd argue what we need to do is, we

153
00:13:22,270 --> 00:13:28,850
need to take this small bundle of methods
and turn it into a much larger bundle of methods

154
00:13:28,850 --> 00:13:33,280
that's equivalent to the sort of methods that
we have for dealing with soluble or tame problems.

155
00:13:33,280 --> 00:13:38,280
In fact, I'd argue that that's one of the
big challenges for our society is to build

156
00:13:38,280 --> 00:13:45,280
up this set of methods into a robust repository
that we can draw on when we're dealing with

157
00:13:45,790 --> 00:13:50,730
wicked problems. And in these lectures on
dealing with ignorance, we're hoping to

158
00:13:50,730 --> 00:13:54,160
also give you some tools for thinking about
that more effectively.

