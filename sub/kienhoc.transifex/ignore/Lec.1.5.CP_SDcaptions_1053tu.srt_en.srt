1
00:00:23,150 --> 00:00:28,689
https://youtu.be/NdKyjm5cwpo
So let's look at what the lectures on ignorance
mean for complex problems in this final week,

2
00:00:28,689 --> 00:00:35,689
week 5. The focus on this week was that gathering
knowledge is not always better: especially

3
00:00:35,910 --> 00:00:41,910
when knowing more can lead to worse decisions;
when it's futile to seek greater uncertainty;

4
00:00:41,910 --> 00:00:46,730
when vagueness is particularly useful; when
fallacies about runs of luck are actually

5
00:00:46,730 --> 00:00:53,150
helpful; and when exploiting unknowns can
be a source of creativity.

6
00:00:53,150 --> 00:00:57,970
How do these understandings help us better
deal with complex problems? Well the important

7
00:00:57,970 --> 00:01:02,839
thing as we already know, is that complex
problems have many kinds of unknowns and require

8
00:01:02,839 --> 00:01:08,750
a range of types of decisions. We are not
yet very sophisticated in dealing with that

9
00:01:08,750 --> 00:01:13,430
range or making the different kinds of decisions.
We are much more sophisticated in how we deal

10
00:01:13,430 --> 00:01:17,540
with knowledge than we are with unknowns.
We are starting to take baby steps and there's

11
00:01:17,540 --> 00:01:23,610
a lot more work that we need to do.
But let's look at a bit...a bit more carefully

12
00:01:23,610 --> 00:01:28,670
at the lessons from this week. The first lesson
was about irrelevant information, which is

13
00:01:28,670 --> 00:01:33,630
common in complex problems. So the important
lessons here are that irrelevant information

14
00:01:33,630 --> 00:01:38,750
distracts us from relevant information, it
makes our predictions less accurate and it

15
00:01:38,750 --> 00:01:43,690
makes us over-confident in our predictions.
It can also lead to knowledge partitioning,

16
00:01:43,690 --> 00:01:50,280
which is when we use the same data but make
opposing conclusions. Remember the example

17
00:01:50,280 --> 00:01:55,930
that Mike gave about wildfires and fire-fighters,
and the predictions of whether the fire was

18
00:01:55,930 --> 00:02:01,479
moving uphill and where the wind was. What
they were using was an irrelevant piece of

19
00:02:01,479 --> 00:02:07,810
information about who started the fire or
how the fire was started, which made them

20
00:02:07,810 --> 00:02:14,080
reach completely different conclusions about
which way the fire was going to head.

21
00:02:14,080 --> 00:02:19,480
A second lesson this week is to watch out
for falling back on the recognition heuristic.

22
00:02:19,480 --> 00:02:22,780
Now I am going to deal with this differently
from the way that Mike dealt with it. Mike

23
00:02:22,780 --> 00:02:27,620
dealt with it in terms of understanding that
sometimes novices can do better than experts,

24
00:02:27,620 --> 00:02:31,249
that sometimes having more information isn't
better.

25
00:02:31,249 --> 00:02:36,969
In complex problems that's pretty rare. What's
more common is that the recognition heuristic

26
00:02:36,969 --> 00:02:41,999
can get in the way. So when recognition cues
are strongly correlated with outcomes, they

27
00:02:41,999 --> 00:02:47,499
can be very helpful and novices can do better
than experts. But when recognition cues are

28
00:02:47,499 --> 00:02:54,480
poorly correlated, they can be quite problematic.
Recognition cues often come from the media,

29
00:02:54,480 --> 00:02:59,299
from things that we've heard a lot about and
for complex problems we only usually hear

30
00:02:59,299 --> 00:03:04,779
a bit about the problem and what we hear is
often very skewed, so that the recognition

31
00:03:04,779 --> 00:03:11,779
cue is not a very reliable source of information
and therefore not a good thing to rely on.

32
00:03:12,909 --> 00:03:17,779
A third lesson this week is that for rare
outcomes, where a high degree of certainty

33
00:03:17,779 --> 00:03:22,889
is not possible, it can be futile to seek
more information, and this can also occur

34
00:03:22,889 --> 00:03:29,400
in complex problems. The consequences of this
are: that decision-makers might end up with

35
00:03:29,400 --> 00:03:34,889
unwarranted criticism for the decisions that
they make, and for people with actual... who

36
00:03:34,889 --> 00:03:40,069
are actually exposed to the outcomes, or potentially
exposed to the outcomes, they need to be helped

37
00:03:40,069 --> 00:03:45,209
to confront that uncertainty. What we need
to do in these situations is put resources

38
00:03:45,209 --> 00:03:52,209
into managing uncertainty, not into developing
more predictable outcomes. An example of this...

39
00:03:52,519 --> 00:03:58,069
of a complex problem that fits into this category
is some kinds of terrorist attacks, and we

40
00:03:58,069 --> 00:04:05,069
probably need to do a lot more analysis on
how best to handle these.

41
00:04:05,840 --> 00:04:11,529
Another lesson is that for complex problems
it's often not helpful to have to make stark

42
00:04:11,529 --> 00:04:16,970
either/or decisions, and that vagueness is
useful when it introduces a third or middle

43
00:04:16,970 --> 00:04:23,240
option. Again, think back on the examples
Mike gave. He talked about in courtroom situations

44
00:04:23,240 --> 00:04:29,440
where it's helpful to have a not proven option
as well as guilty or innocent and in medical

45
00:04:29,440 --> 00:04:35,020
diagnosis where it's not always clear that
somebody has or doesn't have the disease,

46
00:04:35,020 --> 00:04:39,530
but a third middle option saying we need more
investigation and more testing can be quite

47
00:04:39,530 --> 00:04:45,050
useful.
A fifth lesson from this week is about human

48
00:04:45,050 --> 00:04:50,659
recognition. We are not very good at recognising
random patterns but we are good at recognising

49
00:04:50,659 --> 00:04:56,240
both persistent and anti-persistent patterns
in chaos. And here I am using chaos in its

50
00:04:56,240 --> 00:05:02,930
precise physical sciences or mathematical
definition. Then if we have complex problems

51
00:05:02,930 --> 00:05:08,330
that have random processes or that have chaotic
processes, we need different decision-making

52
00:05:08,330 --> 00:05:14,810
tactics. When processes are random, our human
intuition isn't going to be very helpful,

53
00:05:14,810 --> 00:05:21,470
we need proper decision support mechanisms,
so computer models for example. But when environments

54
00:05:21,470 --> 00:05:26,000
are chaotic, our human intuition is actually
pretty good at helping us make predictions

55
00:05:26,000 --> 00:05:33,000
and so we can be much more reliant on that
in those kinds of situations in complex problems.

56
00:05:34,009 --> 00:05:38,879
Let's look at the last lesson from this week,
which is that sometimes safety can be enhanced

57
00:05:38,879 --> 00:05:45,629
by tipping the balance towards false positives.
The example Mike gave was of a smoke detector,

58
00:05:45,629 --> 00:05:49,949
you want a smoke detector to be fairly sensitive,
even if it goes off sometimes when you are

59
00:05:49,949 --> 00:05:55,759
making the toast, so that it is going to go
off when there is a real fire. A complex problem

60
00:05:55,759 --> 00:06:01,199
that's an analogous to this is thinking about
multiple use environments: so if you are working

61
00:06:01,199 --> 00:06:07,810
in an environment where there's more than
one use and one use of the environment can

62
00:06:07,810 --> 00:06:14,289
irrevocably damage the environment for another
use. Putting shipping through sensitive heritage

63
00:06:14,289 --> 00:06:21,289
coral reefs is an example of this, and if
things go wrong with the shipping it can irreparably

64
00:06:21,430 --> 00:06:25,699
damage the coral reefs and the tourism industry
that depends on them.

65
00:06:25,699 --> 00:06:29,759
So what you might want to have here is fairly
close monitoring to make sure that the shipping

66
00:06:29,759 --> 00:06:35,219
stays in shipping lanes and monitoring of
the waste that the shipping discharges, and

67
00:06:35,219 --> 00:06:39,400
you might want to be sensitive enough so that
occasionally you will pick up problems that

68
00:06:39,400 --> 00:06:45,030
aren't partic... that aren't important in
order to be able to act early when important

69
00:06:45,030 --> 00:06:51,159
problems do arise.
I look forward to getting your reflections

70
00:06:51,159 --> 00:06:55,650
and examples in the discussion forum on what
we've learned from this week for complex problems.

