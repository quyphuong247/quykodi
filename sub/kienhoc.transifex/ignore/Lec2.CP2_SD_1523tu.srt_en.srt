1
00:00:22,820 --> 00:00:29,480
https://youtu.be/EU0nq0jc_Rs
So, let's look at what week two lectures on
ignorance mean for complex problems. This

2
00:00:29,480 --> 00:00:34,000
week's lectures covered things that we'd rather
not know, at least for now; things that we

3
00:00:34,000 --> 00:00:40,360
never want to know; unknowns, improvisation
and creativity; knowing can be costly; indecision

4
00:00:40,360 --> 00:00:45,620
and indecisiveness; and imposing ignorance
on others.

5
00:00:45,620 --> 00:00:50,420
In terms of dealing with complex problems,
I'm going to use these to reflect on two issues:

6
00:00:50,420 --> 00:00:57,190
setting boundaries and managing collaborations.
All complex problems require boundary setting,

7
00:00:57,190 --> 00:01:02,219
because they're so big. And boundary setting
is a particular case of what we ignore for

8
00:01:02,219 --> 00:01:08,310
now (but might deal with later, for example,
in another study, or in setting policy), things

9
00:01:08,310 --> 00:01:12,840
we'll ignore permanently, and the fact that
knowing can be costly, especially in terms

10
00:01:12,840 --> 00:01:18,180
of transaction costs and opportunity costs.
It might be helpful to explain this using

11
00:01:18,180 --> 00:01:25,180
an example. So let's take a problem like:
What's the impact of cybercrime? If we're

12
00:01:25,679 --> 00:01:30,790
interested in cybercrime, there are a range
of disciplines that can help us, for example,

13
00:01:30,790 --> 00:01:36,740
psychology, economics, criminology, sociology.
We can also get useful perspectives from a

14
00:01:36,740 --> 00:01:41,810
range of stakeholders that have a perspective
on these problems. So the police will have

15
00:01:41,810 --> 00:01:46,340
a perspective on cybercrime. People who've
been victims, either businesses or individuals,

16
00:01:46,340 --> 00:01:52,259
will have a perspective. There are also political
considerations: how difficult is it to change

17
00:01:52,259 --> 00:01:56,590
laws? What's involved in getting other actions,
like giving the police more money, or more

18
00:01:56,590 --> 00:02:01,869
power? There will also be citizen concerns
that are important. For example, citizens

19
00:02:01,869 --> 00:02:06,369
might be worried about infringement on their
civil liberties by some of the actions that

20
00:02:06,369 --> 00:02:11,500
might be taken. And it would also be useful
to get perspectives of perpetrators, those

21
00:02:11,500 --> 00:02:16,450
committing cybercrime. So it would be useful
to know why they do it, what they get out

22
00:02:16,450 --> 00:02:21,940
of it, what the risks are, what their assessment
of risks and costs is, and what would make

23
00:02:21,940 --> 00:02:28,940
them stop. So, if we're trying to figure out
which of all of those things we will do, we

24
00:02:30,209 --> 00:02:36,139
will take into account the feasibility of
doing it. So, usually we have limited time,

25
00:02:36,139 --> 00:02:40,570
limited money and limited personnel to work
on a problem, and these are the transaction

26
00:02:40,570 --> 00:02:44,040
costs. So that would be one way of determining
the boundaries.

27
00:02:44,040 --> 00:02:48,440
The other is figuring out whether to do one
thing or another. These are the opportunity

28
00:02:48,440 --> 00:02:54,380
costs. So, for example, if we were thinking
about studying perpetrators, we could get

29
00:02:54,380 --> 00:02:59,730
some really valuable information from them.
But it would also potentially be quite dangerous

30
00:02:59,730 --> 00:03:04,600
and quite time consuming. And we might find
that we spend an awful lot of money just trying

31
00:03:04,600 --> 00:03:10,030
to make contact with people, and expend all
our resources in the logistics, without getting

32
00:03:10,030 --> 00:03:15,169
anything useful out of it. So that's an example
of opportunity costs. And therefore, we might

33
00:03:15,169 --> 00:03:22,169
decide not to work with perpetrators, but
to do something else instead.

34
00:03:22,260 --> 00:03:28,010
In terms of thinking about boundaries, Gerald
Midgley has also given us a third type of

35
00:03:28,010 --> 00:03:33,660
boundary to think about. In addition to inclusion
and exclusion, he says that it's worth thinking

36
00:03:33,660 --> 00:03:40,660
about marginalization. This is when something
is not fully included or fully excluded. So,

37
00:03:40,900 --> 00:03:46,570
let's say we're trying to understand how to
deal with homeless youth. So we might work

38
00:03:46,570 --> 00:03:50,669
with a whole range of service providers, and
talk to them at great length about what they

39
00:03:50,669 --> 00:03:56,880
do for homeless youth, but we might only have
one small focus group with young people who

40
00:03:56,880 --> 00:04:01,910
are homeless. In that case, you might say
that homeless youth are marginalized in this

41
00:04:01,910 --> 00:04:05,790
kind of work.
The other thing that Gerald Midgley's done,

42
00:04:05,790 --> 00:04:11,630
is he's thought quite a lot about the role
of values in determining what gets included,

43
00:04:11,630 --> 00:04:16,650
what gets excluded, and what gets marginalized.
I'm not going to talk about that in more depth

44
00:04:16,650 --> 00:04:21,310
here, but if you're interested you can follow
up with his work.

45
00:04:21,310 --> 00:04:26,440
So the last thing I want to say about boundaries,
is to reflect again on the inevitability of

46
00:04:26,440 --> 00:04:32,750
imperfection that I raised in week one. In
that set of lectures, I talked about that

47
00:04:32,750 --> 00:04:37,100
there are three reasons for imperfection.
The first, the more that we know, the more

48
00:04:37,100 --> 00:04:41,410
we're aware of what we don't know. So we might
think that by gathering more knowledge, we're

49
00:04:41,410 --> 00:04:45,160
getting closer to the truth, but in fact,
often we just find out that there's a lot

50
00:04:45,160 --> 00:04:51,380
more that we don't know. The fact that there
will always be unknown unknowns, and finally,

51
00:04:51,380 --> 00:04:56,310
that complex problems are often wicked problems,
and have no perfect solution. So we add to

52
00:04:56,310 --> 00:05:00,620
that the fact that complex problems are big
problems, and we have to set boundaries for

53
00:05:00,620 --> 00:05:04,460
dealing with them, and therefore we can't
know everything, and that's another reason

54
00:05:04,460 --> 00:05:09,340
for imperfection.
Let's move on now to the second issue that

55
00:05:09,340 --> 00:05:14,370
I want to deal with based on Mike's lectures,
which is collaboration. Which I think is useful

56
00:05:14,370 --> 00:05:19,410
to think about in terms of harnessing and
managing differences. There are reasons why

57
00:05:19,410 --> 00:05:23,930
we collaborate, bringing together different
skills or attributes, and they're the things

58
00:05:23,930 --> 00:05:29,900
that we want to harness. There are also things
that happen in collaborations: we don't just

59
00:05:29,900 --> 00:05:34,160
come with useful attributes, we also come
with other attributes that might get in the

60
00:05:34,160 --> 00:05:39,560
way of collaborating, and they're the things
that have to be managed. So whether something

61
00:05:39,560 --> 00:05:46,430
is useful, problematic or irrelevant depends
on the context. Let's take, for example, quantitative

62
00:05:46,430 --> 00:05:52,000
and qualitative skills. In a group of collaborators,
some have quantitative skills, some have qualitative

63
00:05:52,000 --> 00:05:56,390
skills; for some collaborations, that's going
to be the reason for collaborating, and that's

64
00:05:56,390 --> 00:06:01,200
going to be what you want harnessed. In other
collaborations, that's not going to be useful

65
00:06:01,200 --> 00:06:05,020
at all, and it's going to be something that
you're going to have to manage.

66
00:06:05,020 --> 00:06:09,210
Personal attributes are the same. So, some
collaborators will be morning people, some

67
00:06:09,210 --> 00:06:14,000
will be night owls. In some collaborations,
that difference is going to be useful, and

68
00:06:14,000 --> 00:06:17,840
it'll be something you want to work with and
harness. In other collaborations, that's going

69
00:06:17,840 --> 00:06:22,500
to get in the way, and be difficult to find
meeting times, for example, and it's going

70
00:06:22,500 --> 00:06:27,180
to be something that you want to manage.
The two issues that Mike covered in these

71
00:06:27,180 --> 00:06:34,080
lectures that are relevant here are creativity
and decisiveness. Let's think about decisiveness,

72
00:06:34,080 --> 00:06:38,240
for example, imagine somebody who's decisive
trying to work with somebody who's indecisive.

73
00:06:38,240 --> 00:06:43,480
You can imagine that that might lead to a
bit of irritation and frustration.

74
00:06:43,480 --> 00:06:48,500
So there are other things that are also important
in managing collaborations, some of which,

75
00:06:48,500 --> 00:06:54,050
again, we touched on last week. Last week,
we talked about the importance of tacit knowledge,

76
00:06:54,050 --> 00:06:57,970
and the different assumptions that people
bring to collaborations, and how they can

77
00:06:57,970 --> 00:07:02,639
be important in terms of whether they're different
cultural assumptions, different disciplinary

78
00:07:02,639 --> 00:07:06,500
or professional assumptions, and that when
we're filling in the blanks, and when we're

79
00:07:06,500 --> 00:07:12,030
trying to make sense of ambiguous situations,
those assumptions can get in the way of clear

80
00:07:12,030 --> 00:07:17,730
communication. And last week, we also talked
about sensitivity to taboos. The fact, while

81
00:07:17,730 --> 00:07:23,550
this is less common, the fact that some things
might be taboos for some people in a collaboration,

82
00:07:23,550 --> 00:07:29,330
means that you have to be quite careful about
what you force, what you try to force people

83
00:07:29,330 --> 00:07:35,080
to do. This week, we uncovered a couple of
other things that leaders, in particular,

84
00:07:35,080 --> 00:07:39,800
who are managing collaborations, need to be
aware of. And that's sensitivity to the fact

85
00:07:39,800 --> 00:07:44,790
that some people may not want to know things.
So, while we might be quite comfortable knowing

86
00:07:44,790 --> 00:07:50,790
all sorts of sensitive information, other
people might not want to know about genetic

87
00:07:50,790 --> 00:07:56,060
testing, or other issues like that, and we
need to be quite careful about what we impose

88
00:07:56,060 --> 00:08:02,120
on our collaborators. We also need to be careful,
not just about imposing knowledge, but also

89
00:08:02,120 --> 00:08:09,120
about imposing ignorance. So when we're -- ah--
being less than honest about something, or

90
00:08:10,000 --> 00:08:14,419
whether when we're withholding information,
that can actually end up having quite important

91
00:08:14,419 --> 00:08:20,600
ramifications in a collaboration. And so those
things have to be done with a lot of care.

92
00:08:20,600 --> 00:08:25,300
What this illustrates is that it's not just
the problem that's complex, but it's also

93
00:08:25,300 --> 00:08:30,260
the team dynamics that are quite complex.
And some people might find that quite frustrating.

94
00:08:30,260 --> 00:08:35,690
But, in fact, that's part of what makes working
with complex problems so interesting. It's

95
00:08:35,690 --> 00:08:40,560
not just finding out a lot of interesting
and diverse things about the problem itself,

96
00:08:40,560 --> 00:08:46,610
but it's also the process of working with
a team of people that is also quite challenging,

97
00:08:46,610 --> 00:08:53,610
and quite fascinating, I have to say.
So, in wrapping up this week, can I invite

98
00:08:55,060 --> 00:08:59,930
you to also contribute to the discussion forum,
in terms of other insights that you got for

99
00:08:59,930 --> 00:09:04,450
dealing with complex problems from this week's
lectures. And, in particular, if you've got

100
00:09:04,450 --> 00:09:09,870
any experience to share on using improvisation;
you might have been using improvisation in

101
00:09:09,870 --> 00:09:14,750
an emergency situation, for example, or some
other way of using it in a complex problem.

102
00:09:14,750 --> 00:09:17,900
It would be great if you'd share that in this
week's discussion.

