0
00:00:00,000 --> 00:00:00,510
https://youtu.be/VvzikKOTAKw

1
00:00:00,510 --> 00:00:03,780
In this lecture, we'll introduce exponential smoothing.

2
00:00:03,780 --> 00:00:06,620
And specifically, we'll look at exponential smoothing

3
00:00:06,620 --> 00:00:09,950
for both level and trend patterns in data.

4
00:00:09,950 --> 00:00:12,900
But before we do this, let's look back at the forecasting

5
00:00:12,900 --> 00:00:14,814
models we've already explored.

6
00:00:14,814 --> 00:00:16,980
Well, there were differences in the different models

7
00:00:16,980 --> 00:00:18,100
that we looked at.

8
00:00:18,100 --> 00:00:20,500
They all treated history somewhat the same.

9
00:00:20,500 --> 00:00:23,390
Now, they treated the amount of history differently.

10
00:00:23,390 --> 00:00:26,170
But how they treated them were very equal.

11
00:00:26,170 --> 00:00:28,510
So for example, when I look at the cumulative

12
00:00:28,510 --> 00:00:32,009
in moving average models, they applied equal weighting

13
00:00:32,009 --> 00:00:33,360
to all the observations.

14
00:00:33,360 --> 00:00:35,090
Now, they considered different amounts.

15
00:00:35,090 --> 00:00:39,750
So cumulative, considered t, all the data that you had.

16
00:00:39,750 --> 00:00:43,370
And moving average, considered M, the last M periods of data.

17
00:00:43,370 --> 00:00:48,070
But in each case, it treated either the first or the oldest

18
00:00:48,070 --> 00:00:49,630
data equally.

19
00:00:49,630 --> 00:00:51,660
They were all treated the same.

20
00:00:51,660 --> 00:00:55,110
Now, at the other extreme we had the naive model.

21
00:00:55,110 --> 00:00:57,010
And this only had one observation.

22
00:00:57,010 --> 00:01:00,250
And it gave 100% of its value of the weighting

23
00:01:00,250 --> 00:01:02,870
to the most recent observation.

24
00:01:02,870 --> 00:01:07,520
So between these two, where we have over here,

25
00:01:07,520 --> 00:01:12,970
you have the equal weighting versus the 100% on the latest.

26
00:01:12,970 --> 00:01:15,046
Maybe there's something in between these.

27
00:01:15,046 --> 00:01:16,170
And so that's the question.

28
00:01:16,170 --> 00:01:18,460
Is there something in between these extremes

29
00:01:18,460 --> 00:01:20,525
in how we treat the data?

30
00:01:20,525 --> 00:01:21,900
And so the question is, should we

31
00:01:21,900 --> 00:01:24,980
treat the historical data differently as it ages?

32
00:01:24,980 --> 00:01:28,330
Does the value of data degrade over time?

33
00:01:28,330 --> 00:01:30,910
And should we weight newer observations

34
00:01:30,910 --> 00:01:33,010
more than the older ones?

35
00:01:33,010 --> 00:01:35,890
And this is exactly what exponential smoothing does.

36
00:01:35,890 --> 00:01:37,650
Each observation is weighted.

37
00:01:37,650 --> 00:01:41,640
And the weights decrease exponentially as they age.

38
00:01:41,640 --> 00:01:43,320
And so this is the formula that you're

39
00:01:43,320 --> 00:01:44,479
going to get very used to.

40
00:01:44,479 --> 00:01:46,270
And let me just talk about it for a second.

41
00:01:46,270 --> 00:01:48,260
And we'll go into much more detail.

42
00:01:48,260 --> 00:01:52,130
But essentially, you had this smoothing factor alpha.

43
00:01:52,130 --> 00:01:55,240
And this alpha will range between 0 and 1.

44
00:01:55,240 --> 00:01:57,360
And essentially, this is a weighting factor

45
00:01:57,360 --> 00:01:59,990
for how much I'm going to value my xt, which

46
00:01:59,990 --> 00:02:02,090
is my latest observation.

47
00:02:02,090 --> 00:02:08,770
And then 1 minus alpha is how much my historical forecast is.

48
00:02:08,770 --> 00:02:10,770
So what is this equation saying?

49
00:02:10,770 --> 00:02:13,350
Remember, x hat means it's a forecast,

50
00:02:13,350 --> 00:02:17,870
while the actual x without a hat is the actual observation.

51
00:02:17,870 --> 00:02:20,860
So this is saying that the forecast starting

52
00:02:20,860 --> 00:02:25,050
in time t for time t plus 1, essentially sitting

53
00:02:25,050 --> 00:02:27,270
in this period for the next period,

54
00:02:27,270 --> 00:02:29,150
is equal to alpha-- that smoothing

55
00:02:29,150 --> 00:02:33,670
factor-- times my most recent observation xt.

56
00:02:33,670 --> 00:02:36,930
So that's new information, plus 1

57
00:02:36,930 --> 00:02:41,360
minus alpha times my most recent forecast.

58
00:02:41,360 --> 00:02:46,650
And that's what I forecasted in time t minus 1 for t.

59
00:02:46,650 --> 00:02:50,250
So if you think of t as today, then this equation

60
00:02:50,250 --> 00:02:54,390
is looking at the forecast sitting in today for tomorrow

61
00:02:54,390 --> 00:02:58,400
and is equal to alpha times what happened today times 1 minus

62
00:02:58,400 --> 00:03:02,560
alpha for what I forecasted yesterday for today.

63
00:03:02,560 --> 00:03:05,250
So you might ask yourself, why is this called

64
00:03:05,250 --> 00:03:07,710
exponential smoothing?

65
00:03:07,710 --> 00:03:10,740
Well, if we look at the equation and we kind of explode

66
00:03:10,740 --> 00:03:13,850
it a little bit, and so we look at this term

67
00:03:13,850 --> 00:03:16,900
back here-- which is, remember, the forecast in time t

68
00:03:16,900 --> 00:03:21,006
minus 1 for period t-- well, we know what that's going to be.

69
00:03:21,006 --> 00:03:24,330
That's going to be equal to alpha times what actually

70
00:03:24,330 --> 00:03:26,980
happened in t minus 1, yesterday,

71
00:03:26,980 --> 00:03:29,940
times 1 minus alpha times the forecast of what

72
00:03:29,940 --> 00:03:33,870
happened essentially two days ago for yesterday.

73
00:03:33,870 --> 00:03:36,220
So we can keep working backwards.

74
00:03:36,220 --> 00:03:44,532
And here, if I plug this back in for this, I get this term.

75
00:03:44,532 --> 00:03:46,240
And then I just simplify it a little bit,

76
00:03:46,240 --> 00:03:47,260
clean it up a little bit.

77
00:03:47,260 --> 00:03:49,259
Pause the video if you need to see how it works.

78
00:03:49,259 --> 00:03:51,510
It's just straight algebra.

79
00:03:51,510 --> 00:03:53,350
But then we can continue to substitute.

80
00:03:53,350 --> 00:03:56,470
So I find what the real-- what the equation is for this

81
00:03:56,470 --> 00:03:57,310
estimate.

82
00:03:57,310 --> 00:04:00,860
And I forecast, and I add that back in, and keep

83
00:04:00,860 --> 00:04:02,540
continuing to substitute.

84
00:04:02,540 --> 00:04:03,850
What do I end up with?

85
00:04:03,850 --> 00:04:06,160
Well I end up with this general form.

86
00:04:06,160 --> 00:04:10,680
Let me just clean some of this up so it's not distracting.

87
00:04:10,680 --> 00:04:14,180
And so what is this saying down here?

88
00:04:14,180 --> 00:04:16,950
It's saying that the forecast sitting in time

89
00:04:16,950 --> 00:04:19,850
t for time t plus 1.

90
00:04:19,850 --> 00:04:21,970
So it's the forecast sitting in today

91
00:04:21,970 --> 00:04:25,670
for tomorrow is equal to alpha times 1 minus alpha

92
00:04:25,670 --> 00:04:29,640
to the 0th, which reduces to 1, times what happened today,

93
00:04:29,640 --> 00:04:30,140
the xt.

94
00:04:30,140 --> 00:04:32,750


95
00:04:32,750 --> 00:04:36,090
Plus alpha times 1 minus alpha to the 1st times what

96
00:04:36,090 --> 00:04:39,320
happened yesterday, xt minus 1.

97
00:04:39,320 --> 00:04:42,800
Plus alpha times 1 minus alpha squared

98
00:04:42,800 --> 00:04:48,020
times what happened at xt minus 2, two days ago, and so forth.

99
00:04:48,020 --> 00:04:49,087
You can see a pattern.

100
00:04:49,087 --> 00:04:51,170
You should actually call this geometric smoothing,

101
00:04:51,170 --> 00:04:53,680
because this is a geometric progression.

102
00:04:53,680 --> 00:04:57,370
So what I want you to see is as I get older in time--

103
00:04:57,370 --> 00:05:00,840
and these are the observations, this xt, xt minus 1,

104
00:05:00,840 --> 00:05:05,270
xt minus 2, xt minus 3, these are not the forecasts.

105
00:05:05,270 --> 00:05:08,590
These are what actually happened at those times.

106
00:05:08,590 --> 00:05:11,200
And so as I go to the right here in this equation,

107
00:05:11,200 --> 00:05:15,670
as I go this way-- and this would keep on going forever--

108
00:05:15,670 --> 00:05:18,320
then that means the x's are older.

109
00:05:18,320 --> 00:05:20,070
They're older observations.

110
00:05:20,070 --> 00:05:24,030
And so the coefficients, you can see, get smaller.

111
00:05:24,030 --> 00:05:26,900
Alpha times 1 minus alpha squared

112
00:05:26,900 --> 00:05:29,760
is bigger than alpha times 1 minus alpha to the cube,

113
00:05:29,760 --> 00:05:30,860
and so forth.

114
00:05:30,860 --> 00:05:33,850
So as I get older, that coefficient gets smaller.

115
00:05:33,850 --> 00:05:35,540
So this is the weighting.

116
00:05:35,540 --> 00:05:40,660
So you might ask yourself, well, how much is that weighting.

117
00:05:40,660 --> 00:05:42,710
Well, if we look at different alphas--

118
00:05:42,710 --> 00:05:45,990
and here you can see each column is a different alpha,

119
00:05:45,990 --> 00:05:49,560
alpha 0.2, alpha 0.4, alpha 0.6--

120
00:05:49,560 --> 00:05:53,200
and each row is aged observation.

121
00:05:53,200 --> 00:05:54,670
So t is the most recent.

122
00:05:54,670 --> 00:05:58,320
Then t minus 1 is one period ago, two periods ago,

123
00:05:58,320 --> 00:06:00,880
three periods ago, so forth.

124
00:06:00,880 --> 00:06:04,470
If I have an alpha equals 0.2, then I

125
00:06:04,470 --> 00:06:08,430
treat the most recent observation worth 20%.

126
00:06:08,430 --> 00:06:12,460
And then a combined 80% is for all the history.

127
00:06:12,460 --> 00:06:16,390
And you can see how it works down is what the ratios are,

128
00:06:16,390 --> 00:06:17,820
what it turns into.

129
00:06:17,820 --> 00:06:23,320
So essentially, this term is t minus 1 rho

130
00:06:23,320 --> 00:06:28,920
is essentially saying alpha of 0.2 is alpha 0.2 times 1 minus

131
00:06:28,920 --> 00:06:33,040
0.2, 0.8 to the first.

132
00:06:33,040 --> 00:06:36,080
And so you can see what each of these are for the coefficients.

133
00:06:36,080 --> 00:06:39,830
So now, as you see how the alpha gets bigger,

134
00:06:39,830 --> 00:06:41,100
what is that doing?

135
00:06:41,100 --> 00:06:43,090
Well, it's putting more weight on the more

136
00:06:43,090 --> 00:06:44,780
recent observations.

137
00:06:44,780 --> 00:06:48,800
You see for observation t, an alpha of 0.6

138
00:06:48,800 --> 00:06:53,150
gives 60% of the weight to the most recent observation,

139
00:06:53,150 --> 00:07:00,370
while an alpha of 0.2 gives only a 0.2, or 20%.

140
00:07:00,370 --> 00:07:05,230
And so if I looked down to, say, the fifth oldest observation,

141
00:07:05,230 --> 00:07:07,950
or t minus 5, you can see what this translates into.

142
00:07:07,950 --> 00:07:12,010
It gets progressively less weight.

143
00:07:12,010 --> 00:07:16,120
For an alpha equals 0.2, it's 0.06.

144
00:07:16,120 --> 00:07:18,880
For an alpha 0.6, because it puts so much weight up here

145
00:07:18,880 --> 00:07:24,100
on the most recent, it goes down to the 0.006144.

146
00:07:24,100 --> 00:07:26,630
So what is this alpha doing?

147
00:07:26,630 --> 00:07:28,740
This alpha is essentially saying how much

148
00:07:28,740 --> 00:07:31,300
should I value, how much should I weight the more

149
00:07:31,300 --> 00:07:34,490
recent information-- oops, let me go back--

150
00:07:34,490 --> 00:07:36,750
versus the older data.

151
00:07:36,750 --> 00:07:42,110
And so the alpha is the tool, the smoothing tool.

152
00:07:42,110 --> 00:07:44,030
So if I want to see this graphically,

153
00:07:44,030 --> 00:07:45,540
here I have the age of observation.

154
00:07:45,540 --> 00:07:49,240
So this is the-- this would be x sub t.

155
00:07:49,240 --> 00:07:53,600
This is t minus 1, minus 2, minus 3, and so forth.

156
00:07:53,600 --> 00:07:56,380
And on the vertical axis is the effective weight.

157
00:07:56,380 --> 00:07:59,600
You can see how for an alpha of 0.1--

158
00:07:59,600 --> 00:08:02,250
that's that heavy dark line that's right here.

159
00:08:02,250 --> 00:08:05,570
At x sub t, which is the 0 here for the age,

160
00:08:05,570 --> 00:08:08,580
it's the most recent, it's 0.1.

161
00:08:08,580 --> 00:08:10,300
The next one is an alpha of 0.3.

162
00:08:10,300 --> 00:08:13,742
And you can see for the most recent period, it's a 0.3.

163
00:08:13,742 --> 00:08:15,700
Let me just bring some of these other ones out.

164
00:08:15,700 --> 00:08:21,690
This is an alpha 0.5, an alpha of 0.7, and an alpha of 0.9.

165
00:08:21,690 --> 00:08:23,340
What's happening here?

166
00:08:23,340 --> 00:08:27,310
As my alpha gets bigger, I put more of my weighting

167
00:08:27,310 --> 00:08:29,830
on the more recent data.

168
00:08:29,830 --> 00:08:34,809
I say for an alpha of 0.9, I'm saying 90% of my forecast

169
00:08:34,809 --> 00:08:37,460
is going to be what happened in the most recent period, what

170
00:08:37,460 --> 00:08:39,230
happened yesterday.

171
00:08:39,230 --> 00:08:42,960
So if you think about this, what this alpha value is doing,

172
00:08:42,960 --> 00:08:44,580
it's really a trade-off. off.

173
00:08:44,580 --> 00:08:47,680
And it's trading off the new information, the weighting

174
00:08:47,680 --> 00:08:50,880
of the new information I have, any new observations

175
00:08:50,880 --> 00:08:54,070
with the old information, stuff that I had in the past.

176
00:08:54,070 --> 00:08:58,570
Because this term, this x hat t minus 1t,

177
00:08:58,570 --> 00:09:01,660
encapsulates everything in my history.

178
00:09:01,660 --> 00:09:03,910
This is a beautiful thing about exponential smoothing,

179
00:09:03,910 --> 00:09:07,790
because I only have to capture and track two pieces of data--

180
00:09:07,790 --> 00:09:10,960
my most recent and then my most recent forecast.

181
00:09:10,960 --> 00:09:14,360
Because this x hat t minus 1t term

182
00:09:14,360 --> 00:09:18,200
captures all the other old historical data.

183
00:09:18,200 --> 00:09:21,890
And so what the alpha does, one way to think about it

184
00:09:21,890 --> 00:09:24,830
is think about it as it approaches its limits, either 0

185
00:09:24,830 --> 00:09:25,850
or 1.

186
00:09:25,850 --> 00:09:28,540
When alpha hits 1, what's going to happen?

187
00:09:28,540 --> 00:09:29,850
We're almost naive.

188
00:09:29,850 --> 00:09:31,650
We're leading to fast smoothing.

189
00:09:31,650 --> 00:09:34,190
It's going to be very nervous, volatile.

190
00:09:34,190 --> 00:09:36,760
It's that naive kind of forecast.

191
00:09:36,760 --> 00:09:41,090
As it goes the other way and approaches 0, it's slower.

192
00:09:41,090 --> 00:09:43,400
It's calm, it's stayed, it's closer

193
00:09:43,400 --> 00:09:45,910
to that cumulative forecast where things get weighted,

194
00:09:45,910 --> 00:09:48,840
not equally, but much more equally

195
00:09:48,840 --> 00:09:52,070
than if I had a 0.9, which puts all the weight up

196
00:09:52,070 --> 00:09:55,560
on the front at the newer information.

197
00:09:55,560 --> 00:09:57,690
So essentially, you can think of the alpha

198
00:09:57,690 --> 00:10:01,300
as controlling the speed of the forecasting.

199
00:10:01,300 --> 00:10:03,280
And we'll use this and talk about this a lot

200
00:10:03,280 --> 00:10:06,950
as we develop the rest of the exponential smoothing models.

