0
00:00:04,220 --> 00:00:09,350
https://youtu.be/k7kUWilEPvU
Now Steve has been doing this work for a while, looking at the nature of conspiracy theories

1
00:00:09,350 --> 00:00:13,100
and how they&#39;re operating, and he mentioned a couple of the psychological mechanisms that

2
00:00:13,100 --> 00:00:19,470
are operating. One is related to the confirmation bias when you&#39;re only looking at the evidence

3
00:00:19,470 --> 00:00:26,500
that supports what you want to believe, seeing what you expect to see, seeing what you want to see, in a sense.

4
00:00:26,500 --> 00:00:33,519
When it comes to climate-change denial, climate-change deniers actually only cite the evidence that

5
00:00:34,470 --> 00:00:39,000
doesn&#39;t support the fact that the climate is changing due to human involvement and so on.

6
00:00:39,000 --> 00:00:42,130
Paying attention to that one thermometer and ignoring the other.

7
00:00:42,130 --> 00:00:47,710
That&#39;s right. All the billions of thermometers out there just focusing on the one that supports

8
00:00:47,710 --> 00:00:52,610
the evidence that you&#39;re looking for, yes, and kind of picturing it in a two-by-two table—you&#39;re

9
00:00:52,610 --> 00:00:59,610
only focusing on the positive-positive cell. That&#39;s the confirmation bias in a sense and

10
00:01:00,070 --> 00:01:02,710
cherry-picking the evidence that you&#39;re looking for.

11
00:01:02,710 --> 00:01:08,090
This is related to a really nice paper by Hastorf and Cantril. What they did back in

12
00:01:08,090 --> 00:01:15,090
1954, what they did was they essentially interviewed fans of a football team. There&#39;s a football

13
00:01:16,380 --> 00:01:22,120
game, football match that was happening, and you have fans watching exactly the same football

14
00:01:22,120 --> 00:01:29,120
game, and you ask people about their perceptions of dirty play during this football game, and

15
00:01:31,140 --> 00:01:38,140
each team reported that the other team was playing more dirty then then their own. Yes,

16
00:01:39,900 --> 00:01:45,240
they&#39;re watching exactly the same game, but they have completely different perceptions

17
00:01:45,240 --> 00:01:49,140
of what actually happened. You can imagine this. This is just at the level of a single

18
00:01:49,140 --> 00:01:52,340
game—people looking at exactly the same thing.

19
00:01:52,340 --> 00:01:58,460
This is also related to this idea of availability, in a sense. You&#39;re only exposing yourself

20
00:01:58,460 --> 00:02:05,460
to a certain type of information, so when you&#39;re reading about climate change, the news

21
00:02:05,619 --> 00:02:12,619
that you get is always based on a subset of news in some respects. People who read Facebook

22
00:02:13,450 --> 00:02:18,770
are only exposed to information from their friends on Facebook, very like-minded individuals.

23
00:02:18,770 --> 00:02:24,770
In fact, things like Facebook and Google only show you the things that you like and therefore

24
00:02:24,770 --> 00:02:27,620
probably agree with, which is going to bias you even further.

25
00:02:27,620 --> 00:02:32,640
Exactly, as Steve was talking about. Our news is being shaped now that the information we

26
00:02:32,640 --> 00:02:38,900
get on the Web is being more and more encapsulated, so when you do a Google search, it&#39;s catered

27
00:02:38,900 --> 00:02:45,000
to you, in a sense. The information that it pops up is not the information that I would

28
00:02:45,000 --> 00:02:49,630
get when I do the same search, which is fairly strange—same as Facebook and everything

29
00:02:49,630 --> 00:02:56,630
else. We&#39;re, in a sense, suffering from what&#39;s called false consensus. You have this perception

30
00:03:00,920 --> 00:03:03,989
that other people think the same way that you do.

31
00:03:03,989 --> 00:03:07,920
Now we asked a really nice question at the beginning of this episode, beginning of the

32
00:03:07,920 --> 00:03:13,879
course, rather, in the About You section. We asked people whether they&#39;ve had anything

33
00:03:13,879 --> 00:03:20,560
strange happen to them that science can&#39;t really explain. We asked people that question,

34
00:03:20,560 --> 00:03:26,110
and half of the people in the course, tens of thousands of people said yes. Also, tens

35
00:03:26,110 --> 00:03:31,319
of thousands people said no, so roughly 50/50. We had a nice split between the two. Half

36
00:03:31,319 --> 00:03:35,060
the people said, &quot;Yes, something weird has happened to me that science can&#39;t explain,&quot;

37
00:03:35,060 --> 00:03:40,599
and half the people said, &quot;No, nothing like that has happened to me.&quot;

38
00:03:40,599 --> 00:03:44,800
Now that&#39;s interesting in and of itself, but the very next question we asked people to

39
00:03:44,800 --> 00:03:51,800
guess what percentage of the class agrees with them. What percentage of the class also

40
00:03:52,709 --> 00:03:59,069
said yes or also said no. What we did was we compared the results. Of the people who

41
00:03:59,069 --> 00:04:05,900
said, yes, that something weird has happened to them, they estimated that 69 percent of

42
00:04:05,900 --> 00:04:11,769
people agreed with them. The opposite is true. The people who said, &quot;No, nothing weird has

43
00:04:11,769 --> 00:04:18,769
ever happened to me,&quot; said that 64 percent of people agreed with them. This is massive.

44
00:04:18,779 --> 00:04:23,900
I didn&#39;t expect actually the false consensus effect to be so big in this case, but it&#39;s

45
00:04:23,900 --> 00:04:30,900
huge. People literally think that people agree with them. They think that, by virtue of being

46
00:04:31,600 --> 00:04:36,060
exposed to the same sort of information, that other people have the same sort of life experiences

47
00:04:36,060 --> 00:04:39,740
and other people around the world think the same way that they do.

48
00:04:39,740 --> 00:04:46,150
Yes, and scale that up, so the influence of the media, the information you expose yourself

49
00:04:46,150 --> 00:04:53,150
to, the more narrowly you focus on where you get your information, the more and more you&#39;re

50
00:04:53,940 --> 00:04:59,229
going to be reinforced and think that people agree with you; more and more people think

51
00:04:59,229 --> 00:05:05,700
the same way as I do. It&#39;s almost like an availability cascade. It just keeps going and going.

52
00:05:05,740 --> 00:05:11,590
That&#39;s exactly right. In terms of narrowness, if all you&#39;re doing—this is the nature of

53
00:05:11,590 --> 00:05:18,100
conspiracy theories. If I&#39;m a climate-change denier, it&#39;s very unlikely that I&#39;m reading

54
00:05:18,100 --> 00:05:25,100
the scientific evidence for and against climate change. I&#39;m focused on this one small bit

55
00:05:25,660 --> 00:05:31,250
of evidence. I surround myself with like-minded people who also cite the same sort of evidence.

56
00:05:31,250 --> 00:05:34,639
It just keeps building and propagating, exactly as you said.

57
00:05:34,639 --> 00:05:39,759
This, and the anti-establishment bias is operating as well. If the government or an official

58
00:05:39,759 --> 00:05:44,850
body releases some information and says, &quot;Here are the data. This is what&#39;s happening in

59
00:05:44,850 --> 00:05:50,840
the case,&quot; well, that&#39;s not just ignored. It&#39;s taken as evidence that the opposite is

60
00:05:50,840 --> 00:05:56,009
happening. If the authority or if the official body says one thing, then it&#39;s probably actually

61
00:05:56,009 --> 00:06:01,740
the opposite thing that&#39;s going on. It&#39;s going to be really difficult to change your mind,

62
00:06:01,740 --> 00:06:07,259
probably unless you have some help. Unless somebody is helping you through the six leads

63
00:06:07,259 --> 00:06:13,220
to find out whether you are focusing on particular information, falling prey to the confirmation

64
00:06:13,220 --> 00:06:17,700
bias, it&#39;s going to be really difficult to change your mind and get out of that sort of thinking.

65
00:06:17,710 --> 00:06:22,229
This is related to two other biases that are somewhat related when we&#39;re dealing with issues

66
00:06:22,229 --> 00:06:28,669
in the media. One is called the it-must-be-in-the-middle heuristic, in a sense. You can imagine what&#39;s

67
00:06:28,669 --> 00:06:35,630
happening here. Someone presents one thing; someone presents something that&#39;s the opposite,

68
00:06:35,630 --> 00:06:39,090
and the person who&#39;s listening casually just goes, &quot;Yes, well, there&#39;s a bit of truth in

69
00:06:39,090 --> 00:06:44,880
both sides. It must be somewhere in the middle.&quot; Maybe, but in a lot of cases, it&#39;s not somewhere

70
00:06:44,880 --> 00:06:49,289
in the middle. If I say that, &quot;On one hand, the UFO was big. On the other hand, it was

71
00:06:49,289 --> 00:06:55,250
small. It must be medium-sized,&quot; it might not be true at all.

72
00:06:55,250 --> 00:06:59,949
A lot of people would argue in the case of climate change that&#39;s exactly what&#39;s happening.

73
00:06:59,949 --> 00:07:06,949
You say that the vast majority of the scientists around the world, 99.9 percent of scientists,

74
00:07:07,340 --> 00:07:11,979
think that something is happening due to humans that&#39;s changing the climate. On the other

75
00:07:11,979 --> 00:07:17,169
hand, when you&#39;re covering this sort of thing in the media, you bring on a climate-change

76
00:07:17,169 --> 00:07:25,000
denier and the casual listener would say, &quot;Well, it must be happening. Maybe, maybe not, somewhere in the middle.&quot;

77
00:07:25,069 --> 00:07:32,069
That&#39;s related to a second bias called &quot;be-fair-to-both-sides&quot;. The media is trying to be fair and balanced,

78
00:07:35,270 --> 00:07:40,550
and so they include both. You can see how these two go hand in hand. If you&#39;re being

79
00:07:40,550 --> 00:07:46,110
fair to both sides, then you feature both, and thinking about it in terms of availability,

80
00:07:46,110 --> 00:07:50,600
then you&#39;re giving both of these things equal weight and people think they are much more common than they are.

81
00:07:50,660 --> 00:07:54,930
Yes. We have tried to apply everything we&#39;ve learned so far in the course to three specific

82
00:07:54,930 --> 00:08:00,789
examples in this episode. We looked at facilitated communication, expertise in forensic science,

83
00:08:00,789 --> 00:08:05,490
and conspiracy theories, especially about climate change. Those are just three that

84
00:08:05,490 --> 00:08:12,190
we picked mostly because we&#39;re actually interested in them, but I hope that people can see that

85
00:08:12,190 --> 00:08:19,190
the mechanisms that are operating here, the tools they now have, could be applied to any

86
00:08:21,880 --> 00:08:27,440
other area, anything, their pet project that they are particularly interested in. You can

87
00:08:27,440 --> 00:08:31,870
use the six leads; you can have a look; you can try and disentangle what&#39;s operating in

88
00:08:31,870 --> 00:08:38,870
really specific cases, so gay marriage, whether to put fluoride in the water or not...

89
00:08:39,679 --> 00:08:46,679
Vegetarianism, gun laws. Absolutely. Anything that you can think of. People have the tools.

90
00:08:46,869 --> 00:08:53,300
Go out and conquer. Figure out—take a very specific example like these. Take gay marriage,

91
00:08:53,300 --> 00:08:59,569
take anything, asylum seekers, you name it. All of these very relevant topics that are

92
00:08:59,569 --> 00:09:03,410
happening in their lives, something that&#39;s important to them, and apply everything that

93
00:09:03,410 --> 00:09:07,629
we&#39;ve been teaching them to these particular areas, and they&#39;ll see it. They will absolutely

94
00:09:07,629 --> 00:09:10,660
see what&#39;s happening in each of these cases.

