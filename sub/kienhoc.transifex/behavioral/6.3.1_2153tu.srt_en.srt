1
00:00:07,549 --> 00:00:09,929
https://youtu.be/pftmprO4A5U
We're going to spend the next few minutes
talking about perhaps

2
00:00:09,929 --> 00:00:15,120
one of the most standard levers used by policymakers
all over the word--

3
00:00:15,120 --> 00:00:16,740
disclosure.

4
00:00:16,740 --> 00:00:19,800
Disclosure simply refers to the idea that
if you make information

5
00:00:19,800 --> 00:00:24,560
accessible to people, it betters people's
ability to make decisions,

6
00:00:24,560 --> 00:00:28,249
because you have told them all there is to
know about that particular

7
00:00:28,249 --> 00:00:29,249
decision-making task.

8
00:00:29,249 --> 00:00:31,199
Think about a simple example.

9
00:00:31,199 --> 00:00:34,610
The next time you go to the pharmacy to buy
medication and you open the

10
00:00:34,610 --> 00:00:38,220
packet, you're going to see a large sheet
of paper in there.

11
00:00:38,220 --> 00:00:40,989
That paper's going to tell you all of the
things that could go wrong, all of

12
00:00:40,989 --> 00:00:44,270
the side effects, and perhaps every possible
drug interaction.

13
00:00:44,270 --> 00:00:46,429
That's a disclosure.

14
00:00:46,429 --> 00:00:50,920
Or every country has a regulatory authority
that provides disclosure

15
00:00:50,920 --> 00:00:53,609
guidelines for financial markets.

16
00:00:53,609 --> 00:00:58,199
As most regulators will tell you, they would
say, my job is to make sure--

17
00:00:58,199 --> 00:01:02,059
and I paraphrase--
my job is to make sure that people have all

18
00:01:02,059 --> 00:01:05,760
the information they need in
order to understand the risks when they make

19
00:01:05,760 --> 00:01:07,450
investments.

20
00:01:07,450 --> 00:01:10,500
Any regulator will tell you that they're not
there to police what

21
00:01:10,500 --> 00:01:12,690
people should and shouldn't do.

22
00:01:12,690 --> 00:01:16,290
They don't care if people take risky decisions
or not, as long as people

23
00:01:16,290 --> 00:01:18,210
understand the risk.

24
00:01:18,210 --> 00:01:21,270
And so the goal of disclosure is simply to
make sure that people have

25
00:01:21,270 --> 00:01:22,270
information.

26
00:01:22,270 --> 00:01:25,880
Now there are many different kinds of disclosure.

27
00:01:25,880 --> 00:01:29,360
And so for example, there's mandated disclosure.

28
00:01:29,360 --> 00:01:32,720
Every government, every regulatory authority,
will require firms to

29
00:01:32,720 --> 00:01:36,830
disclose certain data when they launch products
and services and financial

30
00:01:36,830 --> 00:01:37,830
instruments.

31
00:01:37,830 --> 00:01:41,330
There are disclosures about conflicts of interest.

32
00:01:41,330 --> 00:01:42,330
You could go to a doctor.

33
00:01:42,330 --> 00:01:44,390
A doctor might prescribe a medication.

34
00:01:44,390 --> 00:01:48,140
And they might say, by the way, I got this
as a free sample from a

35
00:01:48,140 --> 00:01:49,540
pharmaceutical company.

36
00:01:49,540 --> 00:01:53,280
Or a financial adviser might disclose the
fact that he or she has the

37
00:01:53,280 --> 00:01:57,620
incentive to recommend products by a certain
company because they get a

38
00:01:57,620 --> 00:02:00,030
commission from that company.

39
00:02:00,030 --> 00:02:04,330
So a conflict of interest disclosure is one
where an agent informs the

40
00:02:04,330 --> 00:02:09,110
principal about the fact that they might have
the incentive to recommend

41
00:02:09,110 --> 00:02:10,170
certain products.

42
00:02:10,170 --> 00:02:14,060
We've also seen discussions recently about
open governments.

43
00:02:14,060 --> 00:02:17,630
President Obama in the United States has spoken
about the fact that all

44
00:02:17,630 --> 00:02:21,680
data collected by the US government should
be made accessible to the

45
00:02:21,680 --> 00:02:23,640
citizens of the United States.

46
00:02:23,640 --> 00:02:28,720
And likewise, we've heard similar disclosure
policies from several other

47
00:02:28,720 --> 00:02:29,830
governments.

48
00:02:29,830 --> 00:02:33,700
And finally, corporations often disclose all
kinds of things,

49
00:02:33,700 --> 00:02:38,740
including product-related information, accounting
reports, and so on and so

50
00:02:38,740 --> 00:02:43,200
forth, as well as plans for the future in
terms of new

51
00:02:43,200 --> 00:02:46,020
products or new services.

52
00:02:46,020 --> 00:02:49,450
It's fair to say that over the last few years,
we have now moved towards a

53
00:02:49,450 --> 00:02:54,680
regime of comprehensive disclosure, where
the idea is that the more you

54
00:02:54,680 --> 00:02:59,090
disclose, the better informed people are,
and therefore the better

55
00:02:59,090 --> 00:03:01,990
decisions they will make.

56
00:03:01,990 --> 00:03:03,060
Does disclosure work?

57
00:03:03,060 --> 00:03:06,950
Well, here are some simple human truths.

58
00:03:06,950 --> 00:03:10,870
Truth number one--
providing information doesn't mean that that

59
00:03:10,870 --> 00:03:12,980
information is read.

60
00:03:12,980 --> 00:03:16,900
Truth number two--
reading that information doesn't mean that

61
00:03:16,900 --> 00:03:21,170
the information is understood and
used in making a decision.

62
00:03:21,170 --> 00:03:24,820
Truth number three--
once you get to a certain point in time or

63
00:03:24,820 --> 00:03:29,200
a certain quantity of
information, the more information you provide,

64
00:03:29,200 --> 00:03:33,510
the more likely is it that
the information will be not used at all.

65
00:03:33,510 --> 00:03:36,980
So these are three things that we've learned
from this course that should

66
00:03:36,980 --> 00:03:39,790
worry people who are proponents of disclosure.

67
00:03:39,790 --> 00:03:44,520
If you disclose too much, chances are good
that that disclosure will not

68
00:03:44,520 --> 00:03:46,430
work at all.

69
00:03:46,430 --> 00:03:51,090
Now earlier this year, Will Tucker and Dick
Thaler wrote an interesting and

70
00:03:51,090 --> 00:03:54,110
important article in the Harvard Business
Review, and they made a

71
00:03:54,110 --> 00:03:58,840
distinction between disclosure and smart disclosure.

72
00:03:58,840 --> 00:04:00,180
So what is smart disclosure?

73
00:04:00,180 --> 00:04:03,989
Smart disclosure is simply the idea that you
can disclose as much data as

74
00:04:03,989 --> 00:04:09,069
you want, but you should provide consumers
with the tools to curate and

75
00:04:09,069 --> 00:04:13,520
summarize that data into something that's
meaningful for them.

76
00:04:13,520 --> 00:04:17,030
So rather than simply providing copious amounts
of data, can you

77
00:04:17,030 --> 00:04:21,079
provide people with engines--
and they use the term "choice engines" in

78
00:04:21,079 --> 00:04:24,669
their article--
that will take the data and present it in

79
00:04:24,669 --> 00:04:29,430
a simple, meaningful form that
actually helps the consumer make a better

80
00:04:29,430 --> 00:04:30,430
decision?

81
00:04:30,430 --> 00:04:34,020
So curating the data is key, and customizing
the data to what that

82
00:04:34,020 --> 00:04:37,180
individual needs is the second key.

83
00:04:37,180 --> 00:04:40,430
Thaler and Tucker offered a number of examples
in their articles, and I want

84
00:04:40,430 --> 00:04:43,560
to share three of them very, very briefly.

85
00:04:43,560 --> 00:04:46,430
The first one is a company called BrightScope.

86
00:04:46,430 --> 00:04:50,969
What BrightScope does is it basically collects
information from a very large

87
00:04:50,969 --> 00:04:55,530
number of 401(k) plans-- these are retirement
plans in the United States,

88
00:04:55,530 --> 00:05:00,439
about 45,000 of them--
and it takes the data and it can summarize

89
00:05:00,439 --> 00:05:04,889
them into an overall score
which captures the performance of each of

90
00:05:04,889 --> 00:05:06,330
those funds.

91
00:05:06,330 --> 00:05:08,949
It can give you the distribution of the performance
of funds.

92
00:05:08,949 --> 00:05:13,759
It will also allow the end user to compare
two or three or four specific

93
00:05:13,759 --> 00:05:18,009
funds on attributes that they think are meaningful
to them.

94
00:05:18,009 --> 00:05:22,229
So what it is is an engine that looks at a
large mass of data but allows the

95
00:05:22,229 --> 00:05:27,120
user to specify their criteria and then takes
the data and summarizes it

96
00:05:27,120 --> 00:05:31,059
for the user along those criteria.

97
00:05:31,059 --> 00:05:37,129
A second example was the example you see on
the top here, FirstFuel.

98
00:05:37,129 --> 00:05:40,589
This is a company that actually collects data
from a large number of

99
00:05:40,589 --> 00:05:44,580
utility organizations, utility companies,
electricity companies,

100
00:05:44,580 --> 00:05:47,340
water companies in the United States.

101
00:05:47,340 --> 00:05:52,340
And they will then work with a given household
and use information about

102
00:05:52,340 --> 00:05:57,469
the average cost of the utility to project
the costs of that particular

103
00:05:57,469 --> 00:05:59,449
house over a year.

104
00:05:59,449 --> 00:06:01,889
So for example, as a household, I could go
in.

105
00:06:01,889 --> 00:06:05,819
I could report one hour's worth of my own
consumption data, and it will then

106
00:06:05,819 --> 00:06:09,979
project that data into what my annual bills
are going to look like.

107
00:06:09,979 --> 00:06:14,750
It could then make recommendations as to what
I could cut down on to make my

108
00:06:14,750 --> 00:06:18,050
energy consumption more efficient.

109
00:06:18,050 --> 00:06:22,340
Tesco is a retailer in the United Kingdom,
and they just recently

110
00:06:22,340 --> 00:06:25,240
announced that they would make shopping histories
of a given

111
00:06:25,240 --> 00:06:27,219
individual available to that individual.

112
00:06:27,219 --> 00:06:31,479
So if you simply want to go back and look
at your shopping history over the

113
00:06:31,479 --> 00:06:34,729
past three or four years to see how much cheese
you've consumed or how

114
00:06:34,729 --> 00:06:38,059
much yogurt you've bought, you will now be
allowed to do that under

115
00:06:38,059 --> 00:06:40,409
Tesco's pilot program.

116
00:06:40,409 --> 00:06:46,219
These are all examples of initiatives where
disclosure is simply not

117
00:06:46,219 --> 00:06:50,289
providing people with humongous amount of
data, but allowing that person to

118
00:06:50,289 --> 00:06:55,469
specify what data they want and to curate
and then simplify

119
00:06:55,469 --> 00:06:57,560
that data for them.

120
00:06:57,560 --> 00:07:00,520
Let's turn our attention onto a second kind
of disclosure for a moment, and

121
00:07:00,520 --> 00:07:03,259
that's the disclosure of conflicts of interest.

122
00:07:03,259 --> 00:07:07,960
And I want to talk about a very simple example
of a study that was done by

123
00:07:07,960 --> 00:07:11,939
Sunita Sah and her colleagues in a recent
paper.

124
00:07:11,939 --> 00:07:15,139
Let's imagine I give you the choice between
playing one of two games.

125
00:07:15,139 --> 00:07:19,889
You can either choose the blue dice that you
see on the screen or the red

126
00:07:19,889 --> 00:07:23,930
dice, and I give you a schedule which tells
you what the prizes are.

127
00:07:23,930 --> 00:07:25,389
You're going to roll the dice.

128
00:07:25,389 --> 00:07:27,789
And after you roll it, one of six numbers
is going to show up.

129
00:07:27,789 --> 00:07:30,990
For every number that shows up, I'm going
to give you a list of prizes

130
00:07:30,990 --> 00:07:33,089
that you could win.

131
00:07:33,089 --> 00:07:39,219
And imagine for now that the blue dice dominates
the red dice.

132
00:07:39,219 --> 00:07:42,590
It makes perfect sense for you to choose the
blue dice, because overall

133
00:07:42,590 --> 00:07:47,219
the quality of prizes and the value of the
prizes is higher in the blue as

134
00:07:47,219 --> 00:07:48,529
opposed to the red.

135
00:07:48,529 --> 00:07:49,639
Here's a catch.

136
00:07:49,639 --> 00:07:54,069
You've going to get advice from an adviser
as to which dice to roll, and

137
00:07:54,069 --> 00:07:58,499
I'm going to set it up so that the adviser
gets cash if, in fact, they

138
00:07:58,499 --> 00:08:00,900
recommend the bad dice, the red dice.

139
00:08:00,900 --> 00:08:02,900
So here's a simple situation.

140
00:08:02,900 --> 00:08:04,460
There's a blue dice.

141
00:08:04,460 --> 00:08:05,460
You look at the data.

142
00:08:05,460 --> 00:08:08,300
You think you should choose the blue one.

143
00:08:08,300 --> 00:08:11,490
An expert comes in and tells you you should
choose the red one.

144
00:08:11,490 --> 00:08:13,189
What do you do?

145
00:08:13,189 --> 00:08:16,289
Sunita Sah and her colleagues set two conditions
within this basic

146
00:08:16,289 --> 00:08:17,289
experiment.

147
00:08:17,289 --> 00:08:20,449
In one condition, the adviser simply gave
advice.

148
00:08:20,449 --> 00:08:24,199
In a second condition, they disclosed the
fact that they had

149
00:08:24,199 --> 00:08:25,199
a conflict of interest.

150
00:08:25,199 --> 00:08:28,149
They would say something like, I'm going to
make a recommendation, but

151
00:08:28,149 --> 00:08:34,339
you should know that I get paid for the recommendations
that I make if, in

152
00:08:34,339 --> 00:08:37,649
fact, you choose to go with that recommendation.

153
00:08:37,649 --> 00:08:39,380
And here's what they found.

154
00:08:39,380 --> 00:08:44,440
If the adviser does not disclose a conflict
of interest, 42% of people

155
00:08:44,440 --> 00:08:49,959
who were given the bad advice actually took
the bad advice.

156
00:08:49,959 --> 00:08:54,880
Kicker--
if the adviser disclosed the conflict of interest,

157
00:08:54,880 --> 00:08:57,949
76% took the bad advice.

158
00:08:57,949 --> 00:09:02,209
So what happened here was disclosing the conflict
of interest actually

159
00:09:02,209 --> 00:09:04,330
resulted in a negative effect.

160
00:09:04,330 --> 00:09:07,980
People were more likely to accept that advice.

161
00:09:07,980 --> 00:09:09,779
Now this could happen for a number of reasons.

162
00:09:09,779 --> 00:09:13,830
I mean it could be that if you gave me bad
advise but you disclosed that you

163
00:09:13,830 --> 00:09:16,380
were getting paid, I trust you more.

164
00:09:16,380 --> 00:09:17,680
That's one possibility.

165
00:09:17,680 --> 00:09:23,730
Or it somehow lifts the burden of disclosure,
if you will, and now as an

166
00:09:23,730 --> 00:09:28,120
adviser, I've basically told you that I'm
getting paid for

167
00:09:28,120 --> 00:09:29,470
giving you bad advice.

168
00:09:29,470 --> 00:09:31,889
But that might affect the way in which I give
you advice.

169
00:09:31,889 --> 00:09:33,170
I might sound more confident.

170
00:09:33,170 --> 00:09:34,279
I might sound more reassuring.

171
00:09:34,279 --> 00:09:36,681
So there's a number of different things that
go on.

172
00:09:36,681 --> 00:09:40,600
But the point is that here's a classic, simple,
compelling

173
00:09:40,600 --> 00:09:43,920
demonstration of the fact that disclosing
the truth, disclosing the

174
00:09:43,920 --> 00:09:48,100
conflict, actually reduces in a negative effect.

175
00:09:48,100 --> 00:09:51,449
And these authors have actually done a number
of experiments that actually

176
00:09:51,449 --> 00:09:54,709
showed this phenomena again and again and
again.

177
00:09:54,709 --> 00:09:58,399
So this brings us back of the key question--
is disclosure good?

178
00:09:58,399 --> 00:10:04,060
In theory, if people are perfect processors
of information, yes.

179
00:10:04,060 --> 00:10:08,360
If you disclose more, it turns out they will
get more information on

180
00:10:08,360 --> 00:10:12,160
which to base their decisions, and that should
result in better choices.

181
00:10:12,160 --> 00:10:14,420
But we know that people are not, right?

182
00:10:14,420 --> 00:10:17,270
Will smart disclosure help us achieve this?

183
00:10:17,270 --> 00:10:18,270
Perhaps yes.

184
00:10:18,270 --> 00:10:22,139
If, in fact, that smart disclosure engine
could take all of the data and

185
00:10:22,139 --> 00:10:27,890
make it usable, then perhaps it will, in fact,
increase the quality of

186
00:10:27,890 --> 00:10:29,069
decisions made.

187
00:10:29,069 --> 00:10:30,069
Second point to keep in mind.

188
00:10:30,069 --> 00:10:33,820
That are some behavioral biases where we know
that actually providing

189
00:10:33,820 --> 00:10:37,230
information makes the bias worse.

190
00:10:37,230 --> 00:10:38,899
Overconfidence is a classic example.

191
00:10:38,899 --> 00:10:43,449
We've seen research over the past many years
which shows that giving people

192
00:10:43,449 --> 00:10:49,019
more data makes the confidence go up but does
not change the accuracy.

193
00:10:49,019 --> 00:10:53,360
So you might actually have kinds of biases
or certain situations where not

194
00:10:53,360 --> 00:10:56,459
only does disclosure not help, it might actually
hurt.

195
00:10:56,459 --> 00:11:01,230
And we also saw that in the case of Sunita
Sah and her experiments with

196
00:11:01,230 --> 00:11:04,930
the disclosure about conflicts of interest.

197
00:11:04,930 --> 00:11:08,420
Let me end up by talking about what I'm going
to call the perverse effects

198
00:11:08,420 --> 00:11:09,420
of disclosure.

199
00:11:09,420 --> 00:11:13,750
This is where disclosure actually helps, but
not because it provides

200
00:11:13,750 --> 00:11:15,529
consumers with better knowledge.

201
00:11:15,529 --> 00:11:17,759
Two simple examples.

202
00:11:17,759 --> 00:11:22,450
Many cities like Toronto and New York and
Los Angeles post at the entrance

203
00:11:22,450 --> 00:11:28,050
to restaurants a sign which certifies the
hygiene level of that restaurant

204
00:11:28,050 --> 00:11:31,180
based on an inspection.

205
00:11:31,180 --> 00:11:34,680
There was research done that suggests that
people don't actually look at

206
00:11:34,680 --> 00:11:39,089
those signs very carefully when they choose
a restaurant, but knowing that

207
00:11:39,089 --> 00:11:42,560
this information is going to be made public
results in an increase in the

208
00:11:42,560 --> 00:11:44,790
level of hygiene of the restaurant itself.

209
00:11:44,790 --> 00:11:48,550
So the restaurant manager knows that people
are going to see it, and so

210
00:11:48,550 --> 00:11:51,220
they improve efforts at making the product
quality higher.

211
00:11:51,220 --> 00:11:54,790
So it's not that consumers are using the information,
but the burden of

212
00:11:54,790 --> 00:11:58,430
disclosure makes it more likely that the service
provider will actually

213
00:11:58,430 --> 00:12:00,100
increase the quality.

214
00:12:00,100 --> 00:12:02,910
Last example is similar in nature.

215
00:12:02,910 --> 00:12:07,860
In 2000 or 2001, the United States government
introduced a legislation

216
00:12:07,860 --> 00:12:13,100
called TREAD in which automobile manufactures
had to report a rollover

217
00:12:13,100 --> 00:12:16,780
safety rating for their vehicles.

218
00:12:16,780 --> 00:12:20,480
This is the likelihood that a vehicle is going
to topple and roll over--

219
00:12:20,480 --> 00:12:25,529
in particular, an important piece of data
for vehicles that have a high

220
00:12:25,529 --> 00:12:26,529
wheel base.

221
00:12:26,529 --> 00:12:30,000
Now it turns out that a lot of consumers did
not understand what this

222
00:12:30,000 --> 00:12:32,540
rollover index even meant.

223
00:12:32,540 --> 00:12:37,290
However, research shows that ever since the
rollover index was mandated

224
00:12:37,290 --> 00:12:40,769
and legislated, vehicles have become more
safer.

225
00:12:40,769 --> 00:12:44,430
And that's, again, a similar effect where
the manufacturer knows that the

226
00:12:44,430 --> 00:12:47,930
data is going to be made public, and therefore
they try and make sure that

227
00:12:47,930 --> 00:12:49,910
the quality of the product goes up.

228
00:12:49,910 --> 00:12:54,269
And so disclosure can certainly help in many
indirect ways like this.

