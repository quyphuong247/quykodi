0
00:00:06,620 --> 00:00:08,400
[MUSIC PLAYING] BLAKE MCKIMMIE: All right, Alex, thanks for joining us today.

1
00:00:08,420 --> 00:00:09,420
ALEX HASLAM: Thank you.

2
00:00:09,420 --> 00:00:13,960
BLAKE MCKIMMIE: So what I&#39;d like to do is just talk a little bit about Milgram&#39;s study

3
00:00:13,960 --> 00:00:20,320
and the idea that around the end of World War II when people became aware of a lot of

4
00:00:20,320 --> 00:00:24,980
the horrors and the atrocities that had been committed during the War, one of the explanations

5
00:00:24,980 --> 00:00:29,380
that was commonly given for those events was that there must be something wrong with the

6
00:00:29,380 --> 00:00:32,930
German people, and that&#39;s why they all went along with it.

7
00:00:32,930 --> 00:00:38,610
Milgram conducted a study where participants were asked to shock learners.

8
00:00:38,610 --> 00:00:40,560
And we&#39;ve heard about that so far in the course.

9
00:00:40,560 --> 00:00:45,370
Now, he arrived at a slightly different-- or a very different explanation for why those

10
00:00:45,370 --> 00:00:46,960
sort of horrific events happened.

11
00:00:46,960 --> 00:00:48,320
Can you tell us a bit about that?

12
00:00:48,320 --> 00:00:53,020
ALEX HASLAM: Yeah, so the Milgram-- Milgram did his studies sort of 15 years after the

13
00:00:53,020 --> 00:00:55,950
war in the early &#39;60s.

14
00:00:55,950 --> 00:01:02,980
And as you say, he was, I think, unconvinced by individual difference, personality-based

15
00:01:02,980 --> 00:01:04,820
expectations of tyranny.

16
00:01:04,819 --> 00:01:08,870
The pretty obvious point is that it seems unlikely that all the Germans had the same

17
00:01:08,870 --> 00:01:15,850
personality, or they all had the same personality deficit, as it were, or pathology, that had

18
00:01:15,850 --> 00:01:20,460
led to the kinds of events that you saw in the Holocaust.

19
00:01:20,460 --> 00:01:23,160
So you needed some different type of explanation.

20
00:01:23,160 --> 00:01:28,000
And Milgram was, I think, casting around, so he was trying to find what that might be.

21
00:01:28,000 --> 00:01:33,680
He was quite influenced by the work of Solomon Asch on social influence.

22
00:01:33,680 --> 00:01:40,150
So the idea that people were influenced by other people and perhaps would conform to

23
00:01:40,150 --> 00:01:46,300
the views, wishes of other people was something that Milgram thought was plausible.

24
00:01:46,300 --> 00:01:53,870
And so he devised a paradigm in which participants came in, and as the viewers will be kind of

25
00:01:53,870 --> 00:01:58,720
aware, were asked to play the role of teachers in a learning experiment.

26
00:01:58,720 --> 00:02:03,080
The experimenter instructed them that every time their learner made an error on the task,

27
00:02:03,080 --> 00:02:09,470
they had to administer an increasing level of electric shock going up in 15 volt intervals

28
00:02:09,470 --> 00:02:15,600
from 0 volts all the way to 450 volts, which was labeled XXX, beyond the point which was

29
00:02:15,600 --> 00:02:18,210
labeled danger, a severe shock.

30
00:02:18,210 --> 00:02:23,120
So ostensibly, they were-- this was actually all rigged, so none of the shocks were real.

31
00:02:23,120 --> 00:02:24,810
But the participants didn&#39;t know that.

32
00:02:24,810 --> 00:02:30,990
And obstensibly, the question is, would just a normal person, a resident of New Haven,

33
00:02:30,990 --> 00:02:38,890
just outside or around Yale in Connecticut, would they be willing to kill somebody because

34
00:02:38,890 --> 00:02:43,110
they were asked to do so by an experimenter in a psychology experiment?

35
00:02:43,110 --> 00:02:47,709
When Milgram asked psychiatrists and other people whether they would, they all said,

36
00:02:47,709 --> 00:02:49,180
of course, no, of course they wouldn&#39;t.

37
00:02:49,180 --> 00:02:54,260
They said, maybe only 2% of people would go all the way to 450 volts because that&#39;s the

38
00:02:54,260 --> 00:02:57,410
level of psychopathology in the community.

39
00:02:57,410 --> 00:03:02,820
What Milgram found is-- again, pretty much everybody who studies psychology knows-- is

40
00:03:02,820 --> 00:03:08,550
in what he called his baseline condition, that 65% of people went to the end of the

41
00:03:08,550 --> 00:03:09,550
scale.

42
00:03:09,550 --> 00:03:13,069
So that suggests that this was not a behavior that was peculiar to a very select group of

43
00:03:13,069 --> 00:03:16,910
people, rather, there was something much more fundamental that is common to everybody&#39;s

44
00:03:16,910 --> 00:03:17,910
psychology.

45
00:03:17,910 --> 00:03:26,020
And his idea was that, in some sense, people are programmed to obey authority, that that&#39;s

46
00:03:26,020 --> 00:03:31,080
the way society is structured, that&#39;s what we learn to do, and that we typically in kinds

47
00:03:31,080 --> 00:03:36,770
of situations, where there is clear authority, we cede responsibility to them.

48
00:03:36,770 --> 00:03:41,980
We cede accountability in many ways to them, and really ask, what is it that you want me

49
00:03:41,980 --> 00:03:46,290
to do, and I will do it, rather than ask, what is it you&#39;re asking me to do?

50
00:03:46,290 --> 00:03:47,520
And should I do it?

51
00:03:47,520 --> 00:03:51,300
BLAKE MCKIMMIE: So it&#39;s probably more about-- it&#39;s not about the personality, but it&#39;s more

52
00:03:51,300 --> 00:03:53,080
about the social context that people are in.

53
00:03:53,080 --> 00:03:57,550
ALEX HASLAM: That was Milgram&#39;s argument that this was very much a-- it was about the situation

54
00:03:57,550 --> 00:04:02,739
not the person and that in these kinds of strong situations with these high demands

55
00:04:02,739 --> 00:04:06,629
that most people would go along with those situational pressures.

56
00:04:06,629 --> 00:04:11,910
So it was saying-- there&#39;s nothing really here about individuals, much more about the

57
00:04:11,910 --> 00:04:13,120
situation.

58
00:04:13,120 --> 00:04:19,329
The problem, though, was if you look at his data, first you see, well, actually not all

59
00:04:19,329 --> 00:04:21,359
the participants went to the end.

60
00:04:21,358 --> 00:04:26,589
Actually across all of his studies-- he had about 30 variants of his studies-- on average,

61
00:04:26,589 --> 00:04:30,849
most participants didn&#39;t go to the end, and that in those studies, willingness to go to

62
00:04:30,849 --> 00:04:36,819
the end varied from 100% in some conditions down to 0% in others.

63
00:04:36,819 --> 00:04:42,199
So actually, the idea that we just go along with authority doesn&#39;t make sense either because

64
00:04:42,199 --> 00:04:45,860
there&#39;s also clearly a lot of variability.

65
00:04:45,860 --> 00:04:51,120
And the problem with Milgram&#39;s analysis that we just enter what he called an agentic state

66
00:04:51,120 --> 00:04:55,900
where we just obey authority is that it doesn&#39;t really explain his own data either.

67
00:04:55,900 --> 00:04:58,370
I think his contribution is important.

68
00:04:58,370 --> 00:04:59,580
But it&#39;s clearly limited.

69
00:04:59,580 --> 00:05:05,150
BLAKE MCKIMMIE: So you&#39;ve done a bit of work following up some of these ideas and exploring,

70
00:05:05,150 --> 00:05:10,330
well, if that&#39;s not the explanation, what might actually be accounting for the behavior.

71
00:05:10,330 --> 00:05:15,069
ALEX HASLAM: So it&#39;s, I think, there&#39;s lots of places you can kind of start on this.

72
00:05:15,069 --> 00:05:21,270
But one of the most fruitful places to start is to look at the different versions of Milgram&#39;s

73
00:05:21,270 --> 00:05:22,270
study.

74
00:05:22,270 --> 00:05:23,410
As I said, there&#39;s about 30.

75
00:05:23,410 --> 00:05:25,940
And look at, well, which were the ones where people went all the way?

76
00:05:25,940 --> 00:05:27,779
And which were the ones where they didn&#39;t go all the way?

77
00:05:27,779 --> 00:05:33,869
And what you see very clearly is that the ones where people went further was where they

78
00:05:33,869 --> 00:05:40,270
were very locked into their identification with the experimenter and with the science

79
00:05:40,270 --> 00:05:41,719
of the study.

80
00:05:41,719 --> 00:05:46,689
And they were not exposed to the countervailing influences of the learner.

81
00:05:46,689 --> 00:05:50,809
So what you see-- and actually in the baseline condition is where obedience is 65%.

82
00:05:50,809 --> 00:05:57,020
They&#39;re torn between, should I listen to the experimenter, or should I listen to the learner?

83
00:05:57,020 --> 00:06:00,619
And actually, what you see is depending on, I think, their patterns of identification,

84
00:06:00,619 --> 00:06:07,349
the extent to which they identify with what the experimenter is trying to do versus with

85
00:06:07,349 --> 00:06:13,939
what the participant is saying as a normal, decent member of society, depending on which

86
00:06:13,939 --> 00:06:16,999
way they go, that&#39;s how they make-- that&#39;s the critical variable.

87
00:06:16,999 --> 00:06:21,430
And what you see is in different versions of the study, Milgram actually varies the

88
00:06:21,430 --> 00:06:27,149
extent to which the participants are oriented towards the experimenter or oriented towards

89
00:06:27,149 --> 00:06:28,149
the learner.

90
00:06:28,149 --> 00:06:32,409
If the learner is in the room next to them, then they identify much more with him-- it&#39;s

91
00:06:32,409 --> 00:06:36,139
always a male-- and so they are much less likely to administer the shocks.

92
00:06:36,139 --> 00:06:40,430
If the learner is not in the room, and, indeed, you don&#39;t hear from him, all you hear from

93
00:06:40,430 --> 00:06:45,010
is the experimenter, then actually-- and that was in the very first study that Milgram did--

94
00:06:45,010 --> 00:06:46,659
they go all the way to the end.

95
00:06:46,659 --> 00:06:52,789
So again, it&#39;s really about identification and buying into the task that you&#39;re being

96
00:06:52,789 --> 00:06:54,789
asked to contribute to.

97
00:06:54,789 --> 00:06:59,029
And if you undermine the identification in some way or other by-- and there&#39;s lots of

98
00:06:59,029 --> 00:07:04,309
ways in which Milgram does that-- what you see is that you don&#39;t get the effects that

99
00:07:04,309 --> 00:07:06,289
everybody associates with that study.

100
00:07:06,289 --> 00:07:11,029
So they&#39;re very much contingent upon participants thinking that what they&#39;re doing is worthwhile

101
00:07:11,029 --> 00:07:15,240
and useful and helpful and valuable.

102
00:07:15,240 --> 00:07:19,009
And to the extent that they believe, then, that that is the case, they do it.

103
00:07:19,009 --> 00:07:23,409
It&#39;s not that they&#39;re blind, or they&#39;re ignorant, or they&#39;re stupid, or they&#39;re not paying attention.

104
00:07:23,409 --> 00:07:24,800
They are paying attention.

105
00:07:24,800 --> 00:07:28,020
They just think that this is a very important scientific project and that they&#39;re doing

106
00:07:28,020 --> 00:07:29,509
something helpful.

107
00:07:29,509 --> 00:07:33,319
And that&#39;s really a very-- and so they&#39;re cooperating with the experimenter and not

108
00:07:33,319 --> 00:07:35,349
just obeying blindly.

109
00:07:35,349 --> 00:07:42,219
And I think that lends itself to a very different understanding of why people commit-- perpetrate

110
00:07:42,219 --> 00:07:43,800
obscene atrocities.

111
00:07:43,800 --> 00:07:47,499
Again, typically what you find is not that they&#39;re just-- not they don&#39;t know what they&#39;re

112
00:07:47,499 --> 00:07:48,499
doing.

113
00:07:48,499 --> 00:07:52,139
They do it because they actually really believe in the cause that they&#39;re following and the

114
00:07:52,139 --> 00:07:54,330
leadership of that cause.

115
00:07:54,330 --> 00:07:58,539
And they also believe that it&#39;s actually a good and worthy cause.

116
00:07:58,539 --> 00:07:59,979
That&#39;s actually very critical.

117
00:07:59,979 --> 00:08:04,699
It&#39;s not that they think, oh, this is a bad thing, but I&#39;m just being asked to do it.

118
00:08:04,699 --> 00:08:07,139
No, they&#39;ll actually convinced themself it&#39;s a good thing.

119
00:08:07,139 --> 00:08:10,840
BLAKE MCKIMMIE: And I think you&#39;ve also looked at what some of the participants in that study

120
00:08:10,840 --> 00:08:14,219
actually said when they&#39;re interviewed after participating.

121
00:08:14,219 --> 00:08:18,389
And if you look at the words they&#39;re using, it&#39;s clear they&#39;re not just blindly following.

122
00:08:18,389 --> 00:08:20,460
They&#39;re often quite torn about what they&#39;re done, and--

123
00:08:20,460 --> 00:08:23,591
ALEX HASLAM: Yeah, so I think the thing is, clearly, at that time, the participants are

124
00:08:23,591 --> 00:08:25,849
very stressed by what they have to do.

125
00:08:25,849 --> 00:08:29,610
But they steel themselves to that task, and they follow it through because they believe

126
00:08:29,610 --> 00:08:32,390
that they&#39;re making a contribution to the science.

127
00:08:32,390 --> 00:08:37,930
So yeah, this is-- they believe that this enterprise is valuable, so they struggle on.

128
00:08:37,929 --> 00:08:41,780
And then at the end, of course, Milgram says, well, actually, thanks very much for participating

129
00:08:41,780 --> 00:08:42,780
in the study.

130
00:08:42,780 --> 00:08:46,060
You&#39;ve made a fantastic contribution to science because now we understand why people behave

131
00:08:46,060 --> 00:08:49,721
in the way that you did and we see in other kinds of situations.

132
00:08:49,721 --> 00:08:52,320
And the participants feel good about what they&#39;ve done because they&#39;ve made a contribution

133
00:08:52,320 --> 00:08:53,320
to that.

134
00:08:53,320 --> 00:08:58,860
So they actually-- Milgram does a really good job of explaining to them that they have,

135
00:08:58,860 --> 00:09:03,050
indeed, been good subjects and that what they have done is good.

136
00:09:03,050 --> 00:09:09,820
And one of the really interesting consequences of that is that while we have a sense as outsiders

137
00:09:09,820 --> 00:09:13,321
that the participants must have felt pretty crappy because of what they&#39;d done, actually

138
00:09:13,321 --> 00:09:17,960
they felt very good because they didn&#39;t really undersee it from other people&#39;s perspective--

139
00:09:17,960 --> 00:09:19,110
look you&#39;ve done a terrible thing.

140
00:09:19,110 --> 00:09:20,880
No, they didn&#39;t think they&#39;d done a terrible thing.

141
00:09:20,880 --> 00:09:22,690
They thought they&#39;d done a good thing.

142
00:09:22,690 --> 00:09:25,210
And actually, that really, really comes through in their responses.

143
00:09:25,210 --> 00:09:30,720
They&#39;re happy is-- the signature hallmark emotion is one of happiness, not really of

144
00:09:30,720 --> 00:09:33,670
stress or disaffection or regret even.

145
00:09:33,670 --> 00:09:34,740
No, they think this is great.

146
00:09:34,740 --> 00:09:36,150
I did a wonderful thing.

147
00:09:36,150 --> 00:09:38,190
And Milgram helped them to see it that way.

148
00:09:38,190 --> 00:09:41,820
BLAKE MCKIMMIE: So even after shocking somebody all the way to the end, they feel like they&#39;ve

149
00:09:41,820 --> 00:09:42,820
actually achieved something.

150
00:09:42,820 --> 00:09:43,820
ALEX HASLAM: They&#39;ve achieved something.

151
00:09:43,820 --> 00:09:44,820
They&#39;ve been good participants.

152
00:09:44,820 --> 00:09:45,900
They made a contribution to science.

153
00:09:45,900 --> 00:09:46,900
They&#39;ve pushed science forward.

154
00:09:46,900 --> 00:09:49,580
And of course, from Milgram&#39;s perspective, that was indeed what they had done.

155
00:09:49,580 --> 00:09:52,150
They had made that contribution to this cause.

156
00:09:52,150 --> 00:09:55,392
It&#39;s only as outsiders we say, well, but look, hang on a minute here.

157
00:09:55,392 --> 00:09:57,540
You were pre-- you just killed this person for no good reason.

158
00:09:57,540 --> 00:09:58,740
Well, there was a good reason.

159
00:09:58,740 --> 00:10:00,740
And the reason was science.

160
00:10:00,740 --> 00:10:04,140
And again, when you see people do bad things to other people, that&#39;s typically because

161
00:10:04,140 --> 00:10:09,430
they believe in what they&#39;re doing and the value of it, the utility of it, and its contribution

162
00:10:09,430 --> 00:10:12,120
to making the world a better place, not a worse place.

163
00:10:12,120 --> 00:10:17,760
BLAKE MCKIMMIE: So knowing what we know now then, looking at it through those eyes, what

164
00:10:17,760 --> 00:10:24,180
are the sorts of things you can do to maybe undermine some of the negative consequences

165
00:10:24,180 --> 00:10:28,620
of following-- identifying with and following those sorts of goals?

166
00:10:28,620 --> 00:10:31,420
ALEX HASLAM: Yeah, I mean, I think that&#39;s a very important question.

167
00:10:31,420 --> 00:10:36,720
I mean what you see is that Milgram was able to induce obedience because he had created

168
00:10:36,720 --> 00:10:40,220
a kind of total-- where he did, because he didn&#39;t always, bear in mind, when and where

169
00:10:40,220 --> 00:10:46,140
he did-- it was because he created a totalizing environment where you didn&#39;t hear from the

170
00:10:46,140 --> 00:10:47,160
other.

171
00:10:47,160 --> 00:10:51,310
You weren&#39;t aware of the other potential perspectives or identities that were in play.

172
00:10:51,310 --> 00:10:52,580
It was just about the science.

173
00:10:52,580 --> 00:10:54,080
And you&#39;re caught up in the science.

174
00:10:54,080 --> 00:11:04,000
And if you look at most organizations or enterprises which cultivate what we understand as evil,

175
00:11:04,000 --> 00:11:05,960
what you see is they have that totalizing character.

176
00:11:05,960 --> 00:11:11,590
So whether you&#39;re talking about extremist political groups or terrorist groups or cults,

177
00:11:11,590 --> 00:11:16,830
again, it&#39;s very much about one version of reality, one leadership.

178
00:11:16,830 --> 00:11:25,220
And those organizations go to a lot of difficulty to silence and to peripheralize other interests.

179
00:11:25,220 --> 00:11:28,250
So again, you see that in totalitarian regimes.

180
00:11:28,250 --> 00:11:32,870
You don&#39;t allow other perspectives to enter into people&#39;s consciousness.

181
00:11:32,870 --> 00:11:37,510
So I think one issue there is that that&#39;s the importance of pluralistic democracies

182
00:11:37,510 --> 00:11:45,270
in a sense is that they provide a perspective from which to critique your own stance.

183
00:11:45,270 --> 00:11:50,620
And they allow you to sometimes step outside one identity and look at it from the perspective

184
00:11:50,620 --> 00:11:51,690
of another identity.

185
00:11:51,690 --> 00:11:52,810
And that&#39;s really very critical.

186
00:11:52,810 --> 00:11:58,460
And I think with that, comes a particular form of insight and, indeed, a sort of recognition

187
00:11:58,460 --> 00:12:02,320
that maybe what you&#39;re doing isn&#39;t quite as wonderful as it is presented as being.

