1
00:00:00,000 --> 00:00:05,944
(intro music)

2
00:00:05,944 --> 00:00:07,274
My name is Laurie Santos.

3
00:00:07,274 --> 00:00:11,388
I teach psychology at Yale University,
and today I want to talk to you about

4
00:00:11,388 --> 00:00:13,538
reference dependence and loss aversion.

5
00:00:13,538 --> 00:00:17,068
This lecture is part of a
series on cognitive biases.

6
00:00:17,068 --> 00:00:21,429
Imagine that you're a doctor heading a
medical team that's trying to fight a new

7
00:00:21,429 --> 00:00:25,719
strain of deadly flu, one that's currently
spreading at an alarming rate.

8
00:00:25,719 --> 00:00:30,387
The new flu is so devastating that six
hundred million people have already

9
00:00:30,388 --> 00:00:34,230
been infected, and if nothing
is done, all of them will die.

10
00:00:34,229 --> 00:00:38,539
The good news is there are two, drugs
available to treat the disease and your

11
00:00:38,539 --> 00:00:42,140
team can decide which one
to put into mass production.

12
00:00:42,140 --> 00:00:46,959
Clinical trials show that if you go with
the first drug, drug A, you'll be able to

13
00:00:46,959 --> 00:00:49,879
save two hundred million
of the infected people.

14
00:00:49,878 --> 00:00:52,488
The second option is drug B, which has a

15
00:00:52,488 --> 00:00:56,378
one-third chance of saving all six hundred
million people, but a two-thirds

16
00:00:56,378 --> 00:00:58,530
chance that no one infected will be saved.

17
00:00:58,530 --> 00:01:00,429
Which drug do you pick?

18
00:01:00,429 --> 00:01:03,869
You probably thought drug
A was the best one.

19
00:01:03,869 --> 00:01:07,849
After all, with drug A, two hundred
million people will be saved for sure,

20
00:01:07,849 --> 00:01:09,549
which is a pretty good outcome.

21
00:01:09,549 --> 00:01:13,090
But now imagine that your team is faced
with a slightly different choice.

22
00:01:13,090 --> 00:01:16,159
This time, it's between drug C and drug D.

23
00:01:16,159 --> 00:01:19,750
If you choose drug C, four
hundred million infected

24
00:01:19,750 --> 00:01:21,250
people will die for sure.

25
00:01:21,250 --> 00:01:24,459
If you choose drug D, there's a one-third chance

26
00:01:24,459 --> 00:01:28,489
that no one infected will die, and a
two-thirds chance that six hundred million

27
00:01:28,489 --> 00:01:29,929
infected people will die.

28
00:01:29,929 --> 00:01:32,399
Which drug do you choose in this case?

29
00:01:32,399 --> 00:01:34,918
I bet you probably wen with drug D.

30
00:01:34,918 --> 00:01:38,560
After all, a chance that no one will
die seems like a pretty good bet.

31
00:01:38,560 --> 00:01:41,100
If you picked drug A in the first scenario

32
00:01:41,099 --> 00:01:43,689
and drug D in the second, you're not alone.

33
00:01:43,700 --> 00:01:45,719
When behavioral economists Danny Kahneman

34
00:01:45,719 --> 00:01:48,450
and Amos Tversky gave these
scenarios to college students,

35
00:01:48,450 --> 00:01:52,260
seventy-two percent of people said
that drug A was better than B,

36
00:01:52,260 --> 00:01:56,290
and seventy-eight percent of people
said that drug D was better than C.

37
00:01:56,290 --> 00:02:00,570
But let's take a slightly different
look at both sets of outcomes.

38
00:02:00,569 --> 00:02:03,679
In fact, let's depicted both choices in

39
00:02:03,680 --> 00:02:06,618
terms of the number of people
who will live and die.

40
00:02:06,618 --> 00:02:08,900
Here's your first choice.

41
00:02:08,900 --> 00:02:13,230
Drug A will save two hundred million
people for sure, and for drug B, there's a

42
00:02:13,229 --> 00:02:17,399
one-third chance that all six hundred million
infected people will be saved and a

43
00:02:17,400 --> 00:02:20,159
two-thirds chance that no
one infected will be saved.

44
00:02:20,159 --> 00:02:24,039
And now, let's do the same
thing for drugs C and D.

45
00:02:24,039 --> 00:02:28,620
Surprisingly, you can now see
that the two options are identical.

46
00:02:28,620 --> 00:02:32,689
Drugs A and C will save two hundred
million people, while four hundred million

47
00:02:32,689 --> 00:02:34,099
people are certain to die.

48
00:02:34,099 --> 00:02:38,219
And with both drug B and drug D, you
have a one-third chance of saving all

49
00:02:38,219 --> 00:02:41,778
six hundred million people and a
two-thirds chance of saving no one.

50
00:02:41,778 --> 00:02:46,459
We can argue about whether it's better to
save two hundred million people for sure,

51
00:02:46,459 --> 00:02:49,158
or to take a one-third chance
of saving all of them.

52
00:02:49,158 --> 00:02:50,628
But one thing should be clear from

53
00:02:50,628 --> 00:02:56,348
the example: it's pretty weird for you to
prefer drug A over B at the same time as

54
00:02:56,348 --> 00:02:58,069
you prefer drug D over C.

55
00:02:58,069 --> 00:03:02,609
After all, they're exactly the same drugs
with slightly different labels.

56
00:03:02,610 --> 00:03:05,129
Why does a simple change
in wording change our

57
00:03:05,128 --> 00:03:07,840
judgments about exactly the same options?

58
00:03:07,840 --> 00:03:12,969
Kahneman and Tversky figured out that this
strange effect results from two classic

59
00:03:12,969 --> 00:03:15,000
biases that affect human choice,

60
00:03:15,000 --> 00:03:18,289
biases known as "reference
dependence" and "loss aversion."

61
00:03:18,289 --> 00:03:22,439
"Reference dependence" just refers the
fact that we think about our decisions

62
00:03:22,439 --> 00:03:26,840
not in terms of absolutes, but relative
to some status quo or baseline.

63
00:03:26,840 --> 00:03:29,360
This is why, when you find
a dollar on the ground,

64
00:03:29,360 --> 00:03:32,889
you don't think about that dollar
as part of your entire net worth.

65
00:03:32,889 --> 00:03:35,750
Instead, you think in terms
of the change that the dollar

66
00:03:35,750 --> 00:03:37,189
made your status quo.

67
00:03:37,189 --> 00:03:39,590
You think, "Hey, I'm one dollar richer!"

68
00:03:39,590 --> 00:03:43,840
because of reference dependence, you
don't think of the options presented earlier

69
00:03:43,840 --> 00:03:46,370
in terms of the absolute number of lives saved.

70
00:03:46,370 --> 00:03:50,469
Instead, you frame each choice
relative to some status quo.

71
00:03:50,469 --> 00:03:52,599
And that's why the wording matters.

72
00:03:52,599 --> 00:03:54,888
The first scenario is described in terms

73
00:03:54,889 --> 00:03:56,479
of the number of life saved.

74
00:03:56,479 --> 00:03:58,039
That's your reference point.

75
00:03:58,038 --> 00:04:02,088
You're thinking in terms of how many
additional lives you can save.

76
00:04:02,088 --> 00:04:03,878
And in the second, you think relative

77
00:04:03,878 --> 00:04:06,509
to how many less lives you can lose.

78
00:04:06,509 --> 00:04:09,509
And that second part, worrying about losing

79
00:04:09,509 --> 00:04:15,298
lives, leads to the second bias that's
affecting your choices: loss aversion.

80
00:04:15,299 --> 00:04:19,648
Loss aversion is our reluctance to
make choices that lead to losses.

81
00:04:19,648 --> 00:04:24,109
We don't like losing stuff, whether
it's money, or lives, or even candy.

82
00:04:24,108 --> 00:04:28,129
We have an instinct to avoid
potential losses at all costs.

83
00:04:28,129 --> 00:04:31,699
Economists have found that
loss aversion causes us to do

84
00:04:31,699 --> 00:04:33,478
a bunch of irrational stuff.

85
00:04:33,478 --> 00:04:37,248
Loss aversion causes people to
hold onto property that's losing in

86
00:04:37,249 --> 00:04:39,459
value in the housing market, just because

87
00:04:39,459 --> 00:04:41,959
they don't want to sell
their assets at a loss.

88
00:04:41,959 --> 00:04:46,930
Loss aversion also leads people to
invest more poorly, even avoid risky

89
00:04:46,930 --> 00:04:49,180
stocks that overall will do well, because

90
00:04:49,180 --> 00:04:52,730
we're afraid of a small probability of losses.

91
00:04:52,730 --> 00:04:55,470
Loss aversion causes to latch onto the

92
00:04:55,470 --> 00:04:58,990
fact that drugs C and D involve losing lives.

93
00:04:59,000 --> 00:05:01,910
Our aversion to any potential losses causes

94
00:05:01,910 --> 00:05:04,840
us to avoid drug C and to go with drug D,

95
00:05:04,839 --> 00:05:07,979
which is the chance of not losing anyone.

96
00:05:07,980 --> 00:05:10,470
Our loss aversion isn't as activated

97
00:05:10,470 --> 00:05:12,380
when we hear about drugs A and B.

98
00:05:12,379 --> 00:05:16,689
Both of them involve saving people,
so why not go with the safe option,

99
00:05:16,689 --> 00:05:18,829
drug A over drug B?

100
00:05:18,829 --> 00:05:21,829
Merely describing the outcomes differently

101
00:05:21,829 --> 00:05:24,990
changes which scenarios
we find more aversive.

102
00:05:24,990 --> 00:05:27,079
If losses are mentioned, we want to

103
00:05:27,079 --> 00:05:28,819
reduce them as much as possible,

104
00:05:28,819 --> 00:05:33,040
so much so, that we take on a bit
more risk than we usually like

105
00:05:33,040 --> 00:05:36,569
So describing the decision one
way, as opposed to another,

106
00:05:36,569 --> 00:05:39,189
can cause us to make a
completely different choice.

107
00:05:39,189 --> 00:05:41,159
even in a life-or-death decision

108
00:05:41,158 --> 00:05:45,310
like this, we're at the mercy of our
minds interpret information.

109
00:05:45,310 --> 00:05:50,240
And how our minds interpret information
is at the mercy of our cognitive biases.

110
00:05:55,000 --> 00:05:58,000
Subtitles by the Amara.org community

