1
00:00:03,160 --> 00:00:04,680
Should Batman kill the Joker?

2
00:00:04,680 --> 00:00:10,200
If you were to ask the Dark Knight himself, with his hard-and-fast no-killing rule, he’d say absolutely not.

3
00:00:10,200 --> 00:00:13,399
Actually, in fact, he would say: [Batman voice]
“Absolutely not.”

4
00:00:13,410 --> 00:00:16,329
When you think about it, dude is pretty Kantian
in his ethics.

5
00:00:16,329 --> 00:00:25,269
Regardless of what Joker does, there are some lines that good people do not cross, and for Batman, killing definitely falls on the wrong side of that line.

6
00:00:25,270 --> 00:00:29,140
But, let’s be real here: Joker is never
gonna stop killing.

7
00:00:29,140 --> 00:00:37,800
Sure, Batman will have him thrown back in Arkham, but we all know that he’s gonna get out – he always gets out – and once he’s free, he will kill again.

8
00:00:37,800 --> 00:00:39,980
And maim and terrorize.

9
00:00:39,980 --> 00:00:44,260
And when he does won’t a little bit of that
be Batman’s fault?

10
00:00:44,260 --> 00:00:48,120
Batman has been in a position to kill Joker
hundreds of times.

11
00:00:48,129 --> 00:00:53,120
He has had the power to save anyone from ever
being a victim of the Joker again.

12
00:00:53,120 --> 00:00:59,19
If you have the ability to stop a killer, and you don’t,
are you morally pure because you didn’t kill?

13
00:00:59,20 --> 00:01:04,480
Or are you morally dirty because you refused
to do what needs to be done?

14
00:01:04,480 --> 00:01:14,840
[Theme Music]

15
00:01:14,840 --> 00:01:17,920
So, why do I describe Batman as Kantian?

16
00:01:17,920 --> 00:01:24,300
Well, the school of thought laid out by 18th century German philosopher Immanuel Kant – now known as Kantianism – is pretty straightforward.

17
00:01:24,300 --> 00:01:26,539
More precisely: It’s absolute.

18
00:01:26,549 --> 00:01:29,579
Kantianism is all about sticking to the moral
rulebook.

19
00:01:29,579 --> 00:01:34,59
There are never any exceptions, or any excuses,
for violating moral rules.

20
00:01:34,60 --> 00:01:38,60
And our man Batman tries his hardest to stick
to his code, no matter what.

21
00:01:38,60 --> 00:01:40,180
But there are other ways of looking at ethics.

22
00:01:40,180 --> 00:01:45,40
Like, instead of focusing on the intent behind our behavior, what if we paid more attention to the consequences?

23
00:01:45,40 --> 00:01:47,880
One moral theory that does this is utilitarianism.

24
00:01:47,880 --> 00:01:53,560
It focuses on the results, or consequences,
of our actions, and treats intentions as irrelevant.

25
00:01:53,560 --> 00:01:56,620
Good consequences equal good actions, in this
view.

26
00:01:56,620 --> 00:01:58,380
So, what’s a good consequence?

27
00:01:58,380 --> 00:02:03,679
Modern utilitarianism was founded in the 18th century by British philosophers Jeremy Bentham and John Stuart Mill.

28
00:02:03,679 --> 00:02:08,399
But the theory has philosophical ancestors
in ancient Greek thinkers such as Epicurus.

29
00:02:08,400 --> 00:02:14,240
All of these guys agreed that actions should be measured in terms of the happiness, or pleasure, that they produce.

30
00:02:14,240 --> 00:02:19,920
After all, they argued, happiness is our final end – it’s what we do everything else for.

31
00:02:19,920 --> 00:02:23,920
Think about it like this: many things that
you do, you do for the sake of something else.

32
00:02:23,920 --> 00:02:25,599
You study to get a good grade.

33
00:02:25,599 --> 00:02:26,980
You work to get money.

34
00:02:26,980 --> 00:02:29,420
But why do you want good grades, or money?

35
00:02:29,420 --> 00:02:37,79
There are different answers we could give – like maybe we’re seeking affirmation for our intelligence, or the approval of our parents, or a degree that will give us a career we want.

36
00:02:37,80 --> 00:02:39,180
But why do we want that particular career?

37
00:02:39,180 --> 00:02:40,720
Why do we want approval?

38
00:02:40,730 --> 00:02:44,690
We can keep asking questions, but ultimately
our answer will bottom out in,

39
00:02:44,690 --> 00:02:47,630
“I want what I want because I think it will
make me happy.”

40
00:02:47,630 --> 00:02:51,510
That’s what we all want – it’s one of
the few things everyone has in common.

41
00:02:51,510 --> 00:02:55,329
And utilitarians believe that’s what should
drive our morality.

42
00:02:55,330 --> 00:02:59,940
Like Kant, utilitarians agree that a moral
theory should apply equally to everyone.

43
00:02:59,940 --> 00:03:04,180
But they thought the way to do that was to
ground it in something that’s really intuitive.

44
00:03:04,180 --> 00:03:08,760
And there’s really nothing more basic than the primal desire to seek pleasure and avoid pain.

45
00:03:08,760 --> 00:03:20,40
So, it’s often said that utilitarianism is a hedonistic moral theory – this means the good is equal to the pleasant, and we ought, morally, to pursue pleasure and happiness, and work to avoid pain.

46
00:03:20,40 --> 00:03:24,299
But, utilitarianism is not what you’d call
an egoistic theory.

47
00:03:24,300 --> 00:03:28,460
Egoism says that everyone ought, morally,
to pursue their own good.

48
00:03:28,460 --> 00:03:32,000
In contrast to that, utilitarianism is other-regarding.

49
00:03:32,000 --> 00:03:37,960
It says we should pursue pleasure or happiness – not just for ourselves, but for as many sentient beings as possible.

50
00:03:37,960 --> 00:03:43,560
To put it formally: “we should act always so as to produce the greatest good for the greatest number.”

51
00:03:43,560 --> 00:03:46,579
This is known as the principle of utility.

52
00:03:46,579 --> 00:03:50,720
OK, no one’s gonna argue with a philosophy
that tells them to seek pleasure.

53
00:03:50,720 --> 00:03:57,159
But, sometimes doing what provides the most pleasure to the most people can mean that you have to take one for the team.

54
00:03:57,159 --> 00:04:01,379
It can mean sacrificing your pleasure, in
order to produce more good overall.

55
00:04:01,379 --> 00:04:04,950
Like when it’s your birthday and your family
says you can choose any restaurant you want.

56
00:04:04,950 --> 00:04:09,560
The thing that would make you happiest is Thai food, but you know that that would make the rest of your family miserable.

57
00:04:09,560 --> 00:04:15,280
So when you choose Chinese – which is nobody’s favorite, but everybody can make do – then you’ve thought like a utilitarian.

58
00:04:15,280 --> 00:04:22,740
You’ve chosen the action that would produce the most overall happiness for the group, even though it produced less happiness for you than other alternatives would have.

59
00:04:22,740 --> 00:04:26,380
The problem is, for the most part, we’re
all our own biggest fans.

60
00:04:26,380 --> 00:04:29,570
We each come pre-loaded with a bias in favor
of our own interests.

61
00:04:29,570 --> 00:04:34,280
This isn’t necessarily a bad thing – caring
about yourself is a good way to promote survival.

62
00:04:34,280 --> 00:04:40,580
But where morality is concerned, utilitarians argue, as special as you are, you’re no more special than anybody else.

63
00:04:40,580 --> 00:04:43,800
So your interests count, but no more than
anyone else’s.

64
00:04:43,800 --> 00:04:45,800
Now, you might say that you agree with that.

65
00:04:45,800 --> 00:04:48,840
I mean, we all like to think of ourselves
as being generous and selfless.

66
00:04:48,840 --> 00:04:59,840
But, even though I’m sure you are a totally nice person – you have to admit that things seem way more important – weightier, higher-stakes – when they apply to you, rather than to some stranger.

67
00:04:59,840 --> 00:05:05,659
So, utilitarians suggest that we make our moral decisions from the position of a benevolent, disinterested spectator.

68
00:05:05,660 --> 00:05:13,80
Rather than thinking about what I should do, they suggest that I consider what I would think if I were advising a group of strangers about what they should do.

69
00:05:13,80 --> 00:05:16,900
That way, I have a disposition of good will,
but I’m not emotionally invested.

70
00:05:16,900 --> 00:05:19,359
And I’m a spectator, rather than a participant.

71
00:05:19,360 --> 00:05:25,160
This approach is far more likely to yield a fair and unbiased judgment about what’s really best for the group.

72
00:05:25,160 --> 00:05:30,639
Now, to see utilitarianism put to the test, let’s pop over to the Thought Bubble for some Flash Philosophy.

73
00:05:30,639 --> 00:05:35,190
20th century British philosopher Bernard Williams
offered this thought experiment.

74
00:05:35,190 --> 00:05:42,500
Jim is on a botanical expedition in South America when he happens upon a group of 20 indigenous people, and a group of soldiers.

75
00:05:42,500 --> 00:05:48,160
The whole group of indigenous people is about to be executed for protesting their oppressive regime.

76
00:05:48,160 --> 00:05:54,960
For some reason, the leader of the soldiers offers Jim the chance to shoot one of the prisoners, since he’s a guest in their land.

77
00:05:54,970 --> 00:05:59,490
He says that if Jim shoots one of the prisoners,
he’ll let the other 19 go.

78
00:05:59,490 --> 00:06:03,750
But if Jim refuses, then the soldiers will
shoot all 20 protesters.

79
00:06:03,750 --> 00:06:04,970
What should Jim do?

80
00:06:04,970 --> 00:06:07,170
More importantly, what would you do?

81
00:06:07,170 --> 00:06:11,40
Williams actually presents this case as a
critique of utilitarianism.

82
00:06:11,50 --> 00:06:15,60
The theory clearly demands that Jim shoot
one man so that 19 will be saved.

83
00:06:15,60 --> 00:06:20,340
But, Williams argues, no moral theory ought
to demand the taking of an innocent life.

84
00:06:20,340 --> 00:06:29,880
Thinking like a Kantian, Williams argues that it’s not Jim’s fault that the head soldier is a total dirt bag, and Jim shouldn’t have to get literal blood on his hands to try and rectify the situation.

85
00:06:29,880 --> 00:06:34,840
So, although it sounds pretty simple, utilitarianism
is a really demanding moral theory.

86
00:06:34,840 --> 00:06:38,359
It says, we live in a world where sometimes
people do terrible things.

87
00:06:38,360 --> 00:06:43,240
And, if we’re the ones who happen to be there, and we can do something to make things better, we must.

88
00:06:43,240 --> 00:06:45,240
Even if that means getting our hands dirty.

89
00:06:45,240 --> 00:06:50,960
And if I sit by and watch something bad happen when I could have prevented it, my hands are dirty anyway.

90
00:06:50,960 --> 00:06:53,659
So, Jim shouldn’t think about it as killing
one man.

91
00:06:53,660 --> 00:06:57,100
That man was dead already, because they were
all about to be killed.

92
00:06:57,110 --> 00:07:01,470
Instead, Jim should think of his decision
as doing what it takes to save 19.

93
00:07:01,470 --> 00:07:04,410
And Batman needs to kill the Joker already.

94
00:07:04,410 --> 00:07:05,600
Thanks, Thought Bubble!

95
00:07:05,600 --> 00:07:09,800
Now, if you decide you want to follow utilitarian
moral theory, you have options.

96
00:07:09,800 --> 00:07:11,520
Specifically, two of them.

97
00:07:11,520 --> 00:07:19,200
When Bentham and Mill first posed their moral theory, it was in a form now known as Act Utilitarianism, sometimes called classical utilitarianism.

98
00:07:19,200 --> 00:07:25,420
And it says that, in any given situation, you should choose the action that produces the greatest good for the greatest number. Period.

99
00:07:25,420 --> 00:07:30,320
But sometimes, the act that will produce the greatest good for the greatest number can seem just wrong.

100
00:07:30,320 --> 00:07:34,240
For instance, suppose a surgeon has five patients,
all waiting for transplants.

101
00:07:34,240 --> 00:07:38,440
One needs a heart, another a lung. Two are
waiting for kidneys and the last needs a liver.

102
00:07:38,440 --> 00:07:43,480
The doctor is pretty sure that these patients will all die before their names come up on the transplant list.

103
00:07:43,480 --> 00:07:46,980
And he just so happens to have a neighbor
who has no family.

104
00:07:46,980 --> 00:07:49,40
Total recluse.
Not even a very nice guy.

105
00:07:49,40 --> 00:07:52,540
The doctor knows that no one would miss this
guy if he were to disappear.

106
00:07:52,550 --> 00:07:56,810
And by some miracle, the neighbor is a match
for all five of the transplant patients.

107
00:07:56,810 --> 00:08:04,740
So, it seems like, even though this would be a bad day for the neighbor, an act-utilitarian should kill the neighbor and give his organs to the five patients.

108
00:08:04,740 --> 00:08:06,910
It’s the greatest good for the greatest
number.

109
00:08:06,910 --> 00:08:10,970
Yes, one innocent person dies, but five innocent
people are saved.

110
00:08:10,970 --> 00:08:15,790
This might seem harsh, but remember that pain
is pain, regardless of who’s experiencing it.

111
00:08:15,790 --> 00:08:20,750
So the death of the neighbor would be no worse than the death of any of those patients dying on the transplant list.

112
00:08:20,750 --> 00:08:24,710
In fact, it’s five times less bad than all
five of their deaths.

113
00:08:24,710 --> 00:08:29,729
So thought experiments like this led some utilitarians to come up with another framework for their theory.

114
00:08:29,730 --> 00:08:32,470
This one is called Rule Utilitarianism.

115
00:08:32,470 --> 00:08:38,970
This version of the theory says that we ought to live by rules that, in general, are likely to lead to the greatest good for the greatest number.

116
00:08:38,970 --> 00:08:45,170
So, yes, there are going to be situations where killing an innocent person will lead to the greatest good for the greatest number.

117
00:08:45,180 --> 00:08:49,739
But, rule utilitarians want us to think long-term,
and on a larger scale.

118
00:08:49,740 --> 00:09:00,440
And overall, a whole society where innocent people are taken off the streets to be harvested for their organs is gonna have a lot less utility than one where you don’t have to live in constant fear of that happening to you.

119
00:09:00,440 --> 00:09:10,360
So, rule utilitarianism allows us to refrain from acts that might maximize utility in the short run, and instead follow rules that will maximize utility for the majority of the time.

120
00:09:10,360 --> 00:09:13,900
As an owner of human organs, this approach
might make sense to you.

121
00:09:13,900 --> 00:09:19,439
But I still gotta say: If Batman were a utilitarian of either kind, it wouldn’t look very good for the Joker.

122
00:09:19,440 --> 00:09:21,920
Today we learned about utilitarianism.

123
00:09:21,920 --> 00:09:27,699
We studied the principle of utility, and learned about the difference between act and rule utilitarianism.

124
00:09:27,700 --> 00:09:31,700
Next time, we’ll take a look at another
moral theory – contractarianism.

125
00:09:31,710 --> 00:09:35,340
Crash Course Philosophy is produced in association
with PBS Digital Studios.

126
00:09:35,340 --> 00:09:38,920
You can head over to their channel and check out a playlist of the latest episodes from shows like

127
00:09:38,920 --> 00:09:42,180
The Good Stuff, Gross Science, and PBS Idea
Channel.

128
00:09:42,180 --> 00:09:45,620
This episode of Crash Course was filmed in
the Doctor Cheryl C. Kinney Crash Course Studio

129
00:09:45,620 --> 00:09:50,800
with the help of all of these awesome people and our equally fantastic graphics team is Thought Cafe.

