1
00:00:08,970 --> 00:00:13,071
https://youtu.be/RLKzdsfSxtQ
So our third example in Chapter 15 takes
us kind of back to where we started.

2
00:00:13,071 --> 00:00:15,040
We're going back to mailing lists.

3
00:00:15,040 --> 00:00:19,407
And we're going to actually
crawl a web-based archive, and

4
00:00:19,407 --> 00:00:21,950
this is a site called gmane.org.

5
00:00:21,950 --> 00:00:23,672
And it archives mailing lists so

6
00:00:23,672 --> 00:00:28,520
that if a project kind of ends, you can
have an archive for a long period of time.

7
00:00:28,520 --> 00:00:30,812
And so we're going to crawl it,
using standard crawling, and

8
00:00:30,812 --> 00:00:33,470
then we're going to do some analysis and
cleanup, because the mail

9
00:00:33,470 --> 00:00:38,460
is a little bit tricky, because the mail
lasts over almost a ten-year period.

10
00:00:38,460 --> 00:00:41,890
And then we're going to do some
visualization of the data.

11
00:00:41,890 --> 00:00:45,830
And so this is also the largest data
set that we're going to play with,

12
00:00:45,830 --> 00:00:47,880
although the web is gigantic.

13
00:00:47,880 --> 00:00:53,180
You can do the first two things, like
the geography one, with like 20 records,

14
00:00:53,180 --> 00:00:56,290
and you can do like 40 or 50 web pages.

15
00:00:56,290 --> 00:01:00,570
But if you pull this whole data set
size down, it's greater than a gigabyte.

16
00:01:00,570 --> 00:01:02,600
And so if you just set this
thing going all night,

17
00:01:04,240 --> 00:01:07,360
especially if you use gmane.org,
this is not a rate-limited API.

18
00:01:07,360 --> 00:01:11,698
But if you read their websites,
they tell you please don't hurt us.

19
00:01:11,698 --> 00:01:14,410
We don't want to write the software
to do rate limiting and so

20
00:01:14,410 --> 00:01:16,380
as long as we're all friendly.

21
00:01:16,380 --> 00:01:19,340
So I will give you a special
URL in the assignment

22
00:01:19,340 --> 00:01:21,970
that will be a non-rate-limited
copy of this.

23
00:01:21,970 --> 00:01:26,140
So I'll put something up at one of
our university servers that will

24
00:01:26,140 --> 00:01:28,700
be a copy of all this stuff,
that won't be rate limited.

25
00:01:28,700 --> 00:01:30,480
So please use the non-rate-limited copy,

26
00:01:30,480 --> 00:01:35,130
although you can do playing on your
own with talking to the data directly.

27
00:01:35,130 --> 00:01:39,350
But please use the rate,
non-rate-limited copy if possible.

28
00:01:40,570 --> 00:01:41,570
So here we go.

29
00:01:41,570 --> 00:01:44,160
This is the most complex
thing that we're going to do.

30
00:01:44,160 --> 00:01:47,035
We are starting with a gathering phase.

31
00:01:47,035 --> 00:01:49,450
gmane.org has all these mailing lists.

32
00:01:49,450 --> 00:01:51,970
And we're going to do a lot
of work on this stuff.

33
00:01:51,970 --> 00:01:54,880
And so the first thing we're
going to do is make a local copy.

34
00:01:54,880 --> 00:01:59,146
And this is the standard thing where we're
going to retrieve it, hit the URL, parse it,

35
00:01:59,146 --> 00:02:00,504
and stick it in a database.

36
00:02:00,504 --> 00:02:05,432
So this is going to be a relatively simple
looping activity where, you know, we'll have

37
00:02:05,432 --> 00:02:09,564
some data in here, we'll figure out
what the next one is, and

38
00:02:09,564 --> 00:02:12,830
we'll go loop and read and
add a few things.

39
00:02:12,830 --> 00:02:17,020
And if we get blown up that's okay,
because we're not going to go backwards.

40
00:02:17,020 --> 00:02:21,520
So this is our nice local copy, this
is a very raw copy of all of our data.

41
00:02:22,750 --> 00:02:27,770
Now this data is a little messier than
the data that we've been working with before,

42
00:02:27,770 --> 00:02:31,630
because it's email that
lives over a ten-year period.

43
00:02:31,630 --> 00:02:35,430
And it's a lot of data, and
just running something across this,

44
00:02:35,430 --> 00:02:36,670
this is greater than a gigabyte.

45
00:02:38,100 --> 00:02:41,254
Okay? It's not compressed,
we've not parsed it.

46
00:02:41,254 --> 00:02:43,599
And if we really were
going to do analysis off of it,

47
00:02:43,599 --> 00:02:46,789
depending on the speed of your
computer it might take minutes or

48
00:02:46,789 --> 00:02:50,740
hours even just to read through
this data and do all the parsing.

49
00:02:50,740 --> 00:02:52,710
So what we're going to do is we're
going to make a second database,

50
00:02:52,710 --> 00:02:55,680
and we're going to have this
database with multiple tables.

51
00:02:55,680 --> 00:02:56,980
We're going to do third normal form.

52
00:02:56,980 --> 00:02:58,760
Remember that from previous lecture?

53
00:02:58,760 --> 00:03:01,160
Third normal form is going to
be all these integer keys.

54
00:03:01,160 --> 00:03:02,090
Remember that?

55
00:03:02,090 --> 00:03:02,790
Right?

56
00:03:02,790 --> 00:03:05,770
So this is going to be
a wisely built database,

57
00:03:05,770 --> 00:03:09,430
which means it's going to be much smaller
because of the lack of redundancy.

58
00:03:09,430 --> 00:03:13,460
And when you run gmodel.py,
which copies from this database,

59
00:03:13,460 --> 00:03:17,860
just note in your mind how long
it takes to run gmodel.py,

60
00:03:17,860 --> 00:03:20,050
versus reading this after
we've cleaned up the data.

61
00:03:20,050 --> 00:03:24,060
And that's why we do
this cleaning up step.

62
00:03:24,060 --> 00:03:27,220
We tend not to do the cleaning up step when
we're doing this, because we want this to

63
00:03:27,220 --> 00:03:31,830
be simple and focused on re-startability
and reliability and simplicity.

64
00:03:31,830 --> 00:03:35,080
Because we might learn
something about the data.

65
00:03:35,080 --> 00:03:37,950
And we keep the raw data, and
we re-parse it every time.

66
00:03:37,950 --> 00:03:41,460
So you'll probably end up changing this
code a lot more often when you realize oh,

67
00:03:41,460 --> 00:03:43,420
wait, there's another mistake in the data.

68
00:03:43,420 --> 00:03:47,220
And I've got to rerun the thing that goes
from the raw data to the clean data.

69
00:03:47,220 --> 00:03:48,860
So this is the clean data.

70
00:03:48,860 --> 00:03:52,600
So, this is much smaller,
it's normalized, it's cleaned up.

71
00:03:52,600 --> 00:03:58,640
It's a very well-organized database and
the data is much smaller afterwards.

72
00:03:58,640 --> 00:04:02,390
So now we have all of this data in a very
clean little database with a bunch of

73
00:04:02,390 --> 00:04:05,010
little tables in it in content.sqlite.

74
00:04:05,010 --> 00:04:08,270
And we can run a number of
applications on this, okay?

75
00:04:08,270 --> 00:04:10,170
And so this is the actual analysis.

76
00:04:10,170 --> 00:04:13,820
This is the clean phase, this is
the gather phase, this is the analysis.

77
00:04:13,820 --> 00:04:17,390
We're just showing how you can have
this pipeline of data coming in.

78
00:04:17,390 --> 00:04:21,480
And then do a bunch of different
analysis once you've got nice clean data.

79
00:04:21,480 --> 00:04:24,700
And so this is going to be
a loop that just sends an SQL

80
00:04:24,700 --> 00:04:26,280
command to read through all of this data.

81
00:04:26,280 --> 00:04:30,210
And then it's going to do something
like make a dictionary and

82
00:04:30,210 --> 00:04:34,910
compute who the top five people are that
are sending email on this mailing list, or

83
00:04:34,910 --> 00:04:36,410
the top five organizations.

84
00:04:36,410 --> 00:04:40,220
This is going to look a lot like
the programs that you've written before.

85
00:04:40,220 --> 00:04:43,660
Parse, read, split, dictionary,

86
00:04:43,660 --> 00:04:46,540
get, all that kind of stuff
that you've done before, okay?

87
00:04:46,540 --> 00:04:49,520
And it's just dumping them out. So it's
very similar other than the fact that its

88
00:04:49,520 --> 00:04:53,790
source of data is data we've gathered and
cleaned up.

89
00:04:53,790 --> 00:04:56,270
Then we have another thing
that makes a word map.

90
00:04:56,270 --> 00:04:58,470
And it's sort of taking
something similar to this.

91
00:04:58,470 --> 00:05:02,380
But instead of looking for the From
address, it actually takes the body,

92
00:05:02,380 --> 00:05:05,921
and breaks it into words,
splits it into words, throws things away.

93
00:05:05,921 --> 00:05:11,700
And then does a frequency analysis of the
words, and prints that out in JSON format.

94
00:05:11,700 --> 00:05:13,580
So when this program gword.py runs,

95
00:05:13,580 --> 00:05:16,510
it produces gword.js, and
you can take a look at it.

96
00:05:16,510 --> 00:05:19,990
And you'll see some curly braces and
colons and single quotes and stuff like

97
00:05:19,990 --> 00:05:24,760
that, and it really is a mapping
between each word and its frequency.

98
00:05:24,760 --> 00:05:28,340
And we're going to use that to make
a word map, a word diagram,

99
00:05:28,340 --> 00:05:31,110
where the size of the word,
it's all real pretty, and again,

100
00:05:31,110 --> 00:05:34,565
making word maps is part of
this d3.js visualization.

101
00:05:34,565 --> 00:05:37,737
Don't have to worry about that,
already downloaded it, and

102
00:05:37,737 --> 00:05:39,629
I've already written gword.htm.

103
00:05:39,629 --> 00:05:40,940
You don't have to change any of this.

104
00:05:40,940 --> 00:05:44,400
You can if you want, especially if
you're sharp in JavaScript, and

105
00:05:44,400 --> 00:05:46,380
you can make your own visualizations.

106
00:05:46,380 --> 00:05:48,890
And then that basically loads and
reads this stuff.

107
00:05:48,890 --> 00:05:53,390
And then that takes the data from gword.js
and produces like a nice word cloud for

108
00:05:53,390 --> 00:05:54,790
you, and looks real pretty.

109
00:05:54,790 --> 00:05:55,990
So that's a word cloud.

110
00:05:55,990 --> 00:06:01,890
Now, same thing, we can create a line and
keep track of, this is going to basically be

111
00:06:01,890 --> 00:06:06,550
by organizations, and there's this program
that reads gline, it reads all the data.

112
00:06:06,550 --> 00:06:13,630
And it produces some JSON and the
gline.js, and then here is a visualization

113
00:06:13,630 --> 00:06:18,690
web page that reads gline.js and
uses the d3 to do visualization.

114
00:06:18,690 --> 00:06:21,130
So you don't have to write
any of this stuff, you

115
00:06:21,130 --> 00:06:25,240
don't actually have to write any of this
stuff, but it produces nice lines for you.

116
00:06:25,240 --> 00:06:31,100
And again, the whole goal here, most of
the code is pretty much just sitting here.

117
00:06:31,100 --> 00:06:33,490
And the more interesting
question is looking at it and

118
00:06:33,490 --> 00:06:37,000
understanding how a program
that's doing gathering works.

119
00:06:37,000 --> 00:06:38,980
How a program that does cleanup works.

120
00:06:38,980 --> 00:06:41,000
How a data model should look.

121
00:06:41,000 --> 00:06:43,430
How you represent clean
data in a data model.

122
00:06:43,430 --> 00:06:47,460
And then how you write a series
of analysis programs, and

123
00:06:47,460 --> 00:06:50,950
then just one of many
techniques to do visualization.

124
00:06:50,950 --> 00:06:55,550
I just happened to use this d3.js, because
it's widely used by a lot of folks, and

125
00:06:55,550 --> 00:06:59,950
it also just works in a browser so you
didn't have to install any more software.

126
00:06:59,950 --> 00:07:05,570
So, now we see our most
complicated multi-step process and

127
00:07:05,570 --> 00:07:07,990
then we'll take a look at
how these things each run.

128
00:07:09,660 --> 00:07:12,120
Like I said, this is the gmane page.

129
00:07:12,120 --> 00:07:13,590
They're trying to tell you just be nice.

130
00:07:13,590 --> 00:07:15,770
And then make sure to use the URL.

131
00:07:15,770 --> 00:07:19,320
I'll make it so
it has the same API as the gmane.

132
00:07:19,320 --> 00:07:23,160
And it'll make a nice local copy,
so we won't crush the gmane server,

133
00:07:23,160 --> 00:07:26,650
even if 100,000 of you want to
download all the gigabytes.

134
00:07:26,650 --> 00:07:29,490
I'll come up with a way to make it so
that that can happen.

135
00:07:29,490 --> 00:07:34,347
So in summary, what we've seen is three
different examples of a multi-step process

136
00:07:34,347 --> 00:07:35,837
to do simple data mining.

137
00:07:35,837 --> 00:07:40,004
Again I emphasize this is not the sum
total of everything that is data mining.

138
00:07:40,004 --> 00:07:42,837
Everything that is data mining
is far more complex than this,

139
00:07:42,837 --> 00:07:46,004
this is kind of a multi-step process
where we're doing all Python.

140
00:07:46,004 --> 00:07:50,422
And my goal for this is just to have you
look at a series of more complex programs.

141
00:07:50,422 --> 00:07:53,731
And so I'll make sure in other videos
to run through all these programs,

142
00:07:53,731 --> 00:07:57,096
so you can see all the techniques that
each program does to accomplish what

143
00:07:57,096 --> 00:07:58,255
it needs to accomplish.