1
00:00:08,511 --> 00:00:12,991
https://youtu.be/OSKJQW5D4so
So the next example is, we're going to
simulate the Google page rank algorithm.

2
00:00:12,991 --> 00:00:17,751
And before Google there were many search
engines, and they had many ways of picking

3
00:00:17,751 --> 00:00:21,570
what pages to show you when you type
the running speed of a cheetah.

4
00:00:21,570 --> 00:00:23,340
What page is important?

5
00:00:23,340 --> 00:00:28,680
And what Google figured out early
on was that instead of just looking

6
00:00:28,680 --> 00:00:32,210
at the words on the page, you actually
worked on the links between the pages.

7
00:00:32,210 --> 00:00:35,210
And the page rank algorithm
basically values links.

8
00:00:35,210 --> 00:00:40,250
So a page that has a lot of
incoming links is valued.

9
00:00:40,250 --> 00:00:44,952
But then you have to say, are those links,
were those like junk links or good links?

10
00:00:44,952 --> 00:00:48,980
And so the links that come from better
pages are more valuable than the links.

11
00:00:48,980 --> 00:00:50,240
And so you kind of keep going,
and going, and going and

12
00:00:50,240 --> 00:00:52,070
you eventually get to the whole Internet.

13
00:00:52,070 --> 00:00:53,835
And it's like, who points to everybody.

14
00:00:53,835 --> 00:00:56,665
So this whole page rank algorithm that

15
00:00:56,665 --> 00:01:01,575
sort of has goodness flow
throughout the Internet.

16
00:01:01,575 --> 00:01:03,645
And so, we're going to build
that page rank algorithm.

17
00:01:03,645 --> 00:01:06,005
That code's already built, so,
you don't have to worry about that.

18
00:01:06,005 --> 00:01:09,795
But it really takes
the goodness of any link, right?

19
00:01:09,795 --> 00:01:12,695
If this link is a certain amount of
goodness, and there's a bunch of

20
00:01:12,695 --> 00:01:16,880
outbounds, it shares all of its
goodness with its outbound links.

21
00:01:16,880 --> 00:01:20,990
And then it takes all the goodness
from the inbound links.

22
00:01:20,990 --> 00:01:23,340
And at some point you run this over and
over and over again,

23
00:01:23,340 --> 00:01:27,080
and then at some point the flows
stabilize and the amount

24
00:01:27,080 --> 00:01:31,340
outbound is equal to the amount inbound,
and that's what's called the page rank.

25
00:01:31,340 --> 00:01:36,450
So Google does this. Right here I got
a picture of maybe 15 little websites,

26
00:01:36,450 --> 00:01:39,970
as compared to Google doing page
rank on the entire Internet.

27
00:01:39,970 --> 00:01:42,950
And so we're not going to worry too much
about the page rank, it's really more of

28
00:01:42,950 --> 00:01:45,930
a building a web crawler and
then running this calculation and

29
00:01:45,930 --> 00:01:48,110
then making a pretty
picture of the calculation.

30
00:01:49,210 --> 00:01:52,910
And so this is basically how
search engines work,

31
00:01:52,910 --> 00:01:57,860
They really break the task of search
engine into three distinct steps.

32
00:01:57,860 --> 00:02:01,450
One is crawling the web, which
is our gathering phase, and

33
00:02:01,450 --> 00:02:03,980
when we're talking about our
multi-step data analysis.

34
00:02:03,980 --> 00:02:07,510
The next is index building, and that
we're going to do which is the page rank.

35
00:02:07,510 --> 00:02:10,640
Which is like sense making of
the data that you've crawled.

36
00:02:10,640 --> 00:02:13,590
And then searching is
looking at the data and

37
00:02:13,590 --> 00:02:15,650
figuring out what
the right page to show is.

38
00:02:15,650 --> 00:02:19,081
But we're not going to do searching in this
one, we're going to do visualization, and

39
00:02:19,081 --> 00:02:22,080
show, when it's all said and done,
the results of our index building.

40
00:02:22,080 --> 00:02:23,380
And so that's what we're going to do
in this case.

41
00:02:23,380 --> 00:02:25,760
We're going to web crawl,
we're going to index build.

42
00:02:25,760 --> 00:02:28,480
And then instead of searching
we're going to do visualization.

43
00:02:28,480 --> 00:02:31,890
But you could write, if you wanted to,
you could write a search thing

44
00:02:31,890 --> 00:02:34,050
that would read the database by
the time we've done the index,

45
00:02:34,050 --> 00:02:36,260
which is the way a normal browser does it.

46
00:02:36,260 --> 00:02:40,000
Because we're not really
an interactive application class,

47
00:02:40,000 --> 00:02:41,920
we're more like doing offline things.

48
00:02:41,920 --> 00:02:44,490
So we're going to do
visualization instead.

49
00:02:44,490 --> 00:02:49,450
So the web crawler is the term we
give to this piece of software that

50
00:02:49,450 --> 00:02:54,090
gives itself a list of pages to look at,
reads those pages.

51
00:02:54,090 --> 00:02:57,820
Uses something like a Beautiful Soup to
extract all the links from those pages.

52
00:02:57,820 --> 00:03:01,550
Adds the links on those pages to
a list of links yet to be crawled.

53
00:03:01,550 --> 00:03:03,785
And when done, goes and
grabs the next page on the list.

54
00:03:03,785 --> 00:03:10,455
So there's this building list. Every time
you grab a page, it gives you more links.

55
00:03:10,455 --> 00:03:13,225
And if you're Google, eventually
you've kind of finished.

56
00:03:13,225 --> 00:03:16,110
But for our web crawlers that's
starting like on one page,

57
00:03:16,110 --> 00:03:18,890
it would take a very long time and
we'd run out of bandwidth and whatever.

58
00:03:18,890 --> 00:03:22,320
So, we're going to crawl a little corner
of the Web and stay in that corner.

59
00:03:22,320 --> 00:03:25,655
But the idea is,
have a list of to be crawled sites,

60
00:03:25,655 --> 00:03:30,481
grab one off the top of the list,
pull that page down, find all the links.

61
00:03:30,481 --> 00:03:32,721
Then add those links to
the end of the list.

62
00:03:32,721 --> 00:03:34,531
And then go back and
crawl the next site and

63
00:03:34,531 --> 00:03:36,480
then add those links to
the end of the list.

64
00:03:36,480 --> 00:03:39,880
And so that's the basic idea of a
web crawler.

65
00:03:41,300 --> 00:03:42,780
And so we go like that, right?

66
00:03:42,780 --> 00:03:45,180
We have a queue, a list of the URLs.

67
00:03:45,180 --> 00:03:48,880
We go grab some data,
we parse it, find all the links.

68
00:03:48,880 --> 00:03:52,540
New links come in and then we grab one
off the top, and we do it, and we do it.

69
00:03:52,540 --> 00:03:58,340
And all the while, we're putting all
these HTML pages into a database, right?

70
00:03:58,340 --> 00:04:00,610
Round and round and round and round we go.

71
00:04:00,610 --> 00:04:03,410
And again,
it's a really basic kind of thing.

72
00:04:03,410 --> 00:04:07,100
And once you did the previous assignment,
where you used Beautiful Soup to

73
00:04:07,100 --> 00:04:10,750
grab the links, the next thing is
like let's go to that next page.

74
00:04:10,750 --> 00:04:11,720
So that's what we're going to do.

75
00:04:11,720 --> 00:04:12,430
You sort of did that.

76
00:04:12,430 --> 00:04:14,740
You had to go like seven pages or
something like that.

77
00:04:15,830 --> 00:04:18,308
So, things like Google that are doing
web crawling seriously, and

78
00:04:18,308 --> 00:04:21,490
we of course aren't going to be very
serious about it in this class.

79
00:04:21,490 --> 00:04:25,165
But you've got to prioritize which pages,
when to recheck the pages,

80
00:04:25,165 --> 00:04:27,370
how often they're changing.

81
00:04:27,370 --> 00:04:29,470
You don't want to crush a website.

82
00:04:29,470 --> 00:04:32,060
Why Google shouldn't just
kind of beat this website up so

83
00:04:32,060 --> 00:04:35,380
badly that your Internet
service provider gets upset.

84
00:04:35,380 --> 00:04:37,830
And then maybe you have lots
of different web crawlers.

85
00:04:37,830 --> 00:04:41,140
And we're not going to write anything
anywhere near as sophisticated as this.

86
00:04:41,140 --> 00:04:42,760
But it is something to think about.

87
00:04:44,050 --> 00:04:45,900
Also, when you're thinking about crawling,

88
00:04:45,900 --> 00:04:48,850
even though the code we do
doesn't look for this, there is

89
00:04:48,850 --> 00:04:52,900
a way to communicate with web crawlers
on websites if you're building websites

90
00:04:52,900 --> 00:04:57,280
using the concept of a file called
robots.txt that you put at the very top.

91
00:04:57,280 --> 00:05:02,550
And it's a little specification that
basically says, hey crawlers go away

92
00:05:02,550 --> 00:05:07,600
or you are allowed to go here,
this is don't go here, etc, etc.

93
00:05:07,600 --> 00:05:09,990
And it's a very informal,
voluntary standard.

94
00:05:09,990 --> 00:05:16,050
And I remember when this emerged,
web crawlers were just being created and

95
00:05:16,050 --> 00:05:21,550
somebody just wrote something down and the
group of people doing web servers were so

96
00:05:21,550 --> 00:05:25,410
small that everybody just agreed to it and
it sort of lived.

97
00:05:25,410 --> 00:05:28,060
I think it's a little more formal than
it was in the very early days, but

98
00:05:28,060 --> 00:05:34,510
it's a perfect example of a really simple
but good standard that came together

99
00:05:34,510 --> 00:05:39,310
really quickly and turned out to be
the right solution and we all just use it.

100
00:05:39,310 --> 00:05:45,470
Now once you have the crawler that's
creating all the data in the repository,

101
00:05:45,470 --> 00:05:47,610
then you have this thing
called the indexer, right?

102
00:05:47,610 --> 00:05:51,240
That sort of works on the repository and
cleans things up.

103
00:05:51,240 --> 00:05:55,930
And then the searching engine gets all this
information and then sense makes of it.

104
00:05:55,930 --> 00:06:00,340
So search indexing sort
of pulls the text apart.

105
00:06:00,340 --> 00:06:02,340
Right? The crawler gives you the text.

106
00:06:02,340 --> 00:06:06,580
The search indexer crawls and
looks at the text itself, what keywords,

107
00:06:06,580 --> 00:06:10,160
what are the words in the links,
what URLs it goes to.

108
00:06:10,160 --> 00:06:14,890
Does this look like it's a spammy page or
a bad website, etc., etc., etc.

109
00:06:14,890 --> 00:06:16,530
And so it's like actually looking for

110
00:06:16,530 --> 00:06:19,450
the useful meaning inside
of each of these web pages.

111
00:06:19,450 --> 00:06:22,100
And so here's our little crawler, and

112
00:06:22,100 --> 00:06:25,660
it's a little more complex than
the previous thing that we did.

113
00:06:27,130 --> 00:06:31,340
We have, basically, again an application

114
00:06:31,340 --> 00:06:35,440
we call spider.py and we actually
input to that a starting point.

115
00:06:35,440 --> 00:06:36,760
We say where do you want to start?

116
00:06:37,920 --> 00:06:40,710
Right? So it starts with no web pages
to crawl. 

117
00:06:42,330 --> 00:06:44,437
Okay? And then what it does is
it goes out and

118
00:06:44,437 --> 00:06:46,843
it grabs that page and
back comes the page, and

119
00:06:46,843 --> 00:06:49,986
it puts the page in the database and
then it reads all the links.

120
00:06:49,986 --> 00:06:53,501
And then it adds empty slots in
the database that are the things that need

121
00:06:53,501 --> 00:06:54,680
to be crawled.

122
00:06:54,680 --> 00:06:58,440
Then it goes back to the top of the loop
and grabs the next open one, goes and

123
00:06:58,440 --> 00:07:00,880
grabs that page and then brings that page
and adds some text,

124
00:07:00,880 --> 00:07:04,170
reads the all the links in there,
adds it to the end of the thing.

125
00:07:04,170 --> 00:07:09,340
So our queue, our queue is actually
in the database.

126
00:07:09,340 --> 00:07:11,870
We have URLs that we've retrieved.

127
00:07:11,870 --> 00:07:13,290
And we have the data for those URLs.

128
00:07:13,290 --> 00:07:15,440
And we have URLs that are not yet
retrieved.

129
00:07:15,440 --> 00:07:20,440
And so we basically do a database query
to say what URLs are yet to be retrieved?

130
00:07:20,440 --> 00:07:23,540
And so because we're small and

131
00:07:23,540 --> 00:07:26,680
we're not Google,
this will grow really rapidly, right?

132
00:07:26,680 --> 00:07:28,990
We'll have a certain number
of pages in there, but

133
00:07:28,990 --> 00:07:32,940
our list of unretrieved pages
will just go and go and go.

134
00:07:32,940 --> 00:07:36,810
And that's why this code stays on
one website and doesn't wander off,

135
00:07:36,810 --> 00:07:39,300
because if it wandered
off we would never get

136
00:07:39,300 --> 00:07:42,180
anything interesting like one
page would point to another.

137
00:07:42,180 --> 00:07:44,480
It would take a long time to
get enough data on

138
00:07:45,780 --> 00:07:48,370
the whole Internet to make
these pictures make sense.

139
00:07:48,370 --> 00:07:53,000
So we're going to kind of artificially make
web pages that point to each other just so

140
00:07:53,000 --> 00:07:54,060
we can play with it.

141
00:07:54,060 --> 00:07:59,910
So this database ends up with a list of
URLs, some pages, and then some empty.

142
00:07:59,910 --> 00:08:00,720
Okay?

143
00:08:00,720 --> 00:08:05,554
This, again, like all of these
pull process, this can fail,

144
00:08:05,554 --> 00:08:12,110
you could stop it with like a Ctrl+C or
a Ctrl+Z or you shut your computer off.

145
00:08:12,110 --> 00:08:13,126
And it can be restarted.

146
00:08:15,086 --> 00:08:18,506
Because we keep the data on disk and
again you just blow it up and

147
00:08:18,506 --> 00:08:20,310
then you start it up again.

148
00:08:20,310 --> 00:08:23,650
Say, hey, go get me 1000 more and
then, hey get me 1000 more.

149
00:08:23,650 --> 00:08:27,910
And so this process sort of just
kind of fills,

150
00:08:27,910 --> 00:08:30,490
whenever you want you can get
more of these things, but

151
00:08:30,490 --> 00:08:34,270
at some point you switch to saying,
okay, I got enough. I want to do some work.

152
00:08:35,930 --> 00:08:40,380
And so the code that does the
calculations, so now we've got all these

153
00:08:40,380 --> 00:08:45,030
pages and some data and we keep track
of who links to who, right?

154
00:08:45,030 --> 00:08:47,010
This one links to that one, and
that one links to that one, and

155
00:08:47,010 --> 00:08:48,620
that one links to that one.

156
00:08:48,620 --> 00:08:54,320
And then we keep a page rank and
this is a program called sprank.

157
00:08:54,320 --> 00:08:56,820
It's ranking our spidered web pages.

158
00:08:56,820 --> 00:09:02,590
And sprank actually reads the pages,
and then it calculates new page rank,

159
00:09:02,590 --> 00:09:06,120
and it's also a loop that's
sort of updating the page rank.

160
00:09:06,120 --> 00:09:09,910
And so it starts with this
page points to that page, but

161
00:09:09,910 --> 00:09:14,990
then it starts going farther and farther
and farther, to more and more links.

162
00:09:14,990 --> 00:09:18,500
And the page rank is being shared
with all these pages, right?

163
00:09:18,500 --> 00:09:21,270
And so the more times you run this,
if you recall,

164
00:09:21,270 --> 00:09:25,770
I said at some point all the goodness
flows out of all these links, and

165
00:09:25,770 --> 00:09:27,870
then goodness from other links flows in.

166
00:09:27,870 --> 00:09:30,300
And at some point this number stabilizes,

167
00:09:30,300 --> 00:09:35,080
meaning that you can do this and these
numbers of page rank aren't changing, and

168
00:09:35,080 --> 00:09:37,660
then you kind of know you've
reached the stable point.

169
00:09:37,660 --> 00:09:40,630
And so this can run 500 times or
50 times, and

170
00:09:40,630 --> 00:09:43,350
at some point the page rank stabilizes.

171
00:09:44,600 --> 00:09:48,660
There's a real simple program
called spreset which sets all these

172
00:09:48,660 --> 00:09:51,840
numbers back to their default so
you can start this process again.

173
00:09:51,840 --> 00:09:55,860
So this is really very simple, it resets
the page rank to the initial value, and

174
00:09:55,860 --> 00:09:58,930
then you run the page rank
thing a whole bunch of times.

175
00:09:58,930 --> 00:10:01,610
And so if you imagine what Google is doing,

176
00:10:01,610 --> 00:10:03,160
they're doing this all at the same time.

177
00:10:03,160 --> 00:10:06,540
It's two o'clock in the morning,
their servers are bored, so

178
00:10:06,540 --> 00:10:08,427
they start pulling more pages in.

179
00:10:08,427 --> 00:10:11,993
It's two o'clock in the morning,
their servers are bored, they just gently,

180
00:10:11,993 --> 00:10:14,773
quietly re-compute the page rank
on all the pages to adjust for

181
00:10:14,773 --> 00:10:16,207
the fact that new pages come in.

182
00:10:16,207 --> 00:10:18,714
So these are being done
continuously inside of Google or

183
00:10:18,714 --> 00:10:20,690
any other search engine.

184
00:10:20,690 --> 00:10:24,170
For us, we'll do this,
then we'll do this for a while, and

185
00:10:24,170 --> 00:10:28,760
then we'll say okay, we've got good data,
and the page rank's been calculated.

186
00:10:28,760 --> 00:10:31,800
If you then pull more in,
you do have to run page rank again so

187
00:10:31,800 --> 00:10:34,160
it sort of restabilizes
based on the new pages.

188
00:10:34,160 --> 00:10:37,290
And you can do that as many times as you
want, and however much fun you want to do.

189
00:10:37,290 --> 00:10:41,390
And then we have some code that
could be like the search engine.

190
00:10:41,390 --> 00:10:44,170
If we were going to make like an spsearch,
which you could do.

191
00:10:46,330 --> 00:10:48,380
And it could take some characters and
whatever that,

192
00:10:48,380 --> 00:10:50,240
you'd have to write more code,
I don't have that code for you.

193
00:10:50,240 --> 00:10:55,150
But I have a thing that
prints out spdump.py

194
00:10:55,150 --> 00:10:58,490
that looks at all the pages and
what their current page rank is.

195
00:10:58,490 --> 00:11:00,790
Right now all the pages
have a page rank of one.

196
00:11:00,790 --> 00:11:05,150
Here's a page, page rank of one, and
who the friends are, etc., etc., etc.

197
00:11:05,150 --> 00:11:08,020
And so that sort of prints out
kind of like what we did before,

198
00:11:08,020 --> 00:11:09,570
it just prints a nice text output out.

199
00:11:10,720 --> 00:11:15,350
But then, if you're on this spjson, it
reads through all the page ranks and

200
00:11:15,350 --> 00:11:18,640
then creates some data,
again in the JavaScript format.

201
00:11:18,640 --> 00:11:20,950
And if you read this it's a JSON format.

202
00:11:20,950 --> 00:11:22,880
And again we talked about JSON.

203
00:11:22,880 --> 00:11:27,720
And then I've written this, which is an
HTML page that uses a visualization system

204
00:11:27,720 --> 00:11:33,530
called d3.js. And you can go read up on
how the d3 visualization system works.

205
00:11:33,530 --> 00:11:35,250
Be thankful that I wrote
all that stuff for you.

206
00:11:35,250 --> 00:11:39,310
And when this page loads,
it pulls all this in and

207
00:11:39,310 --> 00:11:43,700
then makes a really cool visualization
for you that you can grab and drag.

208
00:11:43,700 --> 00:11:46,860
And I'll show you how that all works
when I'm showing the example of how

209
00:11:46,860 --> 00:11:48,310
all this stuff works.

210
00:11:48,310 --> 00:11:52,660
Ultimately, what we have is sort of a more
step process where we have a gather

211
00:11:52,660 --> 00:11:57,520
process, we have a process to
sort of adjust the data, and

212
00:11:57,520 --> 00:12:00,240
then we have kind of a dump and
visualize process.

213
00:12:00,240 --> 00:12:04,290
But we do it all around this one
database called spider.sqljte so

214
00:12:04,290 --> 00:12:06,425
we can have all these processes work.

215
00:12:06,425 --> 00:12:07,355
Okay?

216
00:12:07,355 --> 00:12:11,745
So, up next we're going to come back and
talk more about the email data, and

217
00:12:11,745 --> 00:12:14,885
now we're going to work on
gigabytes of email data

218
00:12:14,885 --> 00:12:17,525
rather than a few thousand
characters of email data.