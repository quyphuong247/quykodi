0
00:00:00,000 --> 00:00:00,720
https://youtu.be/bG55HbzhKYk

1
00:00:00,720 --> 00:00:02,340
MATT MCGARRITY: In the last lecture, we looked at

2
00:00:02,340 --> 00:00:04,440
fallacies of claim in data.

3
00:00:04,440 --> 00:00:07,200
I want to end this week by talking about fallacies of

4
00:00:07,200 --> 00:00:08,900
reasoning and response.

5
00:00:08,900 --> 00:00:12,720
So these are more things not only to avoid committing, but make sure the

6
00:00:12,720 --> 00:00:15,660
audience hears you as not committing.

7
00:00:15,660 --> 00:00:18,800
So let's start with the fallacy of reasoning, a post hoc fallacy--

8
00:00:18,800 --> 00:00:20,745
post hoc ergo propter hoc.

9
00:00:20,745 --> 00:00:24,810
A post hoc fallacy is misreading a chronological order as a causal

10
00:00:24,810 --> 00:00:25,820
relationship.

11
00:00:25,820 --> 00:00:29,890
So, hypothetically you could say, oh, look, "Censoring cartoons is in the

12
00:00:29,890 --> 00:00:30,610
public interest.

13
00:00:30,610 --> 00:00:31,520
Look at Canada.

14
00:00:31,520 --> 00:00:35,530
They put restrictions on violent Saturday morning cartoons in 1991 and

15
00:00:35,530 --> 00:00:39,780
by 1997, the crime rate had dropped 15%." Boom.

16
00:00:39,780 --> 00:00:40,990
Who's winning that argument?

17
00:00:40,990 --> 00:00:42,170
This guy.

18
00:00:42,170 --> 00:00:44,650
OK, that's a post hoc fallacy, right?

19
00:00:44,650 --> 00:00:46,870
All we've seen is two things.

20
00:00:46,870 --> 00:00:52,100
You're saying that, you tell us a chronological order, they put

21
00:00:52,100 --> 00:00:55,550
restrictions and then by 1997, the crime rate a dropped.

22
00:00:55,550 --> 00:00:59,580
There's nothing in that argument that demonstrates causality.

23
00:00:59,580 --> 00:01:01,200
Nothing whatsoever.

24
00:01:01,200 --> 00:01:04,200
So, this is something you want to make sure that you avoid.

25
00:01:04,200 --> 00:01:07,770
But also, if you see the causal relationship or you're making the

26
00:01:07,770 --> 00:01:11,590
casual relationship, go out of your way to make sure that the audience

27
00:01:11,590 --> 00:01:14,550
understands how that relationship is performed.

28
00:01:14,550 --> 00:01:15,050
OK.

29
00:01:15,050 --> 00:01:16,880
So in this case, maybe you can make that case.

30
00:01:16,880 --> 00:01:20,640
You would need additional testimony from experts saying, yes, it was the

31
00:01:20,640 --> 00:01:24,310
1991 restrictions that led to this unit drop in crime, or at least

32
00:01:24,310 --> 00:01:26,170
significantly contributed to it.

33
00:01:26,170 --> 00:01:28,780
So you would need that type of evidence to demonstrate the

34
00:01:28,780 --> 00:01:33,610
relationship, the causal relationship, that's going on in that argument.

35
00:01:33,610 --> 00:01:35,750
So post hoc fallacy.

36
00:01:35,750 --> 00:01:37,360
Hasty generalization.

37
00:01:37,360 --> 00:01:40,280
Hasty generalization is a question of representativeness.

38
00:01:40,280 --> 00:01:45,260
A hasty generalization, the accusation is that there is insufficient data to

39
00:01:45,260 --> 00:01:46,215
prove the claim.

40
00:01:46,215 --> 00:01:49,500
There is insufficient evidence to merit what you're saying.

41
00:01:49,500 --> 00:01:53,540
So hypothetically I could say, Ah, "I'm in a Communications department

42
00:01:53,540 --> 00:01:55,250
here at the University of Washington.

43
00:01:55,250 --> 00:01:58,550
Before I came here, I was in a Communication department at Indiana

44
00:01:58,550 --> 00:01:59,480
University.

45
00:01:59,480 --> 00:02:05,010
So clearly, US state universities all have Communications departments."

46
00:02:05,010 --> 00:02:06,670
Well, that's a hasty generalization.

47
00:02:06,670 --> 00:02:11,910
I've got two data points on that, and I'm making claim that covers hundreds

48
00:02:11,910 --> 00:02:13,020
of universities--

49
00:02:13,020 --> 00:02:13,940
state universities--

50
00:02:13,940 --> 00:02:18,460
lots of universities, at least 50, maybe more.

51
00:02:18,460 --> 00:02:21,620
But that's insufficient data.

52
00:02:21,620 --> 00:02:23,300
That's a hasty generalization.

53
00:02:23,300 --> 00:02:27,790
Once again, you are convinced by your argument, so you want to make sure

54
00:02:27,790 --> 00:02:30,630
that that audience understands that you're not engaging in a hasty

55
00:02:30,630 --> 00:02:31,510
generalization.

56
00:02:31,510 --> 00:02:35,410
So if there's the threat of that, you want to be able to shore that up by

57
00:02:35,410 --> 00:02:39,630
demonstrating the representativeness of those data sources.

58
00:02:39,630 --> 00:02:41,940
So if you're worried that people are going to hear that as a hasty

59
00:02:41,940 --> 00:02:45,020
generalization, you could even plan in a spot where you're like, now, you may

60
00:02:45,020 --> 00:02:48,300
not think that's enough, but in fact, this expert indicates that it is.

61
00:02:48,300 --> 00:02:50,500
Or here's some additional evidence of this.

62
00:02:50,500 --> 00:02:54,050
Or give us some more statistical evidence to indicate a trend.

63
00:02:54,050 --> 00:02:57,440
But you want to avoid a hasty generalization, but these are ones

64
00:02:57,440 --> 00:03:01,330
that audiences can perceive fairly easily, unless you're doing your due

65
00:03:01,330 --> 00:03:06,320
diligence in making sure that you're clarifying the nature of the evidence

66
00:03:06,320 --> 00:03:07,880
to the claim here.

67
00:03:07,880 --> 00:03:09,115
A non sequitur.

68
00:03:09,115 --> 00:03:16,470
A non sequitur is that the claim has no relationship to the data.

69
00:03:16,470 --> 00:03:17,650
It does not follow--

70
00:03:17,650 --> 00:03:21,820
these two things-- the data, it's unclear of it's relationship.

71
00:03:21,820 --> 00:03:23,850
So I'm going to use an example here.

72
00:03:23,850 --> 00:03:24,850
This happened to me.

73
00:03:24,850 --> 00:03:26,060
I know it happened to a lot of others.

74
00:03:26,060 --> 00:03:28,110
I've certainly seen stuff like this online.

75
00:03:28,110 --> 00:03:34,400
So any time a computer algorithm is making recommendations for you.

76
00:03:34,400 --> 00:03:37,742
So this happens if you're buying books through amazon.com.

77
00:03:37,742 --> 00:03:41,020
So you go on amazon.com, it looks at what you've bought in the past, and it

78
00:03:41,020 --> 00:03:42,770
makes recommendations based on that.

79
00:03:42,770 --> 00:03:44,410
So it's, in essence, making an argument.

80
00:03:44,410 --> 00:03:48,550
So what I see, and what others have seen is something like, you go in and

81
00:03:48,550 --> 00:03:52,740
it says, oh, "You will like Bill Clinton's autobiography, My Life,

82
00:03:52,740 --> 00:03:57,300
because you also bought A Handbook of Logical Fallacies." Hmm, Amazon, what

83
00:03:57,300 --> 00:03:58,730
are you saying?

84
00:03:58,730 --> 00:04:00,480
Are you weighing in on this?

85
00:04:00,480 --> 00:04:02,210
I kind of get an implication here.

86
00:04:02,210 --> 00:04:06,200
But regardless of your political stripe you could look at that and say,

87
00:04:06,200 --> 00:04:10,600
I'm not entirely sure of the relationship of the data--

88
00:04:10,600 --> 00:04:12,900
I did buy A Handbook of Logical Fallaciess--

89
00:04:12,900 --> 00:04:16,459
to the claim, I will like Bill Clinton's autobiography.

90
00:04:16,459 --> 00:04:20,640
What seems to be going on there, of course implied is, you like lying in

91
00:04:20,640 --> 00:04:23,960
the abstract, have some in person, in the specific.

92
00:04:23,960 --> 00:04:31,230
But if you are reading that and you're not quite a strong disliker of Bill

93
00:04:31,230 --> 00:04:35,110
Clinton, that could be seen as a non sequitur.

94
00:04:35,110 --> 00:04:38,230
So those are some fallacies of reasoning.

95
00:04:38,230 --> 00:04:40,260
Some fallacies of response.

96
00:04:40,260 --> 00:04:41,940
We can talk about a straw argument.

97
00:04:41,940 --> 00:04:46,320
So again the etymology here is, instead of doing argumentation, doing

98
00:04:46,320 --> 00:04:52,170
battle with a live opponent who can fight back, you build an effigy of

99
00:04:52,170 --> 00:04:54,310
that person and just wail on it.

100
00:04:54,310 --> 00:04:57,540
I'm winning this argument, because look, they're not fighting back.

101
00:04:57,540 --> 00:05:02,950
So a straw argument is attacking a weak position that's not really held

102
00:05:02,950 --> 00:05:04,350
by one's opponents.

103
00:05:04,350 --> 00:05:07,940
So, again in this discussion about candy tax here in the state of

104
00:05:07,940 --> 00:05:08,470
Washington.

105
00:05:08,470 --> 00:05:12,370
So if they argue, "A vote for the candy tax is a vote for ending

106
00:05:12,370 --> 00:05:17,080
childhood innocence." No one who was voting for the candy tax was getting

107
00:05:17,080 --> 00:05:23,900
up there on the floor the legislature and saying, children are awful.

108
00:05:23,900 --> 00:05:27,770
They need to learn the bitterness of life early and the only way to do that

109
00:05:27,770 --> 00:05:30,670
is to take away the sweetness of there candy.

110
00:05:30,670 --> 00:05:32,500
No one was making that argument.

111
00:05:32,500 --> 00:05:38,650
So to manufacture that as being a principled argument in favor of the

112
00:05:38,650 --> 00:05:42,700
candy tax is disingenuous and it's straw argumentation.

113
00:05:42,700 --> 00:05:46,670
So a strong argument is attacking a fake argument that's not

114
00:05:46,670 --> 00:05:47,960
held by one's opponent.

115
00:05:47,960 --> 00:05:51,300
An ad hominem is just skipping the argument altogether and

116
00:05:51,300 --> 00:05:52,800
attacking the person.

117
00:05:52,800 --> 00:05:53,400
OK.

118
00:05:53,400 --> 00:06:01,690
So instead it would be like, the representative from Walla Walla is in

119
00:06:01,690 --> 00:06:06,770
favor of the candy tax, but I would it like to be on record that he is a jerk

120
00:06:06,770 --> 00:06:08,490
and no one should agree with him.

121
00:06:08,490 --> 00:06:10,230
That would clearly be an ad hominem.

122
00:06:10,230 --> 00:06:13,070
So I want to end this discussion of ad hominem with,

123
00:06:13,070 --> 00:06:14,350
actually, a video example.

124
00:06:14,350 --> 00:06:19,360
So at the end of this video, you'll see on the lesson page for this week a

125
00:06:19,360 --> 00:06:20,430
YouTube clip.

126
00:06:20,430 --> 00:06:24,060
And this comes from the show Anderson Cooper 360 on CNN.

127
00:06:24,060 --> 00:06:28,780
And what you're seeing here is a clash between two speakers.

128
00:06:28,780 --> 00:06:33,920
Father Time in this-- you'll see on here, he looks like Santa except more

129
00:06:33,920 --> 00:06:36,670
kind, more generous--

130
00:06:36,670 --> 00:06:38,230
is James Randi.

131
00:06:38,230 --> 00:06:39,510
He's a famous skeptic.

132
00:06:39,510 --> 00:06:43,030
And the other person on the screen is the publicist for Sylvia Browne.

133
00:06:43,030 --> 00:06:45,030
Sylvia Browne's a psychic.

134
00:06:45,030 --> 00:06:50,610
And in this back and forth, what is happening is James Randi is critiquing

135
00:06:50,610 --> 00:06:55,110
Sylvia Browne for making psychic claims that are just false, and then

136
00:06:55,110 --> 00:06:56,940
her publicist is responding to this.

137
00:06:56,940 --> 00:07:00,250
Now what happens is-- although won't hear James Randi speak-- what you're

138
00:07:00,250 --> 00:07:04,280
seeing is, the publicist is attacking James Randi.

139
00:07:04,280 --> 00:07:08,420
Instead of responding to his claims about the problems with psychics, or

140
00:07:08,420 --> 00:07:09,950
the fact that they're often wrong.

141
00:07:09,950 --> 00:07:13,080
Instead of responding to those claims, she bypasses those claims and goes

142
00:07:13,080 --> 00:07:14,600
immediately after him.

143
00:07:14,600 --> 00:07:15,545
James Randi's an atheist.

144
00:07:15,545 --> 00:07:17,650
And so she says, ah, you can't trust him.

145
00:07:17,650 --> 00:07:19,120
He's an atheist.

146
00:07:19,120 --> 00:07:21,240
And even Anderson Cooper says, well, that kind of

147
00:07:21,240 --> 00:07:22,630
sounds like an ad hominem.

148
00:07:22,630 --> 00:07:26,270
And yet what I find interesting in this clip is not only that you would

149
00:07:26,270 --> 00:07:28,480
be mean to Father Time, but--

150
00:07:28,480 --> 00:07:33,610
because that's kind of who he looks like-- but that this ad hominem could

151
00:07:33,610 --> 00:07:37,840
actually be made a little bit more valid if she changed the

152
00:07:37,840 --> 00:07:38,650
nature of the claim.

153
00:07:38,650 --> 00:07:44,780
If she said, in fact, look you can't really critique Sylvia Browne because

154
00:07:44,780 --> 00:07:47,090
you have to understand psychic ability.

155
00:07:47,090 --> 00:07:50,380
And the only way you could understand psychic ability

156
00:07:50,380 --> 00:07:52,080
is if you are religious.

157
00:07:52,080 --> 00:07:53,430
So that would be--

158
00:07:53,430 --> 00:07:57,500
that changes the direction of the argument, but a lot of these fallacies

159
00:07:57,500 --> 00:08:01,080
can be turned into stronger arguments if you just sort of tweak them a

160
00:08:01,080 --> 00:08:01,840
little bit.

161
00:08:01,840 --> 00:08:04,720
But what was happening in this clip was a contest over fact.

162
00:08:04,720 --> 00:08:07,000
Was Sylvia Browne right or wrong?

163
00:08:07,000 --> 00:08:10,540
That's adjudicated in terms of whether or not it happened or didn't happen.

164
00:08:10,540 --> 00:08:13,560
That doesn't change by the fact that James Randi's an atheist.

165
00:08:13,560 --> 00:08:17,510
So go ahead and take a look at that clip, and then we'll pick up in the

166
00:08:17,510 --> 00:08:20,000
last little bit and just sort of finish out this week and get you ready

167
00:08:20,000 --> 00:08:21,250
for week nine.

168
00:08:21,250 --> 00:08:22,270


