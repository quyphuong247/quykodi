1
00:00:02,089 --> 00:00:04,019
Why do smart people make dumb decisions?

2
00:00:04,019 --> 00:00:08,000
Why do conspiracy theorists think that we
didn’t land on the moon or that Hillary

3
00:00:08,000 --> 00:00:09,759
Clinton is a space alien?

4
00:00:09,759 --> 00:00:13,980
And why won’t Bernice admit that the new
Superman movie just isn’t very good?

5
00:00:13,980 --> 00:00:18,109
We’ve talked about cognition before. We
usually refer to it as the process that we

6
00:00:18,109 --> 00:00:23,350
use to think and solve crossword puzzles and
stuff. But really, cognition involves knowing,

7
00:00:23,350 --> 00:00:26,670
remembering, understanding, communicating,
and to a certain extent, learning.

8
00:00:26,670 --> 00:00:32,590
And as truly wonderful as our brains are,
we can be spectacularly bad at ALL of these

9
00:00:32,590 --> 00:00:32,840
things.

10
00:00:32,759 --> 00:00:37,179
We used to think our cognition worked like
a computer -- logically processing information.

11
00:00:37,179 --> 00:00:41,960
But that cabbage-sized chunk of pink, wet
brain matter in your skull can do a lot more

12
00:00:41,960 --> 00:00:45,859
than math, and the things that it does are
certainly not always logical.

13
00:00:45,859 --> 00:00:50,530
Many experts argue that it’s cognition that
makes us truly human, and that everything

14
00:00:50,530 --> 00:00:55,649
that comes with it -- our preferences, prejudices,
fears, and intuitions -- are what make us

15
00:00:55,649 --> 00:00:57,350
the individuals that we are.

16
00:00:57,350 --> 00:01:00,960
We’re not the only animals that show some
evidence of cognition, of course: Chimps and

17
00:01:00,960 --> 00:01:05,859
gorillas exhibit insight and planning; crows
use tools; elephants teach each other.

18
00:01:05,859 --> 00:01:10,599
But our capacity as humans to figure stuff
out is matched only by our ability to totally

19
00:01:10,599 --> 00:01:16,239
misjudge stuff. As prone as we are to brilliance
and insight, we’re equally likely to succumb

20
00:01:16,239 --> 00:01:18,778
to irrational thinking and false intuition.

21
00:01:18,778 --> 00:01:23,539
So, to borrow a riff from Rene Descartes,
you think, therefore you are.

22
00:01:23,539 --> 00:01:30,539
Which means you’re brilliant a lot of the
time. And sometimes, you’re just going to

23
00:01:35,239 --> 00:01:38,298
look stupid.

24
00:01:38,299 --> 00:01:39,829
[INTRO]

25
00:01:39,828 --> 00:01:43,348
We all want to make sense of the world. And
one of the major ways our cognition allows

26
00:01:43,349 --> 00:01:49,099
us do that is by forming concepts -- mental
groupings of similar objects, people, ideas,

27
00:01:49,099 --> 00:01:51,649
or events. We like to lump things together.

28
00:01:51,649 --> 00:01:55,899
Concepts simplify our thinking in such a fundamental
way that we usually don’t have to stop and

29
00:01:55,899 --> 00:01:57,929
think about using them, they’re just there.

30
00:01:57,929 --> 00:02:01,989
And yet without concepts, we’d need a unique
name for everything. You couldn’t just ask

31
00:02:01,989 --> 00:02:08,090
me to shake the anglerfish -- because there’d
be no concept of shake or fish, let alone

32
00:02:08,090 --> 00:02:09,709
stuffed, blue anglerfish.

33
00:02:09,709 --> 00:02:13,459
And if I told you I was devastated that I
lost my anglerfish -- which I probably would

34
00:02:13,459 --> 00:02:18,680
be -- I’d also have to explain my emotions,
their intensities, even the words themselves

35
00:02:18,680 --> 00:02:19,959
that I had to use.

36
00:02:19,959 --> 00:02:25,158
So basically, without concepts, no one would
ever get anything done. We’d all be like

37
00:02:25,158 --> 00:02:28,878
a bunch of ents taking all morning to say
“Hey, what’s up?”

38
00:02:28,878 --> 00:02:34,429
We often organize our concepts by forming
prototypes--mental images or pinnacle examples

39
00:02:34,430 --> 00:02:35,459
of a certain thing.

40
00:02:35,459 --> 00:02:39,069
For example, if I say “bird”--the general
shape of a songbird probably pops into your

41
00:02:39,068 --> 00:02:44,750
head before like, a penguin or chicken or
emu, because robins and cardinals more closely

42
00:02:44,750 --> 00:02:48,079
resemble our bird prototype.
Still, if I show you a picture of some crazy

43
00:02:48,079 --> 00:02:51,909
creature you’ve never seen before, and you
note that it has feathers and a beak, you’ll

44
00:02:51,908 --> 00:02:56,108
probably file it under the bird category because
it more closely resembles your concept of

45
00:02:56,109 --> 00:03:00,280
bird than your concept of rodent or overcoat
or footstool.

46
00:03:00,280 --> 00:03:05,090
Concepts and prototypes speed up our thinking,
but they also can box in our thinking, and

47
00:03:05,090 --> 00:03:08,370
lead to prejudice if we see something that
doesn’t fit our prototypes.

48
00:03:08,370 --> 00:03:12,629
A hundred years ago the sight of a female
doctor might have caused some heads to explode,

49
00:03:12,628 --> 00:03:16,539
because in peoples’ tiny minds, the prototypes
of “doctor” and “woman” didn’t have

50
00:03:16,539 --> 00:03:21,188
any overlap. And actually some people today
still feel that way. Haters gonna hate.

51
00:03:21,188 --> 00:03:26,358
So it’s important to actively keep your
mind open mind to make room for evolving concepts,

52
00:03:26,359 --> 00:03:29,549
and remember that concepts may sometimes hurt
as much as they help.

53
00:03:29,549 --> 00:03:33,498
One of the biggest ways our cognition works
to our benefit, though, is through our ability

54
00:03:33,498 --> 00:03:36,030
to solve problems.
We use our problem-solving skills all the

55
00:03:36,030 --> 00:03:40,829
time: How to assemble Scandinavian furniture,
bake muffins with a missing ingredient, or

56
00:03:40,829 --> 00:03:43,280
handle the crushing disappointment of the
new Superman movie.

57
00:03:43,280 --> 00:03:47,878
And we approach problem-solving in different
ways -- sometimes we value speed; other times,

58
00:03:47,878 --> 00:03:48,449
accuracy.

59
00:03:48,449 --> 00:03:52,158
Some problems we figure out using trial and
error--you know, you try something and if

60
00:03:52,158 --> 00:03:54,858
it doesn’t work, try it a different way,
and keep at it until something works. Trial

61
00:03:54,859 --> 00:03:59,950
and error is slow and deliberate--which may
be good or bad, depending on the problem.

62
00:03:59,949 --> 00:04:04,539
We can also use algorithms and heuristics
to come up with solutions.

63
00:04:04,539 --> 00:04:10,688
Algorithms are logical, methodical, step-by-step
procedures that guarantee an eventual solution,

64
00:04:10,688 --> 00:04:14,358
though they may be slow to work through.
Heuristics, on the other hand, are more like

65
00:04:14,359 --> 00:04:19,199
mental shortcuts -- simple strategies that
allow us to solve problems faster, although

66
00:04:19,199 --> 00:04:23,220
they’re more error-prone than algorithms.
Say you’re at the store, looking for a family-sized

67
00:04:23,220 --> 00:04:28,560
bottle of Sriracha. You could use an algorithm
and methodically check every shelf and aisle

68
00:04:28,560 --> 00:04:33,519
in the store. Or you could use heuristics
and first search the Asian or condiment sections--the

69
00:04:33,519 --> 00:04:36,549
places that make the most sense based on what
you already know.

70
00:04:36,550 --> 00:04:40,079
Heuristics may be way faster, but the algorithmic
approach guarantees you won’t overlook the

71
00:04:40,079 --> 00:04:43,459
sauce along the way, because they stuck it
in the deli or whatever dumb thing they did

72
00:04:43,459 --> 00:04:44,079
this week.

73
00:04:44,079 --> 00:04:48,300
So algorithms, heuristics, and trial-and-error
are problem-solving strategies that involve

74
00:04:48,300 --> 00:04:51,579
a plan of attack.
But sometimes we get lucky while puzzling

75
00:04:51,579 --> 00:04:56,329
out a problem, and Aha!, out of nowhere a
sudden flash of insight that solves our problem.

76
00:04:56,329 --> 00:05:01,448
I’ll use orange in the muffin recipe instead
of lemon! Or, Sriracha lives in the Mexican

77
00:05:01,449 --> 00:05:04,020
section! For some reason!
Neuroscientists have actually watched that

78
00:05:04,019 --> 00:05:07,620
kind of sudden, happy brain flash on neuroimaging
screens.

79
00:05:07,620 --> 00:05:10,639
In one experiment, they gave subjects a problem
to solve:

80
00:05:10,639 --> 00:05:16,360
What word can be added to the three words
CRAB, PINE, and SAUCE to create a new compound

81
00:05:16,360 --> 00:05:18,729
word?
Then they asked the subjects to press a button

82
00:05:18,728 --> 00:05:19,878
when they had the answer.

83
00:05:19,879 --> 00:05:23,740
While the subjects thought about it, scans
showed activity in their frontal lobes, the

84
00:05:23,740 --> 00:05:27,370
areas involved in the focused attention of
typical problem-solving.

85
00:05:27,370 --> 00:05:32,000
But right at the Aha! moment, just as they
pushed the button, there was a clear burst

86
00:05:32,000 --> 00:05:37,819
of activity just above the ear in the right
temporal lobe, which, among many other things,

87
00:05:37,819 --> 00:05:38,979
is involved with recognition.

88
00:05:38,978 --> 00:05:43,800
The answer, by the way, we already gave you
the hint earlier in the episode.

89
00:05:43,800 --> 00:05:49,270
Where’s my fish?
Those sudden bursts of insight are awesome,

90
00:05:49,269 --> 00:05:52,719
but you can’t count on them to solve all
your problems. And just because something

91
00:05:52,720 --> 00:05:57,639
feels, doesn’t mean it’s truly correct.
Because as inventive and smartypants as we

92
00:05:57,639 --> 00:06:02,069
may be, our cognition often leads us astray
in all kinds of ways.

93
00:06:02,069 --> 00:06:06,770
For instance, we often look for, and favor,
evidence that verifies our ideas, while we’re

94
00:06:06,769 --> 00:06:11,769
more likely to avoid or ignore contradictory
evidence -- a tendency known as confirmation

95
00:06:11,769 --> 00:06:16,799
bias. This is really similar to the overconfidence
we’ve talked about, when you’re basically

96
00:06:16,800 --> 00:06:20,288
more confident than you are correct.
When this kind of cognitive bias takes hold,

97
00:06:20,288 --> 00:06:24,769
you might cling to your initial conceptions
in a kind of belief perseverance, even in

98
00:06:24,769 --> 00:06:29,680
the face of clear proof to the contrary.
This happens all the time, and it can be maddening

99
00:06:29,680 --> 00:06:36,158
for people watching it happen. People still
think that the earth is flat! It’s like...WHAT?

100
00:06:36,158 --> 00:06:41,079
HOW? There’s space pictures!
I probably don’t need to tell you -- people

101
00:06:41,079 --> 00:06:46,550
can really get weird and defensive when they
evade facts and choose to see only the information

102
00:06:46,550 --> 00:06:50,370
that confirms their beliefs.
They may even become functionally fixed, unable

103
00:06:50,370 --> 00:06:55,569
to view a problem from a new perspective.
Instead they just keep approaching a situation

104
00:06:55,569 --> 00:06:58,509
with the same mental set, especially if it’s
worked in the past.

105
00:06:58,509 --> 00:07:01,479
Say you’ve got a nail sticking out from
a board, and you’re like “I need to take

106
00:07:01,478 --> 00:07:07,228
care of that!” There’s rocks, and bricks
all around you. But because of your functional

107
00:07:07,228 --> 00:07:11,779
fixedness on the idea that only hammers work
on nails, you don’t even consider hitting

108
00:07:11,779 --> 00:07:14,989
it with the brick, and instead you waste a
bunch of time in the garage looking for a

109
00:07:14,990 --> 00:07:19,250
hammer, and you’re angry and frustrated,
and there’s still a nail sticking up from

110
00:07:19,250 --> 00:07:22,560
the board.
So, our mental set predisposes how we think,

111
00:07:22,560 --> 00:07:26,649
just as you’ll remember that our perceptual
set predisposes how we perceive.

112
00:07:26,649 --> 00:07:32,008
This is what makes heuristics -- those super-convenient
mental shortcuts that we all use -- so easily

113
00:07:32,009 --> 00:07:34,990
fallible.
In the 1970s, cognitive psychologists Amos

114
00:07:34,990 --> 00:07:40,009
Tversky and Daniel Kahneman researched how
we make snap judgments, and discovered one

115
00:07:40,009 --> 00:07:43,939
way smart people make dumb decisions.
They found that people believe an event will

116
00:07:43,939 --> 00:07:48,569
be more likely to occur if they can conjure
up examples or memories of it, especially

117
00:07:48,569 --> 00:07:52,389
if those examples are particularly vivid,
scary, or awesome.

118
00:07:52,389 --> 00:07:56,819
So, say you’re in a casino and you win two
dollars at a slot machine. Suddenly every

119
00:07:56,819 --> 00:08:01,509
flashing light and ringing bell in the place
goes off. But when you lose -- which is the

120
00:08:01,509 --> 00:08:05,699
vast majority of the time -- it’s just...crickets.
With all their lights and noise-making, the

121
00:08:05,699 --> 00:08:11,530
casino makes sure that wins are super vivid
and memorable, while losses just go away unacknowledged.

122
00:08:11,529 --> 00:08:14,579
That way, the next time you’re standing
there with 100 bucks in your pocket, you’re

123
00:08:14,579 --> 00:08:18,698
more likely to overestimate your chances of
winning, because the memories of winning are

124
00:08:18,699 --> 00:08:21,598
more striking.
The more mentally available those memories

125
00:08:21,598 --> 00:08:26,759
are, the more it seems that it’s going to
happen again. This is known as the availability

126
00:08:26,759 --> 00:08:28,669
heuristic.
And it can warp our judgements of people,

127
00:08:28,668 --> 00:08:34,269
too. If we keep remembering news footage that
shows people of a given group shooting guns,

128
00:08:34,269 --> 00:08:39,329
that can shape our impression of the entire
group -- even if what we saw was only a tiny

129
00:08:39,330 --> 00:08:43,379
minority within that group.
Essentially, we are great at fearing the wrong

130
00:08:43,379 --> 00:08:47,879
things. We worry about being killed in a plane
crash or getting bitten in half by a shark

131
00:08:47,879 --> 00:08:51,909
or accidentally choking on a dumpling.
Thanks to our brain’s b-roll of horrific

132
00:08:51,909 --> 00:08:56,149
images, we come to fear what’s actually
very rare, instead of worrying about much

133
00:08:56,149 --> 00:09:01,199
more common, but less memorable ends like
car accidents, cancer, and heart failure.

134
00:09:01,200 --> 00:09:06,190
Our thinking can also be swayed by framing,
or how an issue is presented. Imagine you’re

135
00:09:06,190 --> 00:09:10,460
considering climbing Everest or getting a
nose job or eating a bowl of raw blowfish.

136
00:09:10,460 --> 00:09:14,389
I can frame the risks in different ways. Telling
you that you’ve got a 95 percent chance

137
00:09:14,389 --> 00:09:18,769
of survival sounds a lot different than saying
five out of a hundred people die doing this

138
00:09:18,769 --> 00:09:23,399
activity, though the information is the same.
Our cognitive minds are capable of incredible

139
00:09:23,399 --> 00:09:29,230
intellectual feats and tremendous failures.
We can solve problems better than any organism

140
00:09:29,230 --> 00:09:33,769
on the planet, but given the chance, we can
also mess up a pretty simple judgment every

141
00:09:33,769 --> 00:09:36,419
day of the week.
But if we’re mindful of our capacity for

142
00:09:36,419 --> 00:09:41,209
error -- and if we honor our ingenuity and
intellect -- I think our ability to solve

143
00:09:41,210 --> 00:09:45,210
any problem is nearly infinite. And that,
gives me a lot of hope.

144
00:09:45,210 --> 00:09:49,889
Seriously though where is my fish?
Today you learned how we use concepts, prototypes,

145
00:09:49,889 --> 00:09:55,639
and our mental sets to think and communicate,
and how algorithms, heuristics, and insight

146
00:09:55,639 --> 00:10:01,279
help us solve problems. You also learned about
how fixation, the availability heuristic,

147
00:10:01,279 --> 00:10:05,819
fear, overconfidence, and belief perseverance
can get in the way of good decision-making

148
00:10:05,820 --> 00:10:08,060
and thinking.
Thank you for watching, especially to our

149
00:10:08,059 --> 00:10:11,729
Subbable subscribers, who make this whole
channel possible. If you’d like to sponsor

150
00:10:11,730 --> 00:10:15,620
an episode of Crash Course, get a special
Laptop Decal, or even be animated into an

151
00:10:15,620 --> 00:10:21,289
upcoming episode, just go to Subbable.com/crashcourse.
This episode was written by Kathleen Yale,

152
00:10:21,289 --> 00:10:25,519
edited by Blake de Pastino, and our consultant
is Dr. Ranjit Bhagwat. Our director and editor

153
00:10:25,519 --> 00:10:29,909
is Nicholas Jenkins, the script supervisor
is Michael Aranda, who is also our sound designer,

154
00:10:29,909 --> 00:10:31,419
and the graphics team is Thought Café.

