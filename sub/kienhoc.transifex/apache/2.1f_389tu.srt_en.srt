0
00:00:00,000 --> 00:00:00,769
https://youtu.be/nqH2ipzFz_A

1
00:00:00,769 --> 00:00:02,560
SPEAKER: To help you get started with Spark

2
00:00:02,560 --> 00:00:05,480
and to make you a better Spark developer,

3
00:00:05,480 --> 00:00:09,040
there's a lot of documentation and resources available.

4
00:00:09,040 --> 00:00:11,180
So the first place to start, always,

5
00:00:11,180 --> 00:00:13,880
is with the Spark online documentation.

6
00:00:13,880 --> 00:00:19,660
And you can find it at Spark.Apache.org/docs/latest,

7
00:00:19,660 --> 00:00:26,270
and then select Python from the API docs dropdown.

8
00:00:26,270 --> 00:00:29,730
Another good source of information and examples

9
00:00:29,730 --> 00:00:31,330
is the Databrick's guide.

10
00:00:31,330 --> 00:00:33,120
You can click on the question mark

11
00:00:33,120 --> 00:00:35,140
and then click on Databrick's Guide.

12
00:00:35,140 --> 00:00:36,460
And browse the notebooks.

13
00:00:36,460 --> 00:00:39,180
You can also copy the notebooks into your workspace

14
00:00:39,180 --> 00:00:45,110
and then run the examples that are in those notebooks.

15
00:00:45,110 --> 00:00:48,830
Another good resource is the technical blogs.

16
00:00:48,830 --> 00:00:50,610
And here are three technical blogs

17
00:00:50,610 --> 00:00:52,670
that you might find interesting to read.

18
00:00:52,670 --> 00:00:54,670
They'll oftentimes have news about what's

19
00:00:54,670 --> 00:00:58,010
upcoming for Spark, and also examples of performing

20
00:00:58,010 --> 00:01:00,220
common tasks using Sparks.

21
00:01:00,220 --> 00:01:02,110
Here are the blogs for Databricks,

22
00:01:02,110 --> 00:01:06,110
for Cloudera, and for IBM.

23
00:01:06,110 --> 00:01:08,880
You can also find Spark on YouTube.

24
00:01:08,880 --> 00:01:13,410
So check out the Apache Spark YouTube channel.

25
00:01:13,410 --> 00:01:16,540
There is also a very large Spark community.

26
00:01:16,540 --> 00:01:20,910
And you can find it by going to the Spark.Apache.org website,

27
00:01:20,910 --> 00:01:23,710
and going to community.html.

28
00:01:23,710 --> 00:01:26,390
And you'll see there are two sets of mailing lists.

29
00:01:26,390 --> 00:01:28,780
There are mailing lists for users,

30
00:01:28,780 --> 00:01:31,490
and there are also mailing lists for developers.

31
00:01:31,490 --> 00:01:36,510
You can also see events and meet ups that are upcoming.

32
00:01:36,510 --> 00:01:40,720
There is a very large Apache Spark meet up community.

33
00:01:40,720 --> 00:01:43,660
It has several hundred thousand members,

34
00:01:43,660 --> 00:01:48,490
and hundreds of meet ups that occur around the globe.

35
00:01:48,490 --> 00:01:51,850
So go to Spark.Meetup.com, and you can find out

36
00:01:51,850 --> 00:01:56,360
more about meet ups nearby.

37
00:01:56,360 --> 00:02:00,140
Databrick's forum, at Forums.Databricks.com,

38
00:02:00,140 --> 00:02:02,890
is a community forum for Databricks users.

39
00:02:02,890 --> 00:02:06,550
So you'll find mostly Databrick specific questions and answers,

40
00:02:06,550 --> 00:02:10,120
and you'll find a few general Spark questions and answers

41
00:02:10,120 --> 00:02:12,700
also.

42
00:02:12,700 --> 00:02:16,480
Spark Packages is a repository where you can find

43
00:02:16,480 --> 00:02:19,560
software packages for Spark.

44
00:02:19,560 --> 00:02:23,270
These are user provided and developed Spark extensions,

45
00:02:23,270 --> 00:02:24,930
and they're rated by the community.

46
00:02:24,930 --> 00:02:27,230
So you can see which extensions are really good,

47
00:02:27,230 --> 00:02:29,703
and which extensions might need a little bit more work.

48
00:02:29,703 --> 00:02:32,270

49
00:02:32,270 --> 00:02:35,220
If you're really interested in how Spark works,

50
00:02:35,220 --> 00:02:37,600
you can look at the GitHub repository.

51
00:02:37,600 --> 00:02:40,580
Apache Spark is completely open source.

52
00:02:40,580 --> 00:02:42,990
And so you can browse through the actual code

53
00:02:42,990 --> 00:02:45,910
that runs everything that you're using in Spark.

54
00:02:45,910 --> 00:02:48,220
For detailed explanations, read the comments

55
00:02:48,220 --> 00:02:50,170
that are in the code.

