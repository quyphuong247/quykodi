To complete the course lab exercises, you will be using the free Databricks Community Edition platform. By using the free platform, you will not need to install any Spark software on your machine. You can use any Internet-connected computer with the Google Chrome (preferred) or Mozilla Firefox browsers. Note that Internet Explorer, Edge, and Safari are not supported. Databricks Community Edition provides a powerful, complete Spark environment that includes a mini 6GB cluster running on Amazon AWS, and interactive notebook environment with visualizations and dashboards, and public environment to share your work. 

Here is a huge list on GitHub of Awesome Public Datasets [https://github.com/caesar0301/awesome-public-datasets], most of which are free. After taking this course, you can download one of the datasets, import it into your Databricks Community Edition account, and explore it with Apache Spark!

(Optional Reading) This paper, Structured Open Urban Data: Understanding the Landscape [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4174913/pdf/big.2014.0020.pdf], examines over 9,000 open data sets from 20 cities in North America, and presents general statistics about the content, size, nature, and popularity of the different data sets, and also examines the data quality issues and time-related aspects of the various datasets.

The City of San Francisco has an extensive collection of online city records [https://data.sfgov.org]. These data cover public safety, health, transportation, housing and many other topics. The data, together with public social media, can provide an unprecedented window into the City's operations. The data is freely available for anyone to explore. Many other cities are also putting their records online. Let's consider a couple of the types of  questions one can ask using this data, but there are many others:

1. In the health section of sf.data.gov there is an extensive set of records about restaurant inspections. San Francisco has the largest number of restaurants per capita of any major city in the United States. Tracking and maintaining the quality of those restaurants is an ongoing challenge for inspectors. An interesting question would be could you create an "early warning system" based on social media (e.g., is it possible to predict restaurants in need of inspection from Yelp reviews?)? This question could be partially answered by building a machine learning classifier using historical social media reviews (e.g., from Yelp or Trip Advisor) and city records of inspections.

2. The City receives many 3-1-1 reports (non-emergency incident reports from citizens). But some of these reports predict serious incidents, which may be recorded later in police reports. Consider the challenge of mining the CABLE reported incidents, and looking for text markers that predict future police reports. This challenge would require tying the two tabular datasets together, a process that is complicated by noisy data - would it be better to tie the datasets together by the name of the protagonist or the location (address) of the incident? This is an entity resolution problem, as the protagonists' names might be listed differently in the two datasets (e.g., Anthony Joseph versus A. Joseph), and the same applies to address information in the two datasets (e.g., SF City Hall versus 1 Dr Carlton B Goodlett Pl, San Francisco, CA 94102). You will explore the entity resolution problem in Lab #3.  After combining the two datasets, the next step would be to look for keywords in the police report marking the type of incident, and attempt to predict incidents from the full text of the CABLE report.

These two are just two challenges, but there are many more are possible ones from this dataset.

User Defined Functions (UDFs): Note that UDFs in Python are slow, so whenever possible, consider using built-in functions instead. For example, instead of creating a lambda function and using a UDF  to subtract one from the values of a column, you should use a select transformation to perform the subtraction.