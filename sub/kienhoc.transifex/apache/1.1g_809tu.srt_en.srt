0
00:00:00,000 --> 00:00:00,830
https://youtu.be/dCzidQ6hAhA

1
00:00:00,830 --> 00:00:04,010
SPEAKER: Now let's talk about Spark actions.

2
00:00:04,010 --> 00:00:07,150
Actions are what causes Spark to execute the recipe

3
00:00:07,150 --> 00:00:11,370
to transform a source into data frames,

4
00:00:11,370 --> 00:00:13,940
and ultimately, it's the mechanism for getting results

5
00:00:13,940 --> 00:00:15,860
out of Spark.

6
00:00:15,860 --> 00:00:18,280
So some of the useful actions that you'll probably use

7
00:00:18,280 --> 00:00:22,320
include things like show, which prints out the first n

8
00:00:22,320 --> 00:00:25,390
rows of a data frame, and take, which

9
00:00:25,390 --> 00:00:29,930
returns the first n rows of a data frame as a list of row.

10
00:00:29,930 --> 00:00:32,820
Collect is an action that returns

11
00:00:32,820 --> 00:00:37,040
all the records of a data frame as a list of row.

12
00:00:37,040 --> 00:00:39,560
Now, you should be very careful when using collect,

13
00:00:39,560 --> 00:00:42,570
because it returns all of the records.

14
00:00:42,570 --> 00:00:44,570
So that means all of the distributed data that's

15
00:00:44,570 --> 00:00:46,111
part of your data frame that's stored

16
00:00:46,111 --> 00:00:48,100
on all of the different worker machines

17
00:00:48,100 --> 00:00:50,900
gets returned and has to fit in the memory of the driver

18
00:00:50,900 --> 00:00:51,920
program.

19
00:00:51,920 --> 00:00:54,420
If it doesn't fit in the memory of the driver program,

20
00:00:54,420 --> 00:00:56,090
you'll get an out of memory error

21
00:00:56,090 --> 00:00:58,240
and your program will crash.

22
00:00:58,240 --> 00:01:00,950
So I don't recommend that you use collect.

23
00:01:00,950 --> 00:01:05,640
I recommend instead that you use take or show.

24
00:01:05,640 --> 00:01:09,560
You should never use collect in production applications,

25
00:01:09,560 --> 00:01:11,660
but sometimes it's useful for debugging.

26
00:01:11,660 --> 00:01:15,540
Again, just be very careful when using it.

27
00:01:15,540 --> 00:01:18,220
Another useful action is count.

28
00:01:18,220 --> 00:01:21,550
It returns the number of rows of a data frame.

29
00:01:21,550 --> 00:01:25,500
Now we just saw a count as a transformation,

30
00:01:25,500 --> 00:01:27,980
that's when you're dealing with group data.

31
00:01:27,980 --> 00:01:30,470
When you're dealing with group data,

32
00:01:30,470 --> 00:01:32,420
count is a transformation, but when

33
00:01:32,420 --> 00:01:37,990
you're dealing with data frames, count is an action.

34
00:01:37,990 --> 00:01:40,590
Finally, there's describe.

35
00:01:40,590 --> 00:01:44,460
This action is useful for exploratory data analysis.

36
00:01:44,460 --> 00:01:49,350
It will compute statistics across the specified columns

37
00:01:49,350 --> 00:01:51,140
that are numerical.

38
00:01:51,140 --> 00:01:52,870
And the statistics include things

39
00:01:52,870 --> 00:01:56,980
like the count, the mean, standard deviation, minimum,

40
00:01:56,980 --> 00:01:58,780
and maximum.

41
00:01:58,780 --> 00:02:03,105
So let's see these actions in use.

42
00:02:03,105 --> 00:02:03,980
So here's an example.

43
00:02:03,980 --> 00:02:05,396
We create a data frame, just like,

44
00:02:05,396 --> 00:02:08,410
before has two rows and two columns.

45
00:02:08,410 --> 00:02:13,330
First row is Alice, age one, second row is Bob, age two.

46
00:02:13,330 --> 00:02:17,910
Now if we use show instead of collect,

47
00:02:17,910 --> 00:02:21,080
we'll see a nice, pretty, printed version

48
00:02:21,080 --> 00:02:24,070
of that data frame.

49
00:02:24,070 --> 00:02:26,880
If we use count as the action, we'll

50
00:02:26,880 --> 00:02:28,500
just see the number two, because there

51
00:02:28,500 --> 00:02:32,410
are two rows in our data frame.

52
00:02:32,410 --> 00:02:35,310
If we use take and give it the argument one,

53
00:02:35,310 --> 00:02:38,900
we'll just see the first row of our data frame.

54
00:02:38,900 --> 00:02:42,870
And if we use describe, well, describe only works

55
00:02:42,870 --> 00:02:45,230
on numerical columns, so it's not to say anything

56
00:02:45,230 --> 00:02:46,090
for the name column.

57
00:02:46,090 --> 00:02:47,465
But for the age column it's going

58
00:02:47,465 --> 00:02:50,180
to tell us there are two entries, two rows,

59
00:02:50,180 --> 00:02:53,290
the mean value is 1.5, the standard deviation

60
00:02:53,290 --> 00:02:56,560
is a long floating point number, the minimum is one,

61
00:02:56,560 --> 00:03:00,290
and the maximum is two.

62
00:03:00,290 --> 00:03:03,440
So the Spark programming model is, again,

63
00:03:03,440 --> 00:03:06,250
we can take and perform a transformation,

64
00:03:06,250 --> 00:03:09,590
sqlContext.read.text, that'll read our file,

65
00:03:09,590 --> 00:03:13,210
but nothing actually happens until we perform an action.

66
00:03:13,210 --> 00:03:15,160
When we perform an action like asking

67
00:03:15,160 --> 00:03:19,440
to print lines data frame.count, that's

68
00:03:19,440 --> 00:03:22,280
going to cause Spark to actually read the data.

69
00:03:22,280 --> 00:03:25,570
Then it's going to perform a sum operation at each

70
00:03:25,570 --> 00:03:27,890
of the partitions on each of the workers,

71
00:03:27,890 --> 00:03:31,350
and then it will combine all of those sums in the driver

72
00:03:31,350 --> 00:03:35,070
so we can print out the value.

73
00:03:35,070 --> 00:03:38,840
Now here, we've added another transformation, that filter

74
00:03:38,840 --> 00:03:41,070
transformation, which is going to filter out only

75
00:03:41,070 --> 00:03:43,290
those lines that are comments.

76
00:03:43,290 --> 00:03:46,100
And we want to print out both the count

77
00:03:46,100 --> 00:03:51,720
of original rows in linesDF, and the number of rows

78
00:03:51,720 --> 00:03:53,100
in commentsDF.

79
00:03:53,100 --> 00:03:56,420
Now, when it prints out the number of rows in commentsDF,

80
00:03:56,420 --> 00:03:59,010
it's going to do that when we do the count action

81
00:03:59,010 --> 00:04:02,510
Spark is actually going to recompute the lines data

82
00:04:02,510 --> 00:04:05,580
frame again, which means it's going to read the data all over

83
00:04:05,580 --> 00:04:07,760
again from disk, it's going to sum

84
00:04:07,760 --> 00:04:10,730
in all the partitions for the comments data frame,

85
00:04:10,730 --> 00:04:14,010
and then it's going to combine those sums in the driver.

86
00:04:14,010 --> 00:04:17,720
If we want to avoid this extra re-computation step,

87
00:04:17,720 --> 00:04:22,620
we can instead tell Spark to cache the lines data frame.

88
00:04:22,620 --> 00:04:24,460
This tells it, store that in memory.

89
00:04:24,460 --> 00:04:26,750
I'm going to be using this again.

90
00:04:26,750 --> 00:04:30,690
Now when we perform count for the comments data frame,

91
00:04:30,690 --> 00:04:34,540
we're going to read from memory instead of reading from disk,

92
00:04:34,540 --> 00:04:40,290
and reuse the lines data frame, which is still in memory.

93
00:04:40,290 --> 00:04:42,730
So the life cycle of a Spark program

94
00:04:42,730 --> 00:04:47,370
is that you create a data frame from external data like a disk,

95
00:04:47,370 --> 00:04:50,980
or you create it from a collection of data

96
00:04:50,980 --> 00:04:53,530
in the driver program.

97
00:04:53,530 --> 00:04:56,830
Next, lazily transform those data frames

98
00:04:56,830 --> 00:05:00,690
into new data frames, so you specify the recipe.

99
00:05:00,690 --> 00:05:02,440
You might cache some of those data frames,

100
00:05:02,440 --> 00:05:04,310
because you're going to reuse them.

101
00:05:04,310 --> 00:05:06,820
And then you perform actions, which

102
00:05:06,820 --> 00:05:10,250
are what actually causes those recipes to be executed,

103
00:05:10,250 --> 00:05:14,760
and they execute in parallel and produce the results.

