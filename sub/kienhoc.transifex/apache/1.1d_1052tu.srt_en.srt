0
00:00:00,000 --> 00:00:00,760
https://youtu.be/R1UENY1sG2c

1
00:00:00,760 --> 00:00:04,390
TEACHER: Let's talk about analysis.

2
00:00:04,390 --> 00:00:07,820
So traditionally, people have performed analysis using tools

3
00:00:07,820 --> 00:00:09,650
like Unix shell commands.

4
00:00:09,650 --> 00:00:15,180
So grep, awk, sed, and also Python pandas,

5
00:00:15,180 --> 00:00:17,610
and the R programming environment.

6
00:00:17,610 --> 00:00:20,840
But the key limitation of all of these tools

7
00:00:20,840 --> 00:00:24,800
is that they only run on a single machine.

8
00:00:24,800 --> 00:00:26,850
Now what if we had analysis tools that

9
00:00:26,850 --> 00:00:29,850
could run on really big data?

10
00:00:29,850 --> 00:00:33,480
We could do things like combine crowdsourced traffic

11
00:00:33,480 --> 00:00:36,810
information, along with physical terrain models,

12
00:00:36,810 --> 00:00:39,620
along with data collected from people's phones,

13
00:00:39,620 --> 00:00:41,930
and combine all of that using a data assimilation

14
00:00:41,930 --> 00:00:46,830
model to deliver real time traffic information.

15
00:00:46,830 --> 00:00:48,630
So today people are already using

16
00:00:48,630 --> 00:00:55,510
Spark in real world large, big data kinds of analysis cases.

17
00:00:55,510 --> 00:00:58,510
So here at UC Berkeley, my research group

18
00:00:58,510 --> 00:01:00,740
is working on big data genomics.

19
00:01:00,740 --> 00:01:05,420
We're building a genomics analysis pipeline called, ADAM,

20
00:01:05,420 --> 00:01:08,590
that allows you to operate on thousands of genomes

21
00:01:08,590 --> 00:01:10,740
simultaneously.

22
00:01:10,740 --> 00:01:13,550
The startup, Conviva, is using Spark

23
00:01:13,550 --> 00:01:17,566
to optimize the delivery of internet video streams.

24
00:01:17,566 --> 00:01:18,940
There are also companies that are

25
00:01:18,940 --> 00:01:21,720
using Spark for data processing for wearables

26
00:01:21,720 --> 00:01:23,970
and for the internet of things.

27
00:01:23,970 --> 00:01:28,390
Yahoo uses Spark to deliver personalized news pages

28
00:01:28,390 --> 00:01:32,650
and perform analytics on their advertising platform.

29
00:01:32,650 --> 00:01:35,140
And the bank, Capital One, is using

30
00:01:35,140 --> 00:01:38,010
Spark for product recommendations

31
00:01:38,010 --> 00:01:41,790
across thousands of categories and thousands of users.

32
00:01:41,790 --> 00:01:45,120

33
00:01:45,120 --> 00:01:49,660
The problem with big data is that data is growing faster

34
00:01:49,660 --> 00:01:52,300
than our computation speeds.

35
00:01:52,300 --> 00:01:54,590
There are many new sources of data

36
00:01:54,590 --> 00:01:57,050
and growing sources of data.

37
00:01:57,050 --> 00:02:01,920
The web, mobile, scientific, we've seen those.

38
00:02:01,920 --> 00:02:04,690
Now storage is getting cheaper, its size

39
00:02:04,690 --> 00:02:07,980
is doubling every 18 months.

40
00:02:07,980 --> 00:02:13,170
But the performance of CPU's, the speeds, is not increasing.

41
00:02:13,170 --> 00:02:16,490
And storage is increasingly becoming a bottleneck.

42
00:02:16,490 --> 00:02:20,960
The time it takes to read and write data.

43
00:02:20,960 --> 00:02:24,300
So here's some real world examples of big data.

44
00:02:24,300 --> 00:02:29,680
Facebook's daily logs are more than 60 terabytes every day.

45
00:02:29,680 --> 00:02:32,600
The 1,000 Genomes Project has generated

46
00:02:32,600 --> 00:02:36,860
over 200 terabytes of human genomes and more

47
00:02:36,860 --> 00:02:39,990
are being added every day.

48
00:02:39,990 --> 00:02:45,790
Google's web index is more than 10 petabytes of information.

49
00:02:45,790 --> 00:02:48,630
Now the good news is disk is inexpensive.

50
00:02:48,630 --> 00:02:52,790
It's only about-- a terabyte cost about $35.

51
00:02:52,790 --> 00:02:57,810
On the other hand, people are uploading a petabyte of video

52
00:02:57,810 --> 00:03:01,870
to YouTube every day.

53
00:03:01,870 --> 00:03:05,080
Now the time it takes to read one terabyte from a disk

54
00:03:05,080 --> 00:03:09,740
is about three hours at 100 megabytes per second.

55
00:03:09,740 --> 00:03:13,280
That's actually quite a long time.

56
00:03:13,280 --> 00:03:16,680
So the challenge we face is that one machine cannot process,

57
00:03:16,680 --> 00:03:19,420
or even store, all of the data.

58
00:03:19,420 --> 00:03:22,280
So our solution is to distribute the data

59
00:03:22,280 --> 00:03:25,110
over a cluster of machines.

60
00:03:25,110 --> 00:03:30,050
Lots of hard drives, and CPU's, and memory.

61
00:03:30,050 --> 00:03:32,040
We'll talk more about this in the next lecture,

62
00:03:32,040 --> 00:03:35,110
but the key thing to remember for today

63
00:03:35,110 --> 00:03:37,170
is that it's all about memory.

64
00:03:37,170 --> 00:03:39,560
When we build a large cluster of machines,

65
00:03:39,560 --> 00:03:43,050
we can have a lot of memory in those machines.

66
00:03:43,050 --> 00:03:46,620
And even more when we look at across the entire cluster

67
00:03:46,620 --> 00:03:47,250
of machines.

68
00:03:47,250 --> 00:03:50,070

69
00:03:50,070 --> 00:03:55,520
So we take our big data and we partition it up

70
00:03:55,520 --> 00:03:58,360
across all of the machines in our cluster.

71
00:03:58,360 --> 00:04:02,550
So each machine gets some number of rows

72
00:04:02,550 --> 00:04:07,670
from that large, big data that we want to store and perform

73
00:04:07,670 --> 00:04:11,060
analysis on.

74
00:04:11,060 --> 00:04:13,800
This concept of partitioning out the big data

75
00:04:13,800 --> 00:04:17,120
across multiple machines and storing it in those machine's

76
00:04:17,120 --> 00:04:20,720
memory is Sparks DataFrame.

77
00:04:20,720 --> 00:04:22,835
This is the key concept in Spark.

78
00:04:22,835 --> 00:04:25,420

79
00:04:25,420 --> 00:04:28,000
Now Spark is a computing framework.

80
00:04:28,000 --> 00:04:31,020
And it provides a programming abstraction and parallel

81
00:04:31,020 --> 00:04:33,910
runtime that hides the complexities of fault

82
00:04:33,910 --> 00:04:36,940
tolerance and slow machines.

83
00:04:36,940 --> 00:04:38,780
You simply say, here's an operation,

84
00:04:38,780 --> 00:04:40,890
run it on all of the data.

85
00:04:40,890 --> 00:04:43,756
You don't have to care about where it runs,

86
00:04:43,756 --> 00:04:45,130
the operating system, the system,

87
00:04:45,130 --> 00:04:48,600
takes care of scheduling that.

88
00:04:48,600 --> 00:04:52,250
In fact, Spark may even run your computation multiple times

89
00:04:52,250 --> 00:04:54,610
on different nodes, because a node fails

90
00:04:54,610 --> 00:04:57,010
or it's running slowly.

91
00:04:57,010 --> 00:04:59,950
We'll learn more about this in the second lecture,

92
00:04:59,950 --> 00:05:02,670
but the key thing to remember is that Spark

93
00:05:02,670 --> 00:05:05,220
hides all of this complexity.

94
00:05:05,220 --> 00:05:08,360
You just say what you want to do and Spark does it

95
00:05:08,360 --> 00:05:12,020
in the most optimal way.

96
00:05:12,020 --> 00:05:14,960
So Apache Spark consists of several components.

97
00:05:14,960 --> 00:05:18,730
There's the core Apache Spark Runtime.

98
00:05:18,730 --> 00:05:25,810
And then there's Spark SQL, Spark Streaming, MLlib, ML,

99
00:05:25,810 --> 00:05:26,390
and GraphX.

100
00:05:26,390 --> 00:05:29,210

101
00:05:29,210 --> 00:05:34,730
We're going to focus on Spark SQL and Apache Spark

102
00:05:34,730 --> 00:05:35,610
in this course.

103
00:05:35,610 --> 00:05:38,730

104
00:05:38,730 --> 00:05:43,370
Here are some important references for Apache Spark.

105
00:05:43,370 --> 00:05:46,690
When I'm programming and writing my Apache Spark applications,

106
00:05:46,690 --> 00:05:50,420
I almost always have a web browser open with one or more

107
00:05:50,420 --> 00:05:51,265
of these links.

108
00:05:51,265 --> 00:05:53,910

109
00:05:53,910 --> 00:05:55,410
Now the environment that we're going

110
00:05:55,410 --> 00:05:59,030
to be using for this course is the Python programming

111
00:05:59,030 --> 00:06:01,980
interface to Spark called, pySpark.

112
00:06:01,980 --> 00:06:03,850
It provides an easy to use programming

113
00:06:03,850 --> 00:06:06,040
abstraction and parallel runtime.

114
00:06:06,040 --> 00:06:08,280
Again, you just say, here's an operation,

115
00:06:08,280 --> 00:06:10,410
run it on all of the data.

116
00:06:10,410 --> 00:06:12,540
The key concept that we're going to use

117
00:06:12,540 --> 00:06:16,980
is spark's notion of data frames.

118
00:06:16,980 --> 00:06:20,180
Now, when you run a Spark program,

119
00:06:20,180 --> 00:06:24,000
it actually consists of two programs.

120
00:06:24,000 --> 00:06:28,710
There's a driver program and there's a workers program.

121
00:06:28,710 --> 00:06:33,550
The driver program runs on one machine.

122
00:06:33,550 --> 00:06:37,700
And the worker program runs either on cluster nodes

123
00:06:37,700 --> 00:06:40,154
or in local threads on the same machine.

124
00:06:40,154 --> 00:06:42,880

125
00:06:42,880 --> 00:06:45,820
The data frames that you create are distributed

126
00:06:45,820 --> 00:06:47,600
across all of the workers.

127
00:06:47,600 --> 00:06:51,180

128
00:06:51,180 --> 00:06:55,710
Now a Spark program first creates a SparkContext object.

129
00:06:55,710 --> 00:06:58,200
And the SparkContext tells Spark how and where

130
00:06:58,200 --> 00:07:00,840
to access a cluster.

131
00:07:00,840 --> 00:07:04,180
Now if you use the pySpark shell or you use the Databricks

132
00:07:04,180 --> 00:07:07,460
Community Edition environment, they both automatically create

133
00:07:07,460 --> 00:07:09,770
the SparkContext for you.

134
00:07:09,770 --> 00:07:14,390
If you're using iPython or you're writing a Spark program,

135
00:07:14,390 --> 00:07:17,920
separate from the shell or the Databricks Community Edition

136
00:07:17,920 --> 00:07:19,810
environment, then you have to create

137
00:07:19,810 --> 00:07:23,140
a new SparkContext yourself.

138
00:07:23,140 --> 00:07:27,020
Next the program creates a SQL context object.

139
00:07:27,020 --> 00:07:31,330
You use the SQL context object to create data frames.

140
00:07:31,330 --> 00:07:33,820
In the labs, we're going to create the SparkContext

141
00:07:33,820 --> 00:07:36,210
and the SQL context for you.

142
00:07:36,210 --> 00:07:39,200
But if you're writing your own programs separately

143
00:07:39,200 --> 00:07:42,350
you may need to create the SparkContext or SQL context,

144
00:07:42,350 --> 00:07:46,670
depending on the environment that you're running it.

145
00:07:46,670 --> 00:07:49,710
Now the master parameter for SparkContext context

146
00:07:49,710 --> 00:07:53,170
determines what type and size of cluster you use.

147
00:07:53,170 --> 00:07:54,980
And there are four different choices.

148
00:07:54,980 --> 00:07:58,480
You can run Spark in local mode, or you

149
00:07:58,480 --> 00:08:04,820
can run Spark in a cluster mode, or with mesos.

150
00:08:04,820 --> 00:08:07,070
For the labs, you don't have to worry about the master

151
00:08:07,070 --> 00:08:10,110
parameter, we'll set that for you.

