0
00:00:00,000 --> 00:00:00,830
https://youtu.be/Rdg19pS_-mQ

1
00:00:00,830 --> 00:00:02,490
New spark programmers are sometimes

2
00:00:02,490 --> 00:00:05,820
confused about where the code in their application runs.

3
00:00:05,820 --> 00:00:08,109
Does it run locally in the driver,

4
00:00:08,109 --> 00:00:11,290
or distributed at the executors, or at both the driver

5
00:00:11,290 --> 00:00:12,630
and the executors?

6
00:00:12,630 --> 00:00:14,290
This is a very important question

7
00:00:14,290 --> 00:00:16,760
because executors run in parallel,

8
00:00:16,760 --> 00:00:18,450
and executors have much more memory

9
00:00:18,450 --> 00:00:21,760
available than the driver does.

10
00:00:21,760 --> 00:00:24,616
Now most Python code runs in the driver,

11
00:00:24,616 --> 00:00:25,990
with the exception of code that's

12
00:00:25,990 --> 00:00:28,220
passed to transformations.

13
00:00:28,220 --> 00:00:30,810
Transformations run at executors.

14
00:00:30,810 --> 00:00:35,660
And actions run at executors and the driver.

15
00:00:35,660 --> 00:00:37,630
Let's look at some examples.

16
00:00:37,630 --> 00:00:40,710
If you have the statement a equals a plus 1,

17
00:00:40,710 --> 00:00:44,280
This runs in the driver program.

18
00:00:44,280 --> 00:00:48,820
If you have the statement linesDF.filter isComment,

19
00:00:48,820 --> 00:00:52,260
this runs at the spark executors, both the filter

20
00:00:52,260 --> 00:00:55,320
transformation and the Python function

21
00:00:55,320 --> 00:00:59,960
that's being passed into this transformation isComment.

22
00:00:59,960 --> 00:01:03,650
If you have the statement commentsDF.count,

23
00:01:03,650 --> 00:01:05,880
this statement runs is an action and it

24
00:01:05,880 --> 00:01:09,005
runs at both the executor and at the driver.

25
00:01:09,005 --> 00:01:11,650

26
00:01:11,650 --> 00:01:15,630
Now let's look at an example of how you should not write code.

27
00:01:15,630 --> 00:01:20,420
Let's say you want to combine two data frames, aDF and bDF.

28
00:01:20,420 --> 00:01:24,200
Now you remember that df.collect returns a list of Row.

29
00:01:24,200 --> 00:01:28,800
And in Python, you can combine two lists with plus.

30
00:01:28,800 --> 00:01:32,250
So a naive implementation would be

31
00:01:32,250 --> 00:01:38,450
to call a equals aDF.collect, b equals bDF.collect,

32
00:01:38,450 --> 00:01:42,080
and cDF equals sqlContext.createDataFrame

33
00:01:42,080 --> 00:01:44,010
a plus b

34
00:01:44,010 --> 00:01:47,570
Now where does this code actually run?

35
00:01:47,570 --> 00:01:51,530
Well the first two statements a equals aDF.collect and b

36
00:01:51,530 --> 00:01:55,170
equals bDF.collect are both actions

37
00:01:55,170 --> 00:01:58,017
that are going to cause all of the distributed data for a

38
00:01:58,017 --> 00:02:01,260
and b to be sent to the driver.

39
00:02:01,260 --> 00:02:04,350
Now what will happen if a is large, or b is large,

40
00:02:04,350 --> 00:02:06,250
or they're both very large?

41
00:02:06,250 --> 00:02:08,400
Well the driver could run out of memory.

42
00:02:08,400 --> 00:02:12,570
This is called an out of memory error, or OOM error.

43
00:02:12,570 --> 00:02:15,460
Also it could take a long time to send the data to the driver.

44
00:02:15,460 --> 00:02:18,030

45
00:02:18,030 --> 00:02:22,100
The last statement combines the two lists of Row

46
00:02:22,100 --> 00:02:26,130
using the Python plus operator, and then creates a new data

47
00:02:26,130 --> 00:02:28,300
frame.

48
00:02:28,300 --> 00:02:31,060
This is going to cause all of the data for a plus

49
00:02:31,060 --> 00:02:35,050
b to be sent from the driver to the executors.

50
00:02:35,050 --> 00:02:37,630
Now what happens if this combined list, a plus b,

51
00:02:37,630 --> 00:02:38,720
is very large?

52
00:02:38,720 --> 00:02:41,330
Well again the driver could run out of memory.

53
00:02:41,330 --> 00:02:44,418
And also it will take a long time to send all of the data

54
00:02:44,418 --> 00:02:45,126
to the executors.

55
00:02:45,126 --> 00:02:47,840

56
00:02:47,840 --> 00:02:52,510
The best implementation is to look at the data frame

57
00:02:52,510 --> 00:02:58,640
reference API and notice that there is a unionAll function.

58
00:02:58,640 --> 00:03:00,900
This function returns a new data frame

59
00:03:00,900 --> 00:03:03,820
containing the union of rows in this frame

60
00:03:03,820 --> 00:03:06,070
and in another frame.

61
00:03:06,070 --> 00:03:08,560
It runs completely at the executors.

62
00:03:08,560 --> 00:03:10,831
This makes it very scalable and efficient.

63
00:03:10,831 --> 00:03:13,600

64
00:03:13,600 --> 00:03:15,900
So some programming best practices.

65
00:03:15,900 --> 00:03:18,740
You should always use Spark Transformations and Actions

66
00:03:18,740 --> 00:03:20,780
wherever possible.

67
00:03:20,780 --> 00:03:22,617
Look through the data frame reference API,

68
00:03:22,617 --> 00:03:24,200
and you'll find that there are over 80

69
00:03:24,200 --> 00:03:27,340
transformations and actions.

70
00:03:27,340 --> 00:03:30,000
You should never use collect in production,

71
00:03:30,000 --> 00:03:33,340
and should instead use take with a parameter.

72
00:03:33,340 --> 00:03:35,910
This is because collect will return all of the data

73
00:03:35,910 --> 00:03:39,830
back to the driver, and could cause an out of memory error.

74
00:03:39,830 --> 00:03:42,514
Finally, if you have data frames that you use often,

75
00:03:42,514 --> 00:03:44,055
you should cache those data frames so

76
00:03:44,055 --> 00:03:47,910
that they won't be recomputed each time that you use them.

