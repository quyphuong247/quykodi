0
00:00:00,000 --> 00:00:01,226
https://youtu.be/FnHF-d95eWE

1
00:00:01,226 --> 00:00:02,600
SPEAKER: So how does Apache Spark

2
00:00:02,600 --> 00:00:05,550
achieve efficient operation?

3
00:00:05,550 --> 00:00:07,490
To answer that question, we first

4
00:00:07,490 --> 00:00:11,830
need to look at the organization of a data center.

5
00:00:11,830 --> 00:00:14,030
Here you can see an individual node,

6
00:00:14,030 --> 00:00:16,480
where we have multiple CPUs that are

7
00:00:16,480 --> 00:00:20,470
connected to hard drives and solid state drives

8
00:00:20,470 --> 00:00:22,250
and to memory.

9
00:00:22,250 --> 00:00:25,360
Those nodes are placed in a rack and interconnected

10
00:00:25,360 --> 00:00:27,330
with a top of rack switch.

11
00:00:27,330 --> 00:00:28,860
And those top of rack switches are

12
00:00:28,860 --> 00:00:33,760
interconnected to other top of rack switches in other racks.

13
00:00:33,760 --> 00:00:35,460
Now a key thing to notice here is

14
00:00:35,460 --> 00:00:38,430
that the bandwidth that we have between the CPUs

15
00:00:38,430 --> 00:00:44,560
and the devices within a node is much greater than the bandwidth

16
00:00:44,560 --> 00:00:47,530
connecting other nodes in the same rack and much,

17
00:00:47,530 --> 00:00:52,910
much greater than the bandwidth connecting other racks.

18
00:00:52,910 --> 00:00:55,020
Now another thing that we have to understand

19
00:00:55,020 --> 00:00:59,220
in terms of efficiency is that when we're using MapReduce,

20
00:00:59,220 --> 00:01:02,880
every stage passes through the hard drives.

21
00:01:02,880 --> 00:01:06,380
So the map initially reads from a hard drive,

22
00:01:06,380 --> 00:01:08,760
writes output to a hard drive.

23
00:01:08,760 --> 00:01:11,940
The Reduce stage then reads input from the hard drive

24
00:01:11,940 --> 00:01:16,220
and writes output to a hard drive.

25
00:01:16,220 --> 00:01:18,680
Now if we have an iterative job, then that's

26
00:01:18,680 --> 00:01:22,170
going to involve a lot of disk I/O for each repetition.

27
00:01:22,170 --> 00:01:25,030
So here we have multiple stages, we have three stages.

28
00:01:25,030 --> 00:01:29,040
And between each stage, we're writing to the disk.

29
00:01:29,040 --> 00:01:33,640
And within each stage, the map stage is reading from disk

30
00:01:33,640 --> 00:01:36,280
and writing to disk, and the reduced stage

31
00:01:36,280 --> 00:01:39,040
is reading from disk and writing to disk.

32
00:01:39,040 --> 00:01:41,570
And remember, disk I/O is very slow.

33
00:01:41,570 --> 00:01:44,220

34
00:01:44,220 --> 00:01:47,060
So the motivation for Apache Spark

35
00:01:47,060 --> 00:01:49,810
is that we use MapReduce for many complex jobs

36
00:01:49,810 --> 00:01:52,720
and interactive queries and for online processing.

37
00:01:52,720 --> 00:01:56,020
And all of this involves lots of disk I/O.

38
00:01:56,020 --> 00:01:58,510
In interactive mining, we're reading

39
00:01:58,510 --> 00:02:01,050
from the disk for each query.

40
00:02:01,050 --> 00:02:03,000
And for string processing, we're reading

41
00:02:03,000 --> 00:02:06,710
from the disk for each job.

42
00:02:06,710 --> 00:02:08,570
Also for iterative jobs, we're reading

43
00:02:08,570 --> 00:02:13,160
from the disk between each stage of that iterative job.

44
00:02:13,160 --> 00:02:17,710
And again, disk I/O is very slow.

45
00:02:17,710 --> 00:02:20,500
Now a technology trend is that the cost

46
00:02:20,500 --> 00:02:25,630
of semiconductors and devices is reducing exponentially.

47
00:02:25,630 --> 00:02:30,420
Here we have a graph that shows on the x-axis, year.

48
00:02:30,420 --> 00:02:35,440
And on the y-axis, it shows price on a logarithmic scale.

49
00:02:35,440 --> 00:02:39,170
And we can see that the cost of disk and flash and memory

50
00:02:39,170 --> 00:02:42,520
are all dropping exponentially.

51
00:02:42,520 --> 00:02:45,010
In 2010, it was about $0.01 per megabyte.

52
00:02:45,010 --> 00:02:48,960
And today it's much less than a penny per megabyte.

53
00:02:48,960 --> 00:02:51,380
Now lower cost for memory means that we

54
00:02:51,380 --> 00:02:56,440
can put much more memory in each one of our nodes.

55
00:02:56,440 --> 00:02:58,880
So now our modern hardware for big data

56
00:02:58,880 --> 00:03:03,240
is that we have lots of hard drives, lots of CPUs,

57
00:03:03,240 --> 00:03:06,320
and lots of memory.

58
00:03:06,320 --> 00:03:08,330
And so this led to the opportunity

59
00:03:08,330 --> 00:03:11,510
to keep more data in memory.

60
00:03:11,510 --> 00:03:15,140
But to do so, we had to create a new distributed execution

61
00:03:15,140 --> 00:03:19,220
engine, Apache Spark.

62
00:03:19,220 --> 00:03:24,540
So remember, with MapReduce, with each iteration,

63
00:03:24,540 --> 00:03:28,250
we're reading from the disk and writing from the disk,

64
00:03:28,250 --> 00:03:32,350
only to re-read from the disk for the second iteration.

65
00:03:32,350 --> 00:03:35,240
And the same thing is true when we're doing exploratory data

66
00:03:35,240 --> 00:03:38,110
analysis, and we're performing multiple queries

67
00:03:38,110 --> 00:03:40,700
against the same input data set.

68
00:03:40,700 --> 00:03:42,560
Each one of those queries requires

69
00:03:42,560 --> 00:03:46,650
us to read from the hard drive.

70
00:03:46,650 --> 00:03:51,680
With Spark, we replace the hard drives with memory.

71
00:03:51,680 --> 00:03:54,060
So there's still an initial step that we have to do,

72
00:03:54,060 --> 00:03:58,530
to read the data in from the hard drive for iteration one

73
00:03:58,530 --> 00:04:01,190
or for the first query that we do.

74
00:04:01,190 --> 00:04:05,370
But then all subsequent iterations and queries

75
00:04:05,370 --> 00:04:07,640
rely on data that we share in memory.

76
00:04:07,640 --> 00:04:10,270

77
00:04:10,270 --> 00:04:13,710
This can yield a performance improvement

78
00:04:13,710 --> 00:04:18,670
that is 10 to 100 times faster than using the network

79
00:04:18,670 --> 00:04:22,290
or using the disk.

80
00:04:22,290 --> 00:04:25,350
There are other differences between Apache Spark an

81
00:04:25,350 --> 00:04:28,810
Apache Hadoop or MapReduce.

82
00:04:28,810 --> 00:04:33,060
So Hadoop and MapReduce only use disk for storage.

83
00:04:33,060 --> 00:04:37,830
Spark uses in memory or disk.

84
00:04:37,830 --> 00:04:42,290
MapReduce only provides the operations Map and Reduce,

85
00:04:42,290 --> 00:04:45,600
whereas Spark provides many transformations and actions.

86
00:04:45,600 --> 00:04:49,820
We saw some of those in the last lecture.

87
00:04:49,820 --> 00:04:54,360
It also provides support for Map and Reduce operations.

88
00:04:54,360 --> 00:04:57,830
The only option for execution is a batch model

89
00:04:57,830 --> 00:04:59,560
when using MapReduce.

90
00:04:59,560 --> 00:05:02,080
Spark supports Batch, Interactive,

91
00:05:02,080 --> 00:05:05,430
and Streaming Execution modes.

92
00:05:05,430 --> 00:05:08,820
MapReduce only supports the Java programming language,

93
00:05:08,820 --> 00:05:13,265
while Spark supports Scala, Java, R, and Python languages.

94
00:05:13,265 --> 00:05:16,010

95
00:05:16,010 --> 00:05:19,600
There are other differences between Spark and MapReduce.

96
00:05:19,600 --> 00:05:23,670
Spark provides a generalized pattern for computation.

97
00:05:23,670 --> 00:05:26,500
It provides a unified engine for many different kinds

98
00:05:26,500 --> 00:05:27,790
of use cases.

99
00:05:27,790 --> 00:05:30,600
And the result is that when you're writing applications,

100
00:05:30,600 --> 00:05:33,900
you'll typically require two to five times less code

101
00:05:33,900 --> 00:05:36,560
when you're using Spark.

102
00:05:36,560 --> 00:05:38,630
As we saw in the last lecture, Spark

103
00:05:38,630 --> 00:05:41,540
performs lazy evaluation of the lineage graph.

104
00:05:41,540 --> 00:05:46,410
We don't execute transformations until an action occurs.

105
00:05:46,410 --> 00:05:48,716
This means that Spark can optimize.

106
00:05:48,716 --> 00:05:51,100
It can also reduce the wait states,

107
00:05:51,100 --> 00:05:53,500
and it can pipeline much better.

108
00:05:53,500 --> 00:05:57,000
Spark also has lower overhead for starting jobs and much less

109
00:05:57,000 --> 00:06:00,070
expensive shuffles.

110
00:06:00,070 --> 00:06:04,480
So operating in memory can make a huge difference.

111
00:06:04,480 --> 00:06:08,500
So here's two experiments from 2013,

112
00:06:08,500 --> 00:06:10,250
using two different iterative machine

113
00:06:10,250 --> 00:06:13,260
learning algorithms, K-means Clustering and Logistic

114
00:06:13,260 --> 00:06:14,990
Regression.

115
00:06:14,990 --> 00:06:18,280
On the x-axis, we have time.

116
00:06:18,280 --> 00:06:20,820
And the red bar is Hadoop MapReduce,

117
00:06:20,820 --> 00:06:23,887
while the blue bar is Spark.

118
00:06:23,887 --> 00:06:25,720
And you can see that there's nearly an order

119
00:06:25,720 --> 00:06:28,230
of magnitude reduction in the running time

120
00:06:28,230 --> 00:06:29,340
one we're using Spark.

121
00:06:29,340 --> 00:06:31,890

122
00:06:31,890 --> 00:06:36,570
In 2014, Spark tied for first place

123
00:06:36,570 --> 00:06:40,770
for sorting 100 terabytes of data.

124
00:06:40,770 --> 00:06:43,460
It did it in only 23 minutes.

125
00:06:43,460 --> 00:06:46,590
The previous record, using Hadoop,

126
00:06:46,590 --> 00:06:51,460
was 72 minutes and on a much larger number of nodes,

127
00:06:51,460 --> 00:06:55,570
nearly 10 times as many nodes.

128
00:06:55,570 --> 00:07:01,060
Spark also was the first execution environment

129
00:07:01,060 --> 00:07:04,570
that was able to sort a petabyte-- 1,000

130
00:07:04,570 --> 00:07:08,855
terabytes-- of data using a public cloud, using Amazon EC2.

131
00:07:08,855 --> 00:07:12,490

132
00:07:12,490 --> 00:07:15,750
Spark has also added two key performance optimizations.

133
00:07:15,750 --> 00:07:19,200
And this is an addition to using memory instead of disk.

134
00:07:19,200 --> 00:07:22,510
The first is the Catalyst Optimization engine.

135
00:07:22,510 --> 00:07:26,350
It can yield up to a 75% reduction in execution

136
00:07:26,350 --> 00:07:28,910
time for your programs.

137
00:07:28,910 --> 00:07:33,160
Project Tungsten uses off-heap memory management

138
00:07:33,160 --> 00:07:36,480
to provide 75% or greater reduction

139
00:07:36,480 --> 00:07:40,920
in memory usage and also much less garbage collection.

140
00:07:40,920 --> 00:07:44,150
And garbage collection is pauses while the system cleans up

141
00:07:44,150 --> 00:07:47,950
memory that you're not using anymore.

142
00:07:47,950 --> 00:07:53,500
Catalyst is the optimization and environment.

143
00:07:53,500 --> 00:07:59,020
Catalyst shares the same optimization execution pipeline

144
00:07:59,020 --> 00:08:05,250
across all of the languages-- Java, R, Python, and Scala--

145
00:08:05,250 --> 00:08:07,830
and also across the different primitives

146
00:08:07,830 --> 00:08:13,940
that we have for data frames, data sets, and Spark SQL.

147
00:08:13,940 --> 00:08:18,430
Now underneath the covers, Spark uses the Java Virtual Machine

148
00:08:18,430 --> 00:08:20,740
execution environment.

149
00:08:20,740 --> 00:08:23,830
And Java treats everything as an object.

150
00:08:23,830 --> 00:08:28,470
So if you have the string, abcd, if we represent that natively,

151
00:08:28,470 --> 00:08:32,510
that would be 4 bytes, one for each character.

152
00:08:32,510 --> 00:08:36,250
However, in Java everything is an object, as I said.

153
00:08:36,250 --> 00:08:40,950
And so as a result, it ends up taking 48 bytes

154
00:08:40,950 --> 00:08:44,220
to store the same object.

155
00:08:44,220 --> 00:08:49,350
With Project Tungsten, we're able to store each object much

156
00:08:49,350 --> 00:08:51,940
more compactly, with the result being

157
00:08:51,940 --> 00:08:55,840
that it requires much less memory-- 75%

158
00:08:55,840 --> 00:09:01,100
or greater reduction in memory-- and fewer garbage collection

159
00:09:01,100 --> 00:09:02,242
pauses.

160
00:09:02,242 --> 00:09:02,742

