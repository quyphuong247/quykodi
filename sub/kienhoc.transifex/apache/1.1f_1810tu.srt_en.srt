0
00:00:00,000 --> 00:00:00,610
https://youtu.be/jleReJ4ABs0

1
00:00:00,610 --> 00:00:03,270
SPEAKER 1: Let's talk about Spark transformations.

2
00:00:03,270 --> 00:00:04,920
Transformations are a way of creating

3
00:00:04,920 --> 00:00:08,990
a new DataFrame from an existing one, or from a data source.

4
00:00:08,990 --> 00:00:12,340
Spark uses lazy evaluation with transformations.

5
00:00:12,340 --> 00:00:14,810
That means that the results are not computed right away.

6
00:00:14,810 --> 00:00:17,530
Instead, Spark remembers the set of transformations

7
00:00:17,530 --> 00:00:20,060
that you requested, and then will apply those

8
00:00:20,060 --> 00:00:22,120
to the base DataFrame.

9
00:00:22,120 --> 00:00:24,680
Spark uses the Catalyst Optimizer

10
00:00:24,680 --> 00:00:27,720
to optimize a required set of calculations,

11
00:00:27,720 --> 00:00:30,820
and Spark can automatically recover from any failures

12
00:00:30,820 --> 00:00:32,900
or slow workers in your cluster.

13
00:00:32,900 --> 00:00:34,830
So, the way to think about transformations

14
00:00:34,830 --> 00:00:39,370
is that they are a recipe for creating a result.

15
00:00:39,370 --> 00:00:42,490
So let's first talk about column transformations.

16
00:00:42,490 --> 00:00:47,000
The apply method creates a new DataFrame from one column.

17
00:00:47,000 --> 00:00:50,200
So, here we have the DataFrame people with a set of columns,

18
00:00:50,200 --> 00:00:54,800
like names and ages, and we select the age column.

19
00:00:54,800 --> 00:00:57,050
We create a new age column DataFrame

20
00:00:57,050 --> 00:01:00,160
that only has a single column.

21
00:01:00,160 --> 00:01:02,840
Now if we want to select one or more columns from a DataFrame,

22
00:01:02,840 --> 00:01:05,690
we can use the select transformation.

23
00:01:05,690 --> 00:01:07,980
If we use it with star, or asterisk,

24
00:01:07,980 --> 00:01:09,550
that will select all of the columns

25
00:01:09,550 --> 00:01:11,470
and create a DataFrame that has all

26
00:01:11,470 --> 00:01:14,520
of the same columns as the original DataFrame.

27
00:01:14,520 --> 00:01:17,090
We can also select specific columns--

28
00:01:17,090 --> 00:01:20,760
like for example, the name column, or the age column.

29
00:01:20,760 --> 00:01:23,070
Here we've placed them in single quotes.

30
00:01:23,070 --> 00:01:26,240
We can also use dotted notation--

31
00:01:26,240 --> 00:01:31,280
so df.name or df.age-- to reference a specific column.

32
00:01:31,280 --> 00:01:33,900
We can also create new columns based

33
00:01:33,900 --> 00:01:37,130
on the contents of an old column with some function applied

34
00:01:37,130 --> 00:01:37,810
to it.

35
00:01:37,810 --> 00:01:39,710
So in this case, we take the age column

36
00:01:39,710 --> 00:01:43,060
and we increment all of the ages by 10.

37
00:01:43,060 --> 00:01:45,620
And we give it a nice name, like age,

38
00:01:45,620 --> 00:01:47,555
by using the alias transformation.

39
00:01:47,555 --> 00:01:50,410

40
00:01:50,410 --> 00:01:52,162
Now if we have too many columns and we

41
00:01:52,162 --> 00:01:53,870
want to get rid of some of those columns,

42
00:01:53,870 --> 00:01:55,670
we can use the drop method, which

43
00:01:55,670 --> 00:02:00,200
will return a new DataFrame that drops the specified columns.

44
00:02:00,200 --> 00:02:02,050
So in this case, we drop the age column,

45
00:02:02,050 --> 00:02:05,910
and now we just have a DataFrame consisting of a single name

46
00:02:05,910 --> 00:02:08,259
column.

47
00:02:08,259 --> 00:02:10,479
Now, a quick review.

48
00:02:10,479 --> 00:02:12,830
Python provides lambda functions.

49
00:02:12,830 --> 00:02:14,660
These are small, anonymous functions

50
00:02:14,660 --> 00:02:16,370
that are not bound to a name.

51
00:02:16,370 --> 00:02:19,220
So here's an example-- we have a function that

52
00:02:19,220 --> 00:02:22,260
takes two arguments, A and B, and returns

53
00:02:22,260 --> 00:02:24,262
the sum of those arguments.

54
00:02:24,262 --> 00:02:25,720
Now, notice that there is no name--

55
00:02:25,720 --> 00:02:27,640
these are anonymous functions.

56
00:02:27,640 --> 00:02:29,710
You can use a lambda function wherever

57
00:02:29,710 --> 00:02:32,080
a function object is required.

58
00:02:32,080 --> 00:02:35,870
But, they're restricted to just a single expression.

59
00:02:35,870 --> 00:02:39,990
So let's see an example of how we could use a lambda function.

60
00:02:39,990 --> 00:02:43,120
So here, we're going to use a User Defined Function

61
00:02:43,120 --> 00:02:44,480
transformation.

62
00:02:44,480 --> 00:02:49,020
So we're going to take-- and create-- a new function that we

63
00:02:49,020 --> 00:02:51,150
can apply to a DataFrame.

64
00:02:51,150 --> 00:02:53,570
It's called a User Defined Function.

65
00:02:53,570 --> 00:02:58,260
Now, we use the UDF operator to do this.

66
00:02:58,260 --> 00:03:03,070
We pass in a lambda, and that lambda function

67
00:03:03,070 --> 00:03:07,800
takes in a string, and returns the length of that string.

68
00:03:07,800 --> 00:03:10,330
That function returns an integer.

69
00:03:10,330 --> 00:03:13,780
We need to tell Spark that this function returns an integer.

70
00:03:13,780 --> 00:03:16,632
We do that by importing the integer type

71
00:03:16,632 --> 00:03:17,590
from pyspark.sql.types.

72
00:03:17,590 --> 00:03:20,850

73
00:03:20,850 --> 00:03:22,470
So now we're ready to create our UDF,

74
00:03:22,470 --> 00:03:24,380
and we create it by passing in this lambda

75
00:03:24,380 --> 00:03:28,080
function and the integer type.

76
00:03:28,080 --> 00:03:32,920
Now we can apply that to create a new DataFrame where

77
00:03:32,920 --> 00:03:37,370
the columns consist of just one column, which

78
00:03:37,370 --> 00:03:39,900
is the length of the name column,

79
00:03:39,900 --> 00:03:41,820
in the original DataFrame.

80
00:03:41,820 --> 00:03:45,070
We do this by passing in that column

81
00:03:45,070 --> 00:03:47,110
to our User Defined Function.

82
00:03:47,110 --> 00:03:49,840
And then we rename that column as slen.

83
00:03:49,840 --> 00:03:54,320
So now we have a new DataFrame, which has two rows, one column,

84
00:03:54,320 --> 00:03:56,260
which is titled slen.

85
00:03:56,260 --> 00:03:58,730
First one is 5 because that's Alice,

86
00:03:58,730 --> 00:04:03,900
and the second one is 3 because that's the length of Bob.

87
00:04:03,900 --> 00:04:05,960
There are many other useful transformations

88
00:04:05,960 --> 00:04:07,230
that Spark provides.

89
00:04:07,230 --> 00:04:11,350
For example, it provides a filter transformation,

90
00:04:11,350 --> 00:04:13,960
which creates a new DataFrame, which

91
00:04:13,960 --> 00:04:18,839
only contains the rows for which the filter function returns

92
00:04:18,839 --> 00:04:20,740
true.

93
00:04:20,740 --> 00:04:25,370
Where is just an alias for filter, and distinct

94
00:04:25,370 --> 00:04:29,100
is a transformation that returns a new DataFrame that

95
00:04:29,100 --> 00:04:32,010
contains the distinct rows of the source DataFrame.

96
00:04:32,010 --> 00:04:34,200
So if we had multiple rows that were identical,

97
00:04:34,200 --> 00:04:37,960
you would remove those and only return one of those rows.

98
00:04:37,960 --> 00:04:40,500
And then there are two other useful transformations

99
00:04:40,500 --> 00:04:44,390
for ordering the rows of a DataFrame, order by and sort.

100
00:04:44,390 --> 00:04:47,240
These allow us to sort a DataFrame

101
00:04:47,240 --> 00:04:50,330
in a specific direction.

102
00:04:50,330 --> 00:04:53,590
Finally, the explode transformation

103
00:04:53,590 --> 00:04:58,170
returns a new row for each element in a specified column.

104
00:04:58,170 --> 00:05:00,880

105
00:05:00,880 --> 00:05:03,080
So let's see how we could use those transformations

106
00:05:03,080 --> 00:05:04,330
in practice.

107
00:05:04,330 --> 00:05:06,430
So we'll create a DataFrame, using

108
00:05:06,430 --> 00:05:09,550
sqlcontext.createDataFrame, we'll pass in data--

109
00:05:09,550 --> 00:05:11,450
and the data in this case is a list

110
00:05:11,450 --> 00:05:16,310
that contains two pools of Alice in 1, and Bob in 2.

111
00:05:16,310 --> 00:05:20,550
And we tell it that the names of our columns are name and age.

112
00:05:20,550 --> 00:05:23,840
So now that's going to create a new DataFrame, two rows, two

113
00:05:23,840 --> 00:05:25,820
columns.

114
00:05:25,820 --> 00:05:32,710
Now, we can use a UDF that is going to double the age column.

115
00:05:32,710 --> 00:05:35,520
So we create the UDF, it takes in a value,

116
00:05:35,520 --> 00:05:37,680
and returns that value doubled.

117
00:05:37,680 --> 00:05:40,400
And that's an integer type, so we tell Spark

118
00:05:40,400 --> 00:05:43,730
that the UDF has type integer.

119
00:05:43,730 --> 00:05:48,120
Now, we can use selection to create a new DataFrame where

120
00:05:48,120 --> 00:05:53,340
the columns are the name column from the original DataFrame,

121
00:05:53,340 --> 00:05:58,520
and apply the UDF doubled to the age column,

122
00:05:58,520 --> 00:06:00,590
and we name that as age.

123
00:06:00,590 --> 00:06:03,480
So now, we'll have a new DataFrame consisting

124
00:06:03,480 --> 00:06:08,620
of again two rows, two columns-- one column is the name,

125
00:06:08,620 --> 00:06:11,280
the second column is the age, doubled

126
00:06:11,280 --> 00:06:15,130
from the original DataFrame.

127
00:06:15,130 --> 00:06:19,220
Here's another example-- the filter function.

128
00:06:19,220 --> 00:06:23,330
So we pass in our Boolean function, which is going to be,

129
00:06:23,330 --> 00:06:27,370
is the age value greater than 3?

130
00:06:27,370 --> 00:06:30,490
We apply that to the DataFrame that we just created,

131
00:06:30,490 --> 00:06:33,870
and only one row meets that criteria,

132
00:06:33,870 --> 00:06:38,240
and that's Bob because age is 4.

133
00:06:38,240 --> 00:06:41,840
Another transformation example-- so here,

134
00:06:41,840 --> 00:06:47,060
we start with a list that contains Alice in 1, Bob in 2,

135
00:06:47,060 --> 00:06:48,620
and Bob in 2.

136
00:06:48,620 --> 00:06:50,480
Now we create a DataFrame, we end up

137
00:06:50,480 --> 00:06:56,740
with three rows, two of which are the same-- Bob with age 2.

138
00:06:56,740 --> 00:07:00,620
If we use distinct, we'll end up with only two rows.

139
00:07:00,620 --> 00:07:05,080
The second instance-- duplicate instance of Bob-- is removed.

140
00:07:05,080 --> 00:07:07,780

141
00:07:07,780 --> 00:07:13,370
And finally, another example-- here, we're going to use sort,

142
00:07:13,370 --> 00:07:15,770
and we're going to sort on the age column.

143
00:07:15,770 --> 00:07:18,550
We denote that here by using age in quotes.

144
00:07:18,550 --> 00:07:22,270
We could have also used df2.age, and ascending false,

145
00:07:22,270 --> 00:07:24,960
so that means it's going to be in descending order.

146
00:07:24,960 --> 00:07:28,780
And you can see we end up with name Bob 2

147
00:07:28,780 --> 00:07:34,240
and name Alice 1 as our rows.

148
00:07:34,240 --> 00:07:38,430
So another transformation-- Here,

149
00:07:38,430 --> 00:07:41,190
we start by creating a row that consists

150
00:07:41,190 --> 00:07:45,740
of two columns-- the first is A, and the second

151
00:07:45,740 --> 00:07:48,780
is intlist, and contains a list of integers.

152
00:07:48,780 --> 00:07:53,470
We're then going to apply the explode transformation.

153
00:07:53,470 --> 00:07:57,450
So we take and create a new DataFrame using select,

154
00:07:57,450 --> 00:08:02,130
where we apply explode to the intlist column

155
00:08:02,130 --> 00:08:03,340
of our DataFrame.

156
00:08:03,340 --> 00:08:08,240
And we're going to rename that resulting column an int.

157
00:08:08,240 --> 00:08:11,400
So explode is going to take each element of that list

158
00:08:11,400 --> 00:08:14,790
and create a new row out of it, and then the select

159
00:08:14,790 --> 00:08:17,960
is going to select only that column.

160
00:08:17,960 --> 00:08:22,600
So now we end up with three rows-- the first one is

161
00:08:22,600 --> 00:08:27,520
an int, 1, the second one is an int, value 2,

162
00:08:27,520 --> 00:08:32,950
and the third one is an int, value 3.

163
00:08:32,950 --> 00:08:35,120
So that's using transformations.

164
00:08:35,120 --> 00:08:38,450
Now let's talk about GroupedData transformations.

165
00:08:38,450 --> 00:08:42,039
GroupBy groups the DataFrame using the specified columns,

166
00:08:42,039 --> 00:08:45,960
so we can run aggregation on those columns.

167
00:08:45,960 --> 00:08:49,050
Here are three examples of group data functions--

168
00:08:49,050 --> 00:08:54,310
aggregate computes aggregates-- for example, average, max,

169
00:08:54,310 --> 00:08:58,550
minimum, sum, or count-- and returns the result

170
00:08:58,550 --> 00:09:00,310
as a DataFrame.

171
00:09:00,310 --> 00:09:03,810
Count counts the number of records for each group,

172
00:09:03,810 --> 00:09:06,500
and average computes average values

173
00:09:06,500 --> 00:09:09,850
for numeric columns for each group.

174
00:09:09,850 --> 00:09:13,110
So here's our first example-- we have

175
00:09:13,110 --> 00:09:19,580
four rows-- the first row is Alice 1,6, the second row is

176
00:09:19,580 --> 00:09:24,790
Bob 2 and 8, the third row is Alice 3 and 9,

177
00:09:24,790 --> 00:09:28,100
and the fourth row is Bob 4 and 7.

178
00:09:28,100 --> 00:09:31,120
So we're going to create a DataFrame with columns

179
00:09:31,120 --> 00:09:34,389
name, age, and grade.

180
00:09:34,389 --> 00:09:36,430
Then, we're going to create a new DataFrame where

181
00:09:36,430 --> 00:09:40,220
we group by the name, and then we're

182
00:09:40,220 --> 00:09:45,960
going to perform aggregation to count the number of elements

183
00:09:45,960 --> 00:09:48,910
in each one of those groups.

184
00:09:48,910 --> 00:09:53,390
So we can see there are two rows of Alice and two rows of Bob,

185
00:09:53,390 --> 00:09:55,870
and so as expected if we group by name,

186
00:09:55,870 --> 00:10:00,070
we end up with a count of two for name Alice

187
00:10:00,070 --> 00:10:03,760
and a count of two for a name Bob.

188
00:10:03,760 --> 00:10:08,340
Here's another example of how we could do that same count--

189
00:10:08,340 --> 00:10:12,390
instead, we could just use the count transformation

190
00:10:12,390 --> 00:10:14,310
on groupBy.

191
00:10:14,310 --> 00:10:18,240
In this case, we get the same result, but just

192
00:10:18,240 --> 00:10:21,660
a little bit simpler in the process.

193
00:10:21,660 --> 00:10:24,920
So here's another example-- our data

194
00:10:24,920 --> 00:10:31,410
that we start with is four sets of tuples.

195
00:10:31,410 --> 00:10:37,690
The first is Alice and 1 and 6, the second is Bob and 2 and 8,

196
00:10:37,690 --> 00:10:41,610
the third is Alice and 3 and 9, and the fourth

197
00:10:41,610 --> 00:10:44,480
is Bob and 4 and 7.

198
00:10:44,480 --> 00:10:47,480
Now we're going to create a DataFrame with columns

199
00:10:47,480 --> 00:10:51,700
name, age, and grade.

200
00:10:51,700 --> 00:10:57,160
If we do a groupBy and then ask what's the average,

201
00:10:57,160 --> 00:11:03,070
we'll end up with a single row in our new DataFrame that

202
00:11:03,070 --> 00:11:09,000
has average 2.5 because that's the average for age across all

203
00:11:09,000 --> 00:11:11,590
of the different tuples, or rows,

204
00:11:11,590 --> 00:11:15,750
and grade 7.5 because that's the average across all

205
00:11:15,750 --> 00:11:19,200
of the grades in the rows.

206
00:11:19,200 --> 00:11:24,410
Instead, we could look at grouping by name,

207
00:11:24,410 --> 00:11:26,190
and then ask the question, what's

208
00:11:26,190 --> 00:11:29,060
the average age and grade?

209
00:11:29,060 --> 00:11:37,360
So in this case, we end up with name Alice average 2.0,

210
00:11:37,360 --> 00:11:41,400
because that's averaging across Alice and 1 and Alice and 3,

211
00:11:41,400 --> 00:11:43,680
and average grade 7.5 because that's

212
00:11:43,680 --> 00:11:47,810
averaging across 6 and 9.

213
00:11:47,810 --> 00:11:53,960
Our second row has name Bob and has an average age of 3,

214
00:11:53,960 --> 00:11:58,660
because we average across 2 and 4, and an average grade of 7.5,

215
00:11:58,660 --> 00:12:04,170
because we're averaging across 8 and 7.

216
00:12:04,170 --> 00:12:07,080
So when we transform a DataFrame,

217
00:12:07,080 --> 00:12:09,390
we start with some source data.

218
00:12:09,390 --> 00:12:13,070
So here's an example where we read from a text file.

219
00:12:13,070 --> 00:12:17,020
We use sqlContext.read.text, give it the file name,

220
00:12:17,020 --> 00:12:19,610
and that will create a lines DataFrame

221
00:12:19,610 --> 00:12:23,180
because we're going to read each line of that file.

222
00:12:23,180 --> 00:12:25,190
We can then filter out the lines that

223
00:12:25,190 --> 00:12:28,210
are comments by using the filter transformation

224
00:12:28,210 --> 00:12:32,019
and a function that gets applied to each one of those lines

225
00:12:32,019 --> 00:12:33,810
to determine whether it's a comment or not.

226
00:12:33,810 --> 00:12:37,010
This allows us to create a comments DataFrame.

227
00:12:37,010 --> 00:12:38,890
Now because we're using lazy evaluation,

228
00:12:38,890 --> 00:12:42,050
it means that nothing actually executes when we have these two

229
00:12:42,050 --> 00:12:43,300
different commands.

230
00:12:43,300 --> 00:12:46,080
Instead, again Spark saves a recipe

231
00:12:46,080 --> 00:12:48,860
for transforming the source.

